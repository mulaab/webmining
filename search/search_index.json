{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Silahkan Register kuliah di Schoology dengan **Access Code**CVW8-55BP-MPFPV \u00b6 Deskripsi Mata Kuliah \u00b6 Diskripsi Mata Kuliah Penambangan Web ( Web Mining ) Mata kuliah Penambangan Web menjelaskan terkait analysis yang dilakukan untuk mengekstraksi pola-pola penting yang tersimpan secara implisit pada berbagai macam kumpulan data bisnis web yang relatif besar dengan berdasarkan konsep machine learning dan statistik. Tujuan utamanya adalah mengekplorasi dan menambang pola data web untuk mendapatkan informasi yang berguna bagi kebutuhan bisnis. Penambangan data Web mining terdiri atas tiga tugas yang dilakukan secara umum yaitu: web content mining, web structure mining, dan web usage mining. Materi dalam kuliah ini akan memberikan pengetahuan, pemahaman, pengalaman penggunaan beberapa teknik dan/atau konsep dalam ekplorasi dan ektraksi informasi data yang besar web sehingga dapat dimanfaatkan untuk kebutuhan strategi dan pengambilan keputusan bisnis Capaian Pembelajaran Mata Kuliah Penambangan Web ( Web Mining ) Memahami proses penambangan data web Memahami data web Memahami praproses penambangan data web Memahami konsep dan pemodelan pengelompokan penambangan data web serta evaluasi pegelompokan Memahami konsep dan pemodelan klasifikasi penambangan data web serta evaluasi klasifikasi Memahami konsep dan pemodelan data jaringan web dan analisa jejaring sosial Mampu mengimplementasikan penambangan data web pengetahuan dari berbagai database besar Materi Pembelajaran Pengantar Penambangan web Dasar dasar data mining Memahami data Rekayasa Fitur Web Crawling Pembelajaran terawasi Pembelajaran tak terawasi Evaluasi Model Analisa Jejaring Sosial Extraksi data terstruktur Penambangan isi web (text Mining) Penambangan data jejaring sosial ( graph mining) Mata Kuliah Prasyarat \u00b6 Statistika(TIK206), Komputasi Aljabar Linier (TIK201), Penambangan Data (TIK502) Daftar Referensi Mata Kuliah \u00b6 Zaki, Mohammed J., and Wagner Meira. Data mining and analysis: fundamental concepts and algorithms. Cambridge University Press, 2014. Liu, Bing. Web data mining: exploring hyperlinks, contents, and usage data. Springer Science & Business Media, 2007. Tsvetovat, Maksim, and Alexander Kouznetsov. Social Network Analysis for Startups: Finding connections on the social web. \" O'Reilly Media, Inc.\", 2011 Jadwal Pembelajaran Mata Kuliah \u00b6 Minggu Ke Materi Pembelajaran Sumber Referensi Tujuan 1 Pengantar Penambangan Web 1 Membuat arsitektur desain penambangan data web 2 Dasar dasar analisa atribut kategorikal 1 Melakukan analisa data multivariate tipe data kategorical 3 Analisa data dimensi tinggi (High-dimensional) 1 4 Rekayasa fitur 1 Melakukan reduksi dimensi dengan PCA, kernel PCA, SVD 5 Pengelompokan berbasi Representatif 1 Melakukan pengelompokan algoritma K-mean,Kernel K-mean, Expecation Maximation 6 Pengelompokan Hirarki 1 Melakukan pengelompokan dengan agglomerative hierarchical 7 Pengelompokan berbasis density 1 Melakukan pengelompokan dengan algoritma dbscan 8 Validasi Pengelompokan 1 Menghitung validasi hasil semua pengelompokan yang telah dilakukan 9 Klasifikasi berbasis Probabilitas 1 Melakukan pengklasifian dengan metode bayes 10 Klasifikasi dengan pohon keputusan 1 Melukan metode pengklasifian dengan algoritma ID3 dan C45 11 Centrality 3 Dapat menghitung centrality dari data jejaring soal 12 Dasar dasar web crawling, Unversel Crawler,Focused crawler,Topical crawler 2 Melakukan crawling terhadap data web 13 Extraksi data terstruktur 1 Dapat membangun vector space model 14 Penambangan isi web dan Jejaring Sosial 2,3 Dapat mengimplementasi penambangan data web isi web dan jejaring sosial analisis dar media sosial (twitter dan linkedin dan web page )","title":"Rencana Pembelajaran Semester"},{"location":"#silahkan-register-kuliah-di-schoology-dengan-access-codecvw8-55bp-mpfpv","text":"","title":"Silahkan Register kuliah di Schoology dengan **Access Code**CVW8-55BP-MPFPV"},{"location":"#deskripsi-mata-kuliah","text":"Diskripsi Mata Kuliah Penambangan Web ( Web Mining ) Mata kuliah Penambangan Web menjelaskan terkait analysis yang dilakukan untuk mengekstraksi pola-pola penting yang tersimpan secara implisit pada berbagai macam kumpulan data bisnis web yang relatif besar dengan berdasarkan konsep machine learning dan statistik. Tujuan utamanya adalah mengekplorasi dan menambang pola data web untuk mendapatkan informasi yang berguna bagi kebutuhan bisnis. Penambangan data Web mining terdiri atas tiga tugas yang dilakukan secara umum yaitu: web content mining, web structure mining, dan web usage mining. Materi dalam kuliah ini akan memberikan pengetahuan, pemahaman, pengalaman penggunaan beberapa teknik dan/atau konsep dalam ekplorasi dan ektraksi informasi data yang besar web sehingga dapat dimanfaatkan untuk kebutuhan strategi dan pengambilan keputusan bisnis Capaian Pembelajaran Mata Kuliah Penambangan Web ( Web Mining ) Memahami proses penambangan data web Memahami data web Memahami praproses penambangan data web Memahami konsep dan pemodelan pengelompokan penambangan data web serta evaluasi pegelompokan Memahami konsep dan pemodelan klasifikasi penambangan data web serta evaluasi klasifikasi Memahami konsep dan pemodelan data jaringan web dan analisa jejaring sosial Mampu mengimplementasikan penambangan data web pengetahuan dari berbagai database besar Materi Pembelajaran Pengantar Penambangan web Dasar dasar data mining Memahami data Rekayasa Fitur Web Crawling Pembelajaran terawasi Pembelajaran tak terawasi Evaluasi Model Analisa Jejaring Sosial Extraksi data terstruktur Penambangan isi web (text Mining) Penambangan data jejaring sosial ( graph mining)","title":"Deskripsi Mata Kuliah"},{"location":"#mata-kuliah-prasyarat","text":"Statistika(TIK206), Komputasi Aljabar Linier (TIK201), Penambangan Data (TIK502)","title":"Mata Kuliah Prasyarat"},{"location":"#daftar-referensi-mata-kuliah","text":"Zaki, Mohammed J., and Wagner Meira. Data mining and analysis: fundamental concepts and algorithms. Cambridge University Press, 2014. Liu, Bing. Web data mining: exploring hyperlinks, contents, and usage data. Springer Science & Business Media, 2007. Tsvetovat, Maksim, and Alexander Kouznetsov. Social Network Analysis for Startups: Finding connections on the social web. \" O'Reilly Media, Inc.\", 2011","title":"Daftar Referensi Mata Kuliah"},{"location":"#jadwal-pembelajaran-mata-kuliah","text":"Minggu Ke Materi Pembelajaran Sumber Referensi Tujuan 1 Pengantar Penambangan Web 1 Membuat arsitektur desain penambangan data web 2 Dasar dasar analisa atribut kategorikal 1 Melakukan analisa data multivariate tipe data kategorical 3 Analisa data dimensi tinggi (High-dimensional) 1 4 Rekayasa fitur 1 Melakukan reduksi dimensi dengan PCA, kernel PCA, SVD 5 Pengelompokan berbasi Representatif 1 Melakukan pengelompokan algoritma K-mean,Kernel K-mean, Expecation Maximation 6 Pengelompokan Hirarki 1 Melakukan pengelompokan dengan agglomerative hierarchical 7 Pengelompokan berbasis density 1 Melakukan pengelompokan dengan algoritma dbscan 8 Validasi Pengelompokan 1 Menghitung validasi hasil semua pengelompokan yang telah dilakukan 9 Klasifikasi berbasis Probabilitas 1 Melakukan pengklasifian dengan metode bayes 10 Klasifikasi dengan pohon keputusan 1 Melukan metode pengklasifian dengan algoritma ID3 dan C45 11 Centrality 3 Dapat menghitung centrality dari data jejaring soal 12 Dasar dasar web crawling, Unversel Crawler,Focused crawler,Topical crawler 2 Melakukan crawling terhadap data web 13 Extraksi data terstruktur 1 Dapat membangun vector space model 14 Penambangan isi web dan Jejaring Sosial 2,3 Dapat mengimplementasi penambangan data web isi web dan jejaring sosial analisis dar media sosial (twitter dan linkedin dan web page )","title":"Jadwal Pembelajaran Mata Kuliah"},{"location":"Apa%20data%20science/","text":"Apa data science? Data science adalah ilmu data adalah analisis sistematis data dalam kerangka ilmiah. Oleh karena itu data science itu bersifat: adaptif, iteratif, dan pendekatan bertahap untuk analisa dat dilakukan dalam kerangka sistematis mencari model yang optimal menilai dan memperhitungkan biaya sebenarnya dari kesalahan prediksi Data science melibatkan beberapa disiplin ilmu diantaranya Pendekatan berbasis data untuk analisas data statistik. Dengan pendekatan statistik ini kita dapat mampu untuk mengeksplorasi data, pengujian signifikasi data dan visualisasi pola data Kemampuan komputasi dan skill pemrograman ilmu komputer Domain tertentu dari kecerdasasan bisnis (bussiness Intelligence) Dengan kata lain, ilmu data memungkinkan kita mengekstrak pengetahuan dari database yang kurang dimanfaatkan. Dengan demikian, gudang data yang telah mengumpulkan banyak data dapat dimanfaatkan untuk menambah keuntungan bisnis. Ilmu data memungkinkan orang untuk memanfaatkan data dan daya komputasi dalam jumlah besar dari problem yang sangat komplek dalam bisnis. Ilmu data dapat menemukan pola dari data yang tidak dapat ditemukan dengan teknik lain. Penemuan ini dapat memberikan keuntungan dalam mengambil keputusan bagi perusahaan. Kenapa kita membutuhkan data science ? Ada beberapa hal sehingga kita sangat membutuhkan data science sebagai pendektan baru diataranya adalah Adanya teknologi baru yang memungkinkan kita dapat mengambil data, merekam data, menyimpan secara cepat dalam jumlah yang besar data sosial media, logging data, dan data sensor. Setelah mengumpulkan data ini semua, kita akan bertanya apa yang dapat kita lakukan selanjutnya terhadap data ini. Kemampuan komputasi yang memungkinkan untuk menganalisa lebih mendalam dalam skala besar data. Termasuk adanya komputasi awan, pembelajaran mesin sehingga dapat mendukung kebutuhan untuk analisa tersebut. https://medium.com/analytics-vidhya/facebook-graph-api-python-3c8bab8a5a2a Karakteristik Data","title":"Apa data science"},{"location":"Autokorelasi%20dan%20Analisa%20Deret%20Berkala/","text":"Autokorelasi dan Analisa Deret Berkala 1 \u00b6 http://www.real-statistics.com/time-series-analysis/stochastic-processes/partial-autocorrelation-function/ Deret berkala adalah pengamatan data terhadap waktu. Jika data pengamatan berkaitan dengan waktu, data deret berkala adala be-rautokorelasi Ye, Nong. Data mining: theories, algorithms, and examples . CRC press, 2013. \u21a9","title":"Autokorelasi dan Analisa Deret Berkala"},{"location":"Autokorelasi%20dan%20Analisa%20Deret%20Berkala/#autokorelasi-dan-analisa-deret-berkala1","text":"http://www.real-statistics.com/time-series-analysis/stochastic-processes/partial-autocorrelation-function/ Deret berkala adalah pengamatan data terhadap waktu. Jika data pengamatan berkaitan dengan waktu, data deret berkala adala be-rautokorelasi Ye, Nong. Data mining: theories, algorithms, and examples . CRC press, 2013. \u21a9","title":"Autokorelasi dan Analisa Deret Berkala1"},{"location":"Data%20science/","text":"What Data Science (data science design manual) What Data Science(Data science using python and R) Getting started with data science (Build a carrier in Data Science) Data Science and Business Strategy (data science for bussines) Data science process ( Data science concept dan practice) \u200b Data Science Lifecycle and Technologies (An introduction math guide for beginner to understanda data science)","title":"Data science"},{"location":"Disritisasi/","text":"Data mining Oleh : mulaab TIF UTM Diskritisasi \u00b6 Diskritisasi juga disebut binning, merubah atribut numerik menjadi atribut kategorikal. Biasanya digunakan untuk metode/model data mining yang tidak dapat menangani atribut numerik. Ini juga dapat membantu mengurangi jumlah nilai atribut, terutama jika ada noise dalam pengukuran numerik; diskretisasi memungkinkan untuk mengabaikan nilai yang kecil dan tidak relevan Secara umum, diberikan atribut numerik X X dan sebanyak n n data sampel acak \\{x_1\\}_{1=1}^n \\{x_1\\}_{1=1}^n dari data X X , tugas diskritisasi adalah membagi rentang nilai dari X X sebanyak k k interval secara berturut turut, yang disebut dengan bin dengan caran mencari k-1 k-1 nilai batas v_1, v_2,...v_{k-1} v_1, v_2,...v_{k-1} yang menghasilkan k k interval. $$ [ x _ { \\operatorname { min } } , v _ { 1 } ] , ( v _ { 1 } , v _ { 2 } ] , \\ldots , ( v _ { k - 1 } , x _ { \\operatorname { max } } ] $$ dimana $$ x _ { \\operatorname { min } } = \\operatorname { min } _ { i } { x _ { i } } \\quad x _ { \\operatorname { max } } = \\operatorname { max } _ { i } { x _ { i } } $$ Menghasilkan k k interval atau bin, yang menjangkau seluruh rentang X, biasanya dipetakan ke nilai simbolik untuk atribut kategorikal yang baru dari X Rentang Lebar Sama (Equal-Width Intervals) \u00b6 Pendekatan diskritisasi yang paling sederhana adalah membagi rentang dari X X menjadi k k rentang dengan lebar sama (equal-width interval). Decara cara yang sederhana yaitu membagi lebar rentang dari X X menjadi k k rentang dengan $$ w = \\frac { x _ { \\operatorname { max } } - x _ { \\operatorname { min } } } { k } $$ Kemudian batas rentang ke-i dinyatakan dengan $$ v _ { i } = x _ { \\operatorname { min } } + i w , \\text { untuk } i = 1 , \\ldots , k - 1 $$ Rentang Frekwensi Sama (Equal-Frequency Intervals) \u00b6 Pada diskritisasi frekwensi sama, kita membagi rentang dari X X menjadi rentang rentang yang berisi jumlah data yang sama (mendekati sama), frekuensi yang sama mungkin tidak dimungkinkan karena ada nilai yang diulang. Rentang dapat dihitung dari fungsi distribusi kumulatif \\hat F^{-1}(q) \\hat F^{-1}(q) untuk X X dengan $$ F ^ { - 1 } ( q ) = \\operatorname { min } { x | \\hat { F } ( x ) \\geq q } \\quad \\text { untuk } q \\in [ 0,1 ] $$ Tentunya, kita perlu bahwa setiap rentagn berisi 1/k 1/k dari massa probabilitas, sehingga batas rentang dinyatakan dengan $$ v _ { i } = \\hat { F } ^ { - 1 } ( i / k ) \\text { untuk } i = 1 , \\ldots , k - 1 $$ Contoh data iris pada atribut sepal length, nilai minimum dan maximumnya adalah $$ x _ { \\operatorname { min } } = 4.3 \\quad x _ { \\operatorname { max } } = 7.9 $$ Kita diskritisasi ke dalam k=4 bin dengan menggunakan equal-width binning. Lebar rentang dinyatakan dengan $$ w = \\frac { 7.9 - 4.3 } { 4 } = \\frac { 3.6 } { 4 } = 0.9 $$ dan oleh karena itu batas rentang adalah $$ v _ { 1 } = 4.3 + 0.9 = 5.2 \\quad v _ { 2 } = 4.3 + 2 \\cdot 0.9 = 6.1 \\quad v _ { 3 } = 4.3 + 3 \\cdot 0.9 = 7.0 $$ Untuk keempat dari hasil bin untuk sepal length ditunjukkan dalam tabel berikut , yang menunjukkan banyaknya data n_i n_i dari setiap bin tidak sama. Untuk diskritisasi equal-frequency, perhatikan fungsi distribusikumulatif invers (Cumulatif Distribution Function) untuk atribut sepal length dalam gambar berikut. Dengan k=4 bin, batas bin adalah nilai-nilai kuartil (yang ditunjukkan dengan garis putus putus) $$ v _ { 1 } = \\hat { F } ^ { - 1 } ( 0.25 ) = 5.1 \\quad v _ { 2 } = \\hat { F } ^ { - 1 } ( 0.50 ) = 5.8 \\quad v _ { 3 } = \\hat { F } ^ { - 1 } ( 0.75 ) = 6.4 $$ Gambar Fungsi Distribusi Kumulatif Invers Empiris (Empirical inverse CDF): sepal length. Gambar diskritisasi untuk sepal length equal-frequency Proses diskritisasi untuk atribut sepal length ada di excell","title":"Disritisasi"},{"location":"Disritisasi/#diskritisasi","text":"Diskritisasi juga disebut binning, merubah atribut numerik menjadi atribut kategorikal. Biasanya digunakan untuk metode/model data mining yang tidak dapat menangani atribut numerik. Ini juga dapat membantu mengurangi jumlah nilai atribut, terutama jika ada noise dalam pengukuran numerik; diskretisasi memungkinkan untuk mengabaikan nilai yang kecil dan tidak relevan Secara umum, diberikan atribut numerik X X dan sebanyak n n data sampel acak \\{x_1\\}_{1=1}^n \\{x_1\\}_{1=1}^n dari data X X , tugas diskritisasi adalah membagi rentang nilai dari X X sebanyak k k interval secara berturut turut, yang disebut dengan bin dengan caran mencari k-1 k-1 nilai batas v_1, v_2,...v_{k-1} v_1, v_2,...v_{k-1} yang menghasilkan k k interval. $$ [ x _ { \\operatorname { min } } , v _ { 1 } ] , ( v _ { 1 } , v _ { 2 } ] , \\ldots , ( v _ { k - 1 } , x _ { \\operatorname { max } } ] $$ dimana $$ x _ { \\operatorname { min } } = \\operatorname { min } _ { i } { x _ { i } } \\quad x _ { \\operatorname { max } } = \\operatorname { max } _ { i } { x _ { i } } $$ Menghasilkan k k interval atau bin, yang menjangkau seluruh rentang X, biasanya dipetakan ke nilai simbolik untuk atribut kategorikal yang baru dari X","title":"Diskritisasi"},{"location":"Disritisasi/#rentang-lebar-sama-equal-width-intervals","text":"Pendekatan diskritisasi yang paling sederhana adalah membagi rentang dari X X menjadi k k rentang dengan lebar sama (equal-width interval). Decara cara yang sederhana yaitu membagi lebar rentang dari X X menjadi k k rentang dengan $$ w = \\frac { x _ { \\operatorname { max } } - x _ { \\operatorname { min } } } { k } $$ Kemudian batas rentang ke-i dinyatakan dengan $$ v _ { i } = x _ { \\operatorname { min } } + i w , \\text { untuk } i = 1 , \\ldots , k - 1 $$","title":"Rentang Lebar Sama (Equal-Width Intervals)"},{"location":"Disritisasi/#rentang-frekwensi-sama-equal-frequency-intervals","text":"Pada diskritisasi frekwensi sama, kita membagi rentang dari X X menjadi rentang rentang yang berisi jumlah data yang sama (mendekati sama), frekuensi yang sama mungkin tidak dimungkinkan karena ada nilai yang diulang. Rentang dapat dihitung dari fungsi distribusi kumulatif \\hat F^{-1}(q) \\hat F^{-1}(q) untuk X X dengan $$ F ^ { - 1 } ( q ) = \\operatorname { min } { x | \\hat { F } ( x ) \\geq q } \\quad \\text { untuk } q \\in [ 0,1 ] $$ Tentunya, kita perlu bahwa setiap rentagn berisi 1/k 1/k dari massa probabilitas, sehingga batas rentang dinyatakan dengan $$ v _ { i } = \\hat { F } ^ { - 1 } ( i / k ) \\text { untuk } i = 1 , \\ldots , k - 1 $$ Contoh data iris pada atribut sepal length, nilai minimum dan maximumnya adalah $$ x _ { \\operatorname { min } } = 4.3 \\quad x _ { \\operatorname { max } } = 7.9 $$ Kita diskritisasi ke dalam k=4 bin dengan menggunakan equal-width binning. Lebar rentang dinyatakan dengan $$ w = \\frac { 7.9 - 4.3 } { 4 } = \\frac { 3.6 } { 4 } = 0.9 $$ dan oleh karena itu batas rentang adalah $$ v _ { 1 } = 4.3 + 0.9 = 5.2 \\quad v _ { 2 } = 4.3 + 2 \\cdot 0.9 = 6.1 \\quad v _ { 3 } = 4.3 + 3 \\cdot 0.9 = 7.0 $$ Untuk keempat dari hasil bin untuk sepal length ditunjukkan dalam tabel berikut , yang menunjukkan banyaknya data n_i n_i dari setiap bin tidak sama. Untuk diskritisasi equal-frequency, perhatikan fungsi distribusikumulatif invers (Cumulatif Distribution Function) untuk atribut sepal length dalam gambar berikut. Dengan k=4 bin, batas bin adalah nilai-nilai kuartil (yang ditunjukkan dengan garis putus putus) $$ v _ { 1 } = \\hat { F } ^ { - 1 } ( 0.25 ) = 5.1 \\quad v _ { 2 } = \\hat { F } ^ { - 1 } ( 0.50 ) = 5.8 \\quad v _ { 3 } = \\hat { F } ^ { - 1 } ( 0.75 ) = 6.4 $$ Gambar Fungsi Distribusi Kumulatif Invers Empiris (Empirical inverse CDF): sepal length. Gambar diskritisasi untuk sepal length equal-frequency Proses diskritisasi untuk atribut sepal length ada di excell","title":"Rentang Frekwensi Sama (Equal-Frequency Intervals)"},{"location":"Eksplorasi%20data%204_analisa%20Multivariate/","text":"Analisa Multivariate \u00b6 Asumsikan bahwa dataset terdiri dari d d atribut kategorical X _ { j } ( 1 \\leq j \\leq d ) X _ { j } ( 1 \\leq j \\leq d ) dengan \\operatorname { dom } ( X _ { j } ) = \\{ a _ { j 1 } , a _ { j 2 } , \\ldots , a _ { j m j } \\} \\operatorname { dom } ( X _ { j } ) = \\{ a _ { j 1 } , a _ { j 2 } , \\ldots , a _ { j m j } \\} . Kita memiliki n n titik-titik kategorikal dari bentuk x_i= ( x _ { i1 } , x _ { i2 } , \\ldots , x _ { i d } ) ^ { T } x_i= ( x _ { i1 } , x _ { i2 } , \\ldots , x _ { i d } ) ^ { T } dengan x_{ij }\\in dom (X_j) x_{ij }\\in dom (X_j) . Datase kemudian dinayatakan dalam bentuk matrik ukuran n\\times d n\\times d $$ D = \\left( \\begin{array} { c c c c } { X _ { 1 } } & { X _ { 2 } } & { \\cdots } & { X _ { d } } \\ \\hline x _ { 11 } & { x _ { 12 } } & { \\cdots } & { x _ { 1 d } } \\ { x _ { 21 } } & { x _ { 22 } } & { \\cdots } & { x _ { 2 d } } \\ { \\vdots } & { \\vdots } & { \\ddots } & { \\vdots } \\ { x _ { n 1 } } & { x _ { n 2 } } & { \\cdots } & { x _ { n d } } \\end{array} \\right) $$ Setiap atribut X_i X_i dimodelkan sebagai m_i m_i dimensi multivariate variabel Bernoulli X_i X_i dan distribusi gabungannya dimodelkan sebagai d ^ { \\prime } = \\sum _ { j = 1 } ^ { d } m _ { j } d ^ { \\prime } = \\sum _ { j = 1 } ^ { d } m _ { j } variabel acak vektor dimensi $$ X = \\left( \\begin{array} { c } { X _ { 1 } } \\ { \\vdots } \\ { X _ { d } } \\end{array} \\right) $$ Setiap titik data categorikal v = ( v _ { 1 } , v _ { 2 } , \\ldots , v _ { d } ) ^ { T } v = ( v _ { 1 } , v _ { 2 } , \\ldots , v _ { d } ) ^ { T } kemudian dinyatakan sebagai vektor biner d d dimensi $$ X ( v ) = \\left( \\begin{array} { c } { X _ { 1 } ( v _ { 1 } ) } \\ { \\vdots } \\ { X _ { d } ( v _ { d } ) } \\end{array} \\right) = \\left( \\begin{array} { c } { e _ { 1 k _ { 1 } } } \\ { \\vdots } \\ { e _ { d k _ { d } } } \\end{array} \\right) $$ dengan v_i=a_{ikj} v_i=a_{ikj} simbol ke k_i k_i dari X_i X_i . Disini e_{ikj} e_{ikj} adalah vektor basis standr dalam $\\mathbb R^{m_i} $ Mean \u00b6 Mengeneralisasi dari kasus bivariate, mea dan sample mean untuk X X dinyatakan dengan $$ \\mu = E [ X ] = \\left( \\begin{array} { c } { \\mu _ { 1 } } \\ { \\vdots } \\ { \\mu _ { d } } \\end{array} \\right) = \\left( \\begin{array} { c } { p _ { 1 } } \\ { \\vdots } \\ { p _ { d } } \\end{array} \\right) \\quad \\hat { \\mu } = \\left( \\begin{array} { c } { \\hat { \\mu } _ { 1 } } \\ { \\vdots } \\ { \\hat { \\mu } _ { d } } \\end{array} \\right) = \\left( \\begin{array} { c } { \\hat { p } _ { 1 } } \\ { \\vdots } \\ { \\hat { p } _ { d } } \\end{array} \\right) $$ dimana p _ { i } = ( p _ { 1 } ^ { i } , \\ldots , p _ { m _ { i } } ^ { i } ) ^ { T } p _ { i } = ( p _ { 1 } ^ { i } , \\ldots , p _ { m _ { i } } ^ { i } ) ^ { T } adalah fungsi massa probabilitas (PMF) untuk X_i X_i dan \\hat { p } _ { i } = ( \\hat { p } _ { 1 } ^ { i } , \\ldots , \\hat { p } _ { m _ { i } } ^ { i } ) ^ { T } \\hat { p } _ { i } = ( \\hat { p } _ { 1 } ^ { i } , \\ldots , \\hat { p } _ { m _ { i } } ^ { i } ) ^ { T } adalah fungsi massa probabilitas empiris untuk X_i X_i Matrik Covariance \u00b6 Matrik kovarian untuk X X dan estimasi dari sample diberikan dengan matrik d' \\times d' d' \\times d' $$ \\Sigma = \\left( \\begin{array} { c c c c } { \\Sigma _ { 11 } } & { \\Sigma _ { 12 } } & { \\cdots } & { \\Sigma _ { 1 d } } \\ { \\Sigma _ { 12 } ^ { T } } & { \\Sigma _ { 22 } } & { \\cdots } & { \\Sigma _ { 2 d } } \\ { \\cdots } & { \\cdots } & { \\ddots } & { \\cdots } \\ { \\Sigma _ { 1 d } ^ { T } } & { \\Sigma _ { 2 d } ^ { T } } & { \\cdots } & { \\Sigma _ { d d } } \\end{array} \\right) \\hat { \\Sigma } = \\left( \\begin{array} { c c c c } { \\hat { \\Sigma } _ { 11 } } & { \\hat { \\Sigma } _ { 12 } } & { \\cdots } & { \\hat { \\Sigma } _ { 1 d } } \\ { \\hat { \\Sigma } _ { 12 } ^ { T } } & { \\hat { \\Sigma } _ { 22 } } & { \\cdots } & { \\hat { \\Sigma } _ { 2 d } } \\ { \\ldots } & { \\ldots } & { \\ddots } & { \\ldots } \\ { \\hat { \\Sigma } _ { 1 d } ^ { T } } & { \\hat { \\Sigma } _ { 2 d } ^ { T } } & { \\cdots } & { \\hat { \\Sigma } _ { d d } } \\end{array} \\right) $$ dimana d ^ { \\prime } = \\sum _ { i = 1 } ^ { d } m _ { i } d ^ { \\prime } = \\sum _ { i = 1 } ^ { d } m _ { i } dan \\Sigma_{ij} \\Sigma_{ij} dam \\hat {\\Sigma}_{ij} \\hat {\\Sigma}_{ij} adalah m_i \\times m_j m_i \\times m_j matrik kovarian (dan estimasinya ) untuk atribut X_i X_i dan X_j X_j : $$ \\Sigma _ { i j } = P _ { i j } - p _ { i } p _ { j } ^ { T } \\quad \\hat { \\Sigma } _ { i j } = \\hat { P } _ { i j } - \\hat { p } _ { i } \\hat { p } _ { j } ^ { T } $$ Disini P_{ij} P_{ij} adalah fungsi massa probabilitas gabungan dan \\hat {P}_{ij} \\hat {P}_{ij} adalah fungsi massa probabilitas empiris yang dapat dihitung menggunakan persamaan (3.13) Contoh 3.11. Analisa Multivariate . Perhatikan subset dari dataset 3-dimensi dengan atribut yang telah didiskritisasi, sepal length ( X_1 X_1 ) dan sepal width ( X_2 X_2 ) dan atribut kategorikal class ( X_3 X_3 ). Domain untuk X_1 X_1 dan X_2 X_2 diberikan dalam tabel 3.1 dan tabel 3.3. dan dom (X_3)=(iris-versicolor, iris-setosa,iris-verginica) dom (X_3)=(iris-versicolor, iris-setosa,iris-verginica) . Setiap nilai dari X_3 X_3 terjadi 50 kali. Titik x categorical =(Short, Medium, iris-versicolor) dimodelkan sebagai vektor $$ X ( x ) = \\left( \\begin{array} { l } { e _ { 12 } } \\ { e _ { 22 } } \\ { e _ { 31 } } \\end{array} \\right) = ( 0,1,0,0 | 0,1,0 | 1,0,0 ) ^ { T } \\in \\mathbb R ^ { 10 } $$ Dari contoh 3.8 dan fakta bahwa setiap nilai dalam dom(X_3) dom(X_3) terjadi 50m kali dalam sample n=50 n=50 mean sample untuk tiga atribut ini dinyatakan dengan $$ \\hat { \\mu } = \\left( \\begin{array} { l } { \\hat { \\mu } _ { 1 } } \\ { \\hat { \\mu } _ { 2 } } \\ { \\hat { \\mu } _ { 3 } } \\end{array} \\right) = \\left( \\begin{array} { l } { \\hat { p } _ { 1 } } \\ { \\hat { p } _ { 2 } } \\ { \\hat { p } _ { 3 } } \\end{array} \\right) = ( 0.3,0.333,0.287,0.08 | 0.313,0.587,0.1 | 0.33,0.33,0.33 ) ^ { T } $$ Menggunakan \\hat { p } _ { 3 } = ( 0.33,0.33,0.33 ) ^ { T } \\hat { p } _ { 3 } = ( 0.33,0.33,0.33 ) ^ { T } , kita dapat menghitung matrik covariance sample untuk X_3 X_3 menggunakan persamaan (3.9) $$ \\hat { \\Sigma } _ { 33 } = \\left( \\begin{array} { r r r } { 0.222 } & { - 0.111 } & { - 0.111 } \\ { - 0.111 } & { 0.222 } & { - 0.111 } \\ { - 0.111 } & { - 0.111 } & { 0.222 } \\end{array} \\right) $$ Menggunakan persamaan (3.18) kita peroleh $$ \\hat { \\Sigma } _ { 13 } = \\left( \\begin{array} { r r r } { - 0.067 } & { 0.16 } & { - 0.093 } \\ { 0.082 } & { - 0.038 } & { - 0.044 } \\ { 0.011 } & { - 0.096 } & { 0.084 } \\ { - 0.027 } & { - 0.027 } & { 0.053 } \\end{array} \\right) $$ \\hat { \\Sigma } _ { 23 } = \\left( \\begin{array} { r r r } { 0.076 } & { - 0.098 } & { 0.022 } \\\\ { - 0.042 } & { 0.044 } & { - 0.002 } \\\\ { - 0.033 } & { 0.053 } & { - 0.02 } \\end{array} \\right) \\hat { \\Sigma } _ { 23 } = \\left( \\begin{array} { r r r } { 0.076 } & { - 0.098 } & { 0.022 } \\\\ { - 0.042 } & { 0.044 } & { - 0.002 } \\\\ { - 0.033 } & { 0.053 } & { - 0.02 } \\end{array} \\right) Dikombinasikan dengan \\widehat{\\Sigma}_{11} \\widehat{\\Sigma}_{11} , \\widehat{\\Sigma}_{22} \\widehat{\\Sigma}_{22} dan \\widehat{\\Sigma}_{12} \\widehat{\\Sigma}_{12} dari persamaan 3.8 matrik covarian sample akhir adalah matrik simetris 10 \\times 10 10 \\times 10 dinyatakan dengan $$ \\hat { \\Sigma } = \\left( \\begin{array} { l l l } { \\hat { \\Sigma } _ { 11 } } & { \\hat { \\Sigma } _ { 12 } } & { \\hat { \\Sigma } _ { 13 } } \\ { \\hat { \\Sigma } _ { 12 } ^ { T } } & { \\hat { \\Sigma } _ { 22 } } & { \\hat { \\Sigma } _ { 23 } } \\ { \\hat { \\Sigma } _ { 13 } ^ { 7 } } & { \\hat { \\Sigma } _ { 23 } ^ { T } } & { \\hat { \\Sigma } _ { 33 } } \\end{array} \\right) $$ Analisa Contingency Multiway \u00b6 Untuk analisa kebergantungan multiway, kita pertama kali harus menentukan fungsi massa probabilitas empiris untuk X X : $$ \\hat { f } ( e _ { 1 i _ { 1 } } , e _ { 2 i _ { 2 } } , \\ldots , e _ { d i _ { d } } ) = \\frac { 1 } { n } \\sum _ { k = 1 } ^ { n } I _ { i _ { 1 } i _ { 2 } , \\ldots _ { d } } ( x _ { k } ) = \\frac { n _ { i _ { 1 } i _ { 2 } \\ldots i _ { d } } } { n } = \\hat { p } _ { i _ { 1 } i _ { 2 } \\ldots i _ { d } } $$ dimana I _ { i _ { 1 } i _ { 2 } \\ldots l _ { d } } I _ { i _ { 1 } i _ { 2 } \\ldots l _ { d } } adalah variabel indikator $$ I _ { i _ { 1 } i _ { 2 } \\ldots i _ { d } } ( x _ { k } ) = \\left{ \\begin{array} { l l } { 1 } & { \\text { jika } x _ { k 1 } = e _ { 1 i _ { 1 } } , x _ { k 2 } = e _ { 2 i _ { 2 } } , \\ldots , x _ { k d } = e _ { d i _ { d } } } \\ { 0 } & { \\text { lainnya } } \\end{array} \\right. $$ Jumlah dari I _ { i _ { 1 } i _ { 2 } \\ldots i _ { d } } I _ { i _ { 1 } i _ { 2 } \\ldots i _ { d } } terhadap semua n n titik dalam sample menghasilkan jumlah kejadian n _ { i _ { 1 } i _ { 2 } \\ldots i _ { d } } n _ { i _ { 1 } i _ { 2 } \\ldots i _ { d } } dari simbol vektor ( a _ { 1 i _ { 1 } } , a _ { 2 i _ { i } } , \\ldots , a _ { d i _ { d } } ) ( a _ { 1 i _ { 1 } } , a _ { 2 i _ { i } } , \\ldots , a _ { d i _ { d } } ) . Membagi kejadian dengan ukuran sample menghasilkan dalam probabilitas dari simbol simbol pengamatan. Dengan menggunakan notasi i = ( i _ { 1 } , i _ { 2 } , \\ldots , i _ { d } ) i = ( i _ { 1 } , i _ { 2 } , \\ldots , i _ { d } ) untuk menotasikan index baris, kita dapat menulis fungsi massa probabilitas empiris gabungan sebagai matrik d- d- dimensi \\widehat P \\widehat P dari ukuran m_1 \\times m_2 \\times ...\\times m_d= \\prod _ { i = 1 } ^ { d } m _ { i } m_1 \\times m_2 \\times ...\\times m_d= \\prod _ { i = 1 } ^ { d } m _ { i } dinyatakan dengan \\widehat { P } ( i ) = \\{ \\hat { p } _ { i } \\} \\widehat { P } ( i ) = \\{ \\hat { p } _ { i } \\} untuk semua indek baris i i dengan 1 \\leq i _ { 1 } \\leq m _ { 1 } , \\ldots , 1 \\leq i _ { d } \\leq m _ { d } 1 \\leq i _ { 1 } \\leq m _ { 1 } , \\ldots , 1 \\leq i _ { d } \\leq m _ { d } Tabel contingency d d - dimensi dinyatakan dengan $$ N = n \\times \\hat { P } = { n _ { i } } \\text { untuk semua indek baris i, dengan } 1 \\leq i _ { 1 } \\leq m _ { 1 } , \\ldots , 1 \\leq i _ { d } \\leq m _ { d } $$ Tabel contingency ditambah dengan vektor hitung marjinal N_i N_i untuk semua d d atribut X $$ N _ { i } = n \\hat { p } _ { i } = \\left( \\begin{array} { c } { n _ { 1 } ^ { i } } \\ { \\vdots } \\ { n _ { m _ { i } } ^ { i } } \\end{array} \\right) $$ dimana \\hat p_i \\hat p_i adalah fungsi massa probabilitas empiris untuk X_i X_i Uji \\chi^2 \\chi^2 \u00b6 Kita dapat menguji untuk sebesar d d kebergantungan antara d d atribut kategorikal menggunakan hipotesa nol H_0 H_0 dengan d-way saling bebas. Hipotesa lain H_1 H_1 yang mana $ d$ cara yang tidak saling bebas. Sehingga atribut tersebut saling bergantung. Perhatikan bahwa analisa contingency d d dimensi menyatakan apakah semua atribut bersama sama saling bebas atau tidak. Umumnya kita harus melakukan k- k- cara analisa contingency untuk menguji apakah sebagaian dari k k atribut adalah saling bebas atau tidak. Terhadap hipotesa nol, jumlah harapan dari kejadian baris simbol ( a _ { 1 i _ { 1 } } , a _ { 2 i _ { 2 } } , \\ldots , a _ { d i _ { d } } ) ( a _ { 1 i _ { 1 } } , a _ { 2 i _ { 2 } } , \\ldots , a _ { d i _ { d } } ) dinyatakan dengan $$ e _ { i } = n \\cdot \\hat { p } _ { i } = n \\cdot \\prod _ { i = 1 } ^ { d } \\hat { p } _ { i j } ^ { j } = \\frac { n _ { i 1 } ^ { 1 } n _ { i _ { 2 } } ^ { 2 } \\cdots n _ { i _ { d } } ^ { d } } { n ^ { d - 1 } } $$ Ukuran statistik chi-squared mengukur selisih antara banyaknya pengamatan n_i n_i dan banyaknya yang diharapkan e_i e_i : $$ \\chi ^ { 2 } = \\sum _ { i } \\frac { ( n _ { i } - e _ { i } ) ^ { 2 } } { e _ { i } } = \\sum _ { i _ { 1 } = 1 } ^ { m _ { 1 } } \\sum _ { i _ { 2 } = 1 } ^ { m _ { 2 } } \\cdots \\sum _ { i _ { d } = 1 } ^ { m _ { d } } \\frac { ( n _ { i _ { 1 } , i _ { 2 } , \\ldots , i _ { d } } - e _ { i _ { 1 } , i _ { 2 } , \\ldots , i _ { d } } ) ^ { 2 } } { e _ { i _ { 1 } , i _ { 2 } \\ldots , i _ { d } } } $$ Statistik \\chi^2 \\chi^2 mengikuti fungsi padat chi-squared dengan derajat kebebasan q q Untuk tabel kontingensi d-cara kita dapat menghitung q q dengan mencatat bahwa seolah-olah ada \\prod _ { i = 1 } ^ { d } | d o m ( X _ { i } ) | \\prod _ { i = 1 } ^ { d } | d o m ( X _ { i } ) | parameter saling bebas (jumlah).Namun, kita harus menghapus \\prod _ { i = 1 } ^ { d } | d o m ( X _ { i } ) | \\prod _ { i = 1 } ^ { d } | d o m ( X _ { i } ) | derajat kebebasan karena jumlah marginal vektor masing masing dimensi X_i X_i harus sama dengn N_i N_i . Akan tetapi, dengan menghapus salah satu parameter d d kali, kita perlu menambahkan lagi d-1 d-1 ke jumlah parameter bebas. Jumlah total derajat kebebasan dinyatakan dengan $$ q = \\prod _ { i = 1 } ^ { d } | \\operatorname { dom } ( X _ { i } ) | - \\sum _ { i = 1 } ^ { d } | \\operatorname { dom } ( X _ { i } ) | + ( d - 1 ) $$ = \\left ( \\prod _ { i = 1 } ^ { d } m _ { i } \\right) - \\left( \\sum _ { i = 1 } ^ { d } m _ { i } \\right) + d - 1 = \\left ( \\prod _ { i = 1 } ^ { d } m _ { i } \\right) - \\left( \\sum _ { i = 1 } ^ { d } m _ { i } \\right) + d - 1 Untuk menolak hipotesa nol, kita harus mengecek apakah p p -value dari nilai \\chi^2 \\chi^2 pengamatan adalah lebih kecil dari tingkat signifikan yang diharapkan \\alpha \\alpha (katatakan \\alpha=0.01) \\alpha=0.01) menggukana pada chi-squared dengan q q derajat kebebasan (persamaan 3.16) Contoh Perhatikan tabel contingency 3-cara dalam gambar 3.4. Itu menunjukkan jumlah pengamatan untuk setiap baris (a_{1i},a_{2j},a_{3k}) (a_{1i},a_{2j},a_{3k}) untuk tiga atribut sepal length ( X_1) X_1) , sepal width ( X_2) X_2) dan class ( X_3 X_3 ). Dari jumlah tepi funtuk X_1 X_1 dan X_2 X_2 dalam tabel 3.5 dan fakta bahwa semua dari tiga nilai dari X_3 X_3 terjadi 50 kali, kita dapat menghitung jumlah harapan [persamaan 3.19] untu semua sel. Misalkan $$ e_{(4,1,1)}= \\frac {n 1_4.n 2_1.n 3_1}{150 2}=\\frac{45.47.50}{150.150}=4.7 $$ Banyaknya harapan adalah sama untuk semua dari tiga nilai daru X_3 X_3 dan dinyatakan dalam tabel 3.7. Nilai dari statistik \\chi^2 \\chi^2 [persamaan 3.20] dinyatakan dengan $$ \\chi^2 =231.06 $$ Dengan menggunakan persamaan (3.21) banyaknya derajat kebebasan dinyatakan dengan $$ q=3.3.3-(4+3+3)+2=36-10+2=28 $$ Dalam gambar 3.4 jumlah dalam bold adalah parameter terikat. Untuk semua jumlah lain adalah bebas. Dalam faktanya 8 sel berbeda telah dipilih sebagai parameter terikat. Untuk tingkat signifikan \\alpha =0.01 \\alpha =0.01 nilai kritis dari distribusi chi-square ada;aj z=48.28 z=48.28 . Nilai pengamatan dari \\chi^2=231.06 \\chi^2=231.06 lebh besar dari z z dan itu sangat tidak mungkin terjadi dibawah hipotesa nol. Kita dapat memberikan kesimpulan bahwa ketiga atribut adalah bukan 3 cara bebas tetapi saling berganting diantaranya. Jarak dan Sudut \u00b6 Dengan memodelkan atribut kategorikal sebagai variabel Bernolli multivariate, maka memungkinkan untuk menghitung jarak atau sudat antara dua titik x_i x_i dan x_j x_j : $$ x_i = \\left( \\begin{array} { c } { e _ { 1i_1 } } \\ { \\vdots } \\ { e _ { di_d } } \\end{array} \\right) \\space \\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space x_j = \\left( \\begin{array} { c } { e _ { 1j_1 } } \\ { \\vdots } \\ { e _ { dj_d } } \\end{array} \\right) $$ Berbagai ukuran jaran dan similaritas bergantung pada banyaknya nilai yang cocok dan tidak cocok (simbol ) diantara d d atribut X_k X_k . Misalkan, kita dapat menghingung banyaknya nilai yang cocok s s melalui perkalian titik $$ s = x _ { i } ^ { T } x _ { j } = \\sum _ { k = 1 } ^ { d } ( e _ { k i _ { k } } ) ^ { T } e _ { k j _ { k } } $$ Dengan kata lain, banyaknya yang tidak cocok adalah d-s d-s . Juga berguna norm dari setiap titik $$ | x _ { i } | ^ { 2 } = x _ { i } ^ { T } x _ { i } = d $$ Jarak Euclidian \u00b6 Jarak euclidian antara x_i x_i dan x_j x_j dinyatakan dengan $$ \\delta ( x _ { i } , x _ { j } ) = | x _ { i } - x _ { j } | = \\sqrt { x _ { i } ^ { T } x _ { i } - 2 x _ { i } x _ { j } + x _ { j } ^ { T } x _ { j } } = \\sqrt { 2 ( d - s ) } $$ Sehingga, maximum jarak Euclidian antara dua titik adalah \\sqrt{2d} \\sqrt{2d} yang terjadi bila tidak ada simbol umum diantaranya, maka s=0 s=0 Jarak Hamming \u00b6 Jarak Hamming diantara x_i x_i dan x_j x_j dinyatakan sebagai banyaknya dari nilai ketidakcocokan $$ \\delta _ { H } ( x _ { i } , x _ { j } ) = d - s = \\frac { 1 } { 2 } \\delta ( x _ { i } , x _ { j } ) ^ { 2 } $$ Jarak Hamming adalah kemudian ekivalen dengan setengah kuadrat jarak Euclidian Cosine Similarity \u00b6 Cosinus dari sudut antara x_i x_i dan x_j x_j dinyatakan dengan $$ \\operatorname { cos } \\theta = \\frac { x _ { i } ^ { T } x _ { j } } { | x _ { i } | \\cdot | x _ { j } | } = \\frac { s } { d } $$ Koefisien Jaccard \u00b6 Koefisien Jaccard (Jaccard Coefficient) adalah ukuran kesamaan yang umum digunakan antara dua titik kategori. Ini didefinisikan sebagai rasio dari jumlah nilai yang cocok dengan jumlah nilai berbeda yang muncul di kedua x_i x_i dan x_j x_j , dari semua atribut d d : $$ J ( x _ { i } , x _ { j } ) = \\frac { s } { 2 ( d - s ) + s } = \\frac { s } { 2 d - s } $$ dimana kita menggunakan pengamatan dimana ketika dua titik tidak cocok untuk dimensi k k . mereka berkontribis 2 ke banyaknya simbol yang berbeda , sebaliknya jika mereka cocok, jumlah simbol yang berbeda naik 1. Atas ketidakcocokan d - s dan kecocokan, jumlah simbol yang berbeda adalah 2 (d - s) + s 2 (d - s) + s . Contoh Perhatikan data kategorikal 3-dimensi dari contoh 3.11. Titik simbolik (Short, Medium, iris versicolor) dimodelkan sebagai vektor $$ x _ { 1 } = \\left( \\begin{array} { l } { e _ { 12 } } \\ { e _ { 22 } } \\ { e _ { 31 } } \\end{array} \\right) = ( 0,1,0,0 | 0,1,0 | 1,0,0 ) ^ { T } \\in \\mathbb R ^ { 10 } $$ dan titik simbol (veryShort, Medium, iris-setoso) dimodelkan dengan $$ x _ { 1 } = \\left( \\begin{array} { l } { e _ { 12 } } \\ { e _ { 22 } } \\ { e _ { 31 } } \\end{array} \\right) = ( 0,1,0,0 | 0,1,0 | 1,0,0 ) ^ { T } \\in R ^ { 10 } $$ Banyaknya simbol yang cocok dinyatakan dengan $$ s = x _ { 1 } ^ { T } x _ { 2 } = ( e _ { 12 } ) ^ { T } e _ { 11 } + ( e _ { 22 } ) ^ { T } e _ { 22 } + ( e _ { 31 } ) ^ { T } e _ { 32 } $$ = \\left( \\begin{array} { l l l l } { 0 } & { 1 } & { 0 } & { 0 } \\end{array} \\right) \\left( \\begin{array} { l } { 1 } \\\\ { 0 } \\\\ { 0 } \\\\ { 0 } \\end{array} \\right) + \\left( \\begin{array} { l l l } { 0 } & { 1 } & { 0 } \\end{array} \\right) \\left( \\begin{array} { l } { 0 } \\\\ { 1 } \\\\ { 0 } \\end{array} \\right) + \\left( \\begin{array} { l l l } { 1 } & { 0 } & { 0 } \\end{array} \\right) \\left( \\begin{array} { l } { 0 } \\\\ { 1 } \\\\ { 0 } \\end{array} \\right) =0+1+0=1 = \\left( \\begin{array} { l l l l } { 0 } & { 1 } & { 0 } & { 0 } \\end{array} \\right) \\left( \\begin{array} { l } { 1 } \\\\ { 0 } \\\\ { 0 } \\\\ { 0 } \\end{array} \\right) + \\left( \\begin{array} { l l l } { 0 } & { 1 } & { 0 } \\end{array} \\right) \\left( \\begin{array} { l } { 0 } \\\\ { 1 } \\\\ { 0 } \\end{array} \\right) + \\left( \\begin{array} { l l l } { 1 } & { 0 } & { 0 } \\end{array} \\right) \\left( \\begin{array} { l } { 0 } \\\\ { 1 } \\\\ { 0 } \\end{array} \\right) =0+1+0=1 Jarak Euclidian dan Jarak Hamming dinyatakan dengan $$ \\left. \\begin{array}{l}{ \\delta ( x _ { 1 } , x _ { 2 } ) = \\sqrt { 2 ( d - s ) } = \\sqrt { 2 \\cdot 2 } = \\sqrt { 4 } = 2 }\\{ \\delta _ { H } ( x _ { 1 } , x _ { 2 } ) = d - s = 3 - 1 = 2 }\\end{array} \\right. $$ Jarak cosinus dan jarak Jaccard dinyatakan dengan $$ \\left. \\begin{array}{l}{ \\operatorname { cos } \\theta = \\frac { s } { d } = \\frac { 1 } { 3 } = 0.333 }\\{ J ( x _ { 1 } , x _ { 2 } ) = \\frac { s } { 2 d - s } = \\frac { 1 } { 5 } = 0.2 }\\end{array} \\right. $$","title":"Eksplorasi data 4 analisa Multivariate"},{"location":"Eksplorasi%20data%204_analisa%20Multivariate/#analisa-multivariate","text":"Asumsikan bahwa dataset terdiri dari d d atribut kategorical X _ { j } ( 1 \\leq j \\leq d ) X _ { j } ( 1 \\leq j \\leq d ) dengan \\operatorname { dom } ( X _ { j } ) = \\{ a _ { j 1 } , a _ { j 2 } , \\ldots , a _ { j m j } \\} \\operatorname { dom } ( X _ { j } ) = \\{ a _ { j 1 } , a _ { j 2 } , \\ldots , a _ { j m j } \\} . Kita memiliki n n titik-titik kategorikal dari bentuk x_i= ( x _ { i1 } , x _ { i2 } , \\ldots , x _ { i d } ) ^ { T } x_i= ( x _ { i1 } , x _ { i2 } , \\ldots , x _ { i d } ) ^ { T } dengan x_{ij }\\in dom (X_j) x_{ij }\\in dom (X_j) . Datase kemudian dinayatakan dalam bentuk matrik ukuran n\\times d n\\times d $$ D = \\left( \\begin{array} { c c c c } { X _ { 1 } } & { X _ { 2 } } & { \\cdots } & { X _ { d } } \\ \\hline x _ { 11 } & { x _ { 12 } } & { \\cdots } & { x _ { 1 d } } \\ { x _ { 21 } } & { x _ { 22 } } & { \\cdots } & { x _ { 2 d } } \\ { \\vdots } & { \\vdots } & { \\ddots } & { \\vdots } \\ { x _ { n 1 } } & { x _ { n 2 } } & { \\cdots } & { x _ { n d } } \\end{array} \\right) $$ Setiap atribut X_i X_i dimodelkan sebagai m_i m_i dimensi multivariate variabel Bernoulli X_i X_i dan distribusi gabungannya dimodelkan sebagai d ^ { \\prime } = \\sum _ { j = 1 } ^ { d } m _ { j } d ^ { \\prime } = \\sum _ { j = 1 } ^ { d } m _ { j } variabel acak vektor dimensi $$ X = \\left( \\begin{array} { c } { X _ { 1 } } \\ { \\vdots } \\ { X _ { d } } \\end{array} \\right) $$ Setiap titik data categorikal v = ( v _ { 1 } , v _ { 2 } , \\ldots , v _ { d } ) ^ { T } v = ( v _ { 1 } , v _ { 2 } , \\ldots , v _ { d } ) ^ { T } kemudian dinyatakan sebagai vektor biner d d dimensi $$ X ( v ) = \\left( \\begin{array} { c } { X _ { 1 } ( v _ { 1 } ) } \\ { \\vdots } \\ { X _ { d } ( v _ { d } ) } \\end{array} \\right) = \\left( \\begin{array} { c } { e _ { 1 k _ { 1 } } } \\ { \\vdots } \\ { e _ { d k _ { d } } } \\end{array} \\right) $$ dengan v_i=a_{ikj} v_i=a_{ikj} simbol ke k_i k_i dari X_i X_i . Disini e_{ikj} e_{ikj} adalah vektor basis standr dalam $\\mathbb R^{m_i} $","title":"Analisa Multivariate"},{"location":"Eksplorasi%20data%204_analisa%20Multivariate/#mean","text":"Mengeneralisasi dari kasus bivariate, mea dan sample mean untuk X X dinyatakan dengan $$ \\mu = E [ X ] = \\left( \\begin{array} { c } { \\mu _ { 1 } } \\ { \\vdots } \\ { \\mu _ { d } } \\end{array} \\right) = \\left( \\begin{array} { c } { p _ { 1 } } \\ { \\vdots } \\ { p _ { d } } \\end{array} \\right) \\quad \\hat { \\mu } = \\left( \\begin{array} { c } { \\hat { \\mu } _ { 1 } } \\ { \\vdots } \\ { \\hat { \\mu } _ { d } } \\end{array} \\right) = \\left( \\begin{array} { c } { \\hat { p } _ { 1 } } \\ { \\vdots } \\ { \\hat { p } _ { d } } \\end{array} \\right) $$ dimana p _ { i } = ( p _ { 1 } ^ { i } , \\ldots , p _ { m _ { i } } ^ { i } ) ^ { T } p _ { i } = ( p _ { 1 } ^ { i } , \\ldots , p _ { m _ { i } } ^ { i } ) ^ { T } adalah fungsi massa probabilitas (PMF) untuk X_i X_i dan \\hat { p } _ { i } = ( \\hat { p } _ { 1 } ^ { i } , \\ldots , \\hat { p } _ { m _ { i } } ^ { i } ) ^ { T } \\hat { p } _ { i } = ( \\hat { p } _ { 1 } ^ { i } , \\ldots , \\hat { p } _ { m _ { i } } ^ { i } ) ^ { T } adalah fungsi massa probabilitas empiris untuk X_i X_i","title":"Mean"},{"location":"Eksplorasi%20data%204_analisa%20Multivariate/#matrik-covariance","text":"Matrik kovarian untuk X X dan estimasi dari sample diberikan dengan matrik d' \\times d' d' \\times d' $$ \\Sigma = \\left( \\begin{array} { c c c c } { \\Sigma _ { 11 } } & { \\Sigma _ { 12 } } & { \\cdots } & { \\Sigma _ { 1 d } } \\ { \\Sigma _ { 12 } ^ { T } } & { \\Sigma _ { 22 } } & { \\cdots } & { \\Sigma _ { 2 d } } \\ { \\cdots } & { \\cdots } & { \\ddots } & { \\cdots } \\ { \\Sigma _ { 1 d } ^ { T } } & { \\Sigma _ { 2 d } ^ { T } } & { \\cdots } & { \\Sigma _ { d d } } \\end{array} \\right) \\hat { \\Sigma } = \\left( \\begin{array} { c c c c } { \\hat { \\Sigma } _ { 11 } } & { \\hat { \\Sigma } _ { 12 } } & { \\cdots } & { \\hat { \\Sigma } _ { 1 d } } \\ { \\hat { \\Sigma } _ { 12 } ^ { T } } & { \\hat { \\Sigma } _ { 22 } } & { \\cdots } & { \\hat { \\Sigma } _ { 2 d } } \\ { \\ldots } & { \\ldots } & { \\ddots } & { \\ldots } \\ { \\hat { \\Sigma } _ { 1 d } ^ { T } } & { \\hat { \\Sigma } _ { 2 d } ^ { T } } & { \\cdots } & { \\hat { \\Sigma } _ { d d } } \\end{array} \\right) $$ dimana d ^ { \\prime } = \\sum _ { i = 1 } ^ { d } m _ { i } d ^ { \\prime } = \\sum _ { i = 1 } ^ { d } m _ { i } dan \\Sigma_{ij} \\Sigma_{ij} dam \\hat {\\Sigma}_{ij} \\hat {\\Sigma}_{ij} adalah m_i \\times m_j m_i \\times m_j matrik kovarian (dan estimasinya ) untuk atribut X_i X_i dan X_j X_j : $$ \\Sigma _ { i j } = P _ { i j } - p _ { i } p _ { j } ^ { T } \\quad \\hat { \\Sigma } _ { i j } = \\hat { P } _ { i j } - \\hat { p } _ { i } \\hat { p } _ { j } ^ { T } $$ Disini P_{ij} P_{ij} adalah fungsi massa probabilitas gabungan dan \\hat {P}_{ij} \\hat {P}_{ij} adalah fungsi massa probabilitas empiris yang dapat dihitung menggunakan persamaan (3.13) Contoh 3.11. Analisa Multivariate . Perhatikan subset dari dataset 3-dimensi dengan atribut yang telah didiskritisasi, sepal length ( X_1 X_1 ) dan sepal width ( X_2 X_2 ) dan atribut kategorikal class ( X_3 X_3 ). Domain untuk X_1 X_1 dan X_2 X_2 diberikan dalam tabel 3.1 dan tabel 3.3. dan dom (X_3)=(iris-versicolor, iris-setosa,iris-verginica) dom (X_3)=(iris-versicolor, iris-setosa,iris-verginica) . Setiap nilai dari X_3 X_3 terjadi 50 kali. Titik x categorical =(Short, Medium, iris-versicolor) dimodelkan sebagai vektor $$ X ( x ) = \\left( \\begin{array} { l } { e _ { 12 } } \\ { e _ { 22 } } \\ { e _ { 31 } } \\end{array} \\right) = ( 0,1,0,0 | 0,1,0 | 1,0,0 ) ^ { T } \\in \\mathbb R ^ { 10 } $$ Dari contoh 3.8 dan fakta bahwa setiap nilai dalam dom(X_3) dom(X_3) terjadi 50m kali dalam sample n=50 n=50 mean sample untuk tiga atribut ini dinyatakan dengan $$ \\hat { \\mu } = \\left( \\begin{array} { l } { \\hat { \\mu } _ { 1 } } \\ { \\hat { \\mu } _ { 2 } } \\ { \\hat { \\mu } _ { 3 } } \\end{array} \\right) = \\left( \\begin{array} { l } { \\hat { p } _ { 1 } } \\ { \\hat { p } _ { 2 } } \\ { \\hat { p } _ { 3 } } \\end{array} \\right) = ( 0.3,0.333,0.287,0.08 | 0.313,0.587,0.1 | 0.33,0.33,0.33 ) ^ { T } $$ Menggunakan \\hat { p } _ { 3 } = ( 0.33,0.33,0.33 ) ^ { T } \\hat { p } _ { 3 } = ( 0.33,0.33,0.33 ) ^ { T } , kita dapat menghitung matrik covariance sample untuk X_3 X_3 menggunakan persamaan (3.9) $$ \\hat { \\Sigma } _ { 33 } = \\left( \\begin{array} { r r r } { 0.222 } & { - 0.111 } & { - 0.111 } \\ { - 0.111 } & { 0.222 } & { - 0.111 } \\ { - 0.111 } & { - 0.111 } & { 0.222 } \\end{array} \\right) $$ Menggunakan persamaan (3.18) kita peroleh $$ \\hat { \\Sigma } _ { 13 } = \\left( \\begin{array} { r r r } { - 0.067 } & { 0.16 } & { - 0.093 } \\ { 0.082 } & { - 0.038 } & { - 0.044 } \\ { 0.011 } & { - 0.096 } & { 0.084 } \\ { - 0.027 } & { - 0.027 } & { 0.053 } \\end{array} \\right) $$ \\hat { \\Sigma } _ { 23 } = \\left( \\begin{array} { r r r } { 0.076 } & { - 0.098 } & { 0.022 } \\\\ { - 0.042 } & { 0.044 } & { - 0.002 } \\\\ { - 0.033 } & { 0.053 } & { - 0.02 } \\end{array} \\right) \\hat { \\Sigma } _ { 23 } = \\left( \\begin{array} { r r r } { 0.076 } & { - 0.098 } & { 0.022 } \\\\ { - 0.042 } & { 0.044 } & { - 0.002 } \\\\ { - 0.033 } & { 0.053 } & { - 0.02 } \\end{array} \\right) Dikombinasikan dengan \\widehat{\\Sigma}_{11} \\widehat{\\Sigma}_{11} , \\widehat{\\Sigma}_{22} \\widehat{\\Sigma}_{22} dan \\widehat{\\Sigma}_{12} \\widehat{\\Sigma}_{12} dari persamaan 3.8 matrik covarian sample akhir adalah matrik simetris 10 \\times 10 10 \\times 10 dinyatakan dengan $$ \\hat { \\Sigma } = \\left( \\begin{array} { l l l } { \\hat { \\Sigma } _ { 11 } } & { \\hat { \\Sigma } _ { 12 } } & { \\hat { \\Sigma } _ { 13 } } \\ { \\hat { \\Sigma } _ { 12 } ^ { T } } & { \\hat { \\Sigma } _ { 22 } } & { \\hat { \\Sigma } _ { 23 } } \\ { \\hat { \\Sigma } _ { 13 } ^ { 7 } } & { \\hat { \\Sigma } _ { 23 } ^ { T } } & { \\hat { \\Sigma } _ { 33 } } \\end{array} \\right) $$","title":"Matrik Covariance"},{"location":"Eksplorasi%20data%204_analisa%20Multivariate/#analisa-contingency-multiway","text":"Untuk analisa kebergantungan multiway, kita pertama kali harus menentukan fungsi massa probabilitas empiris untuk X X : $$ \\hat { f } ( e _ { 1 i _ { 1 } } , e _ { 2 i _ { 2 } } , \\ldots , e _ { d i _ { d } } ) = \\frac { 1 } { n } \\sum _ { k = 1 } ^ { n } I _ { i _ { 1 } i _ { 2 } , \\ldots _ { d } } ( x _ { k } ) = \\frac { n _ { i _ { 1 } i _ { 2 } \\ldots i _ { d } } } { n } = \\hat { p } _ { i _ { 1 } i _ { 2 } \\ldots i _ { d } } $$ dimana I _ { i _ { 1 } i _ { 2 } \\ldots l _ { d } } I _ { i _ { 1 } i _ { 2 } \\ldots l _ { d } } adalah variabel indikator $$ I _ { i _ { 1 } i _ { 2 } \\ldots i _ { d } } ( x _ { k } ) = \\left{ \\begin{array} { l l } { 1 } & { \\text { jika } x _ { k 1 } = e _ { 1 i _ { 1 } } , x _ { k 2 } = e _ { 2 i _ { 2 } } , \\ldots , x _ { k d } = e _ { d i _ { d } } } \\ { 0 } & { \\text { lainnya } } \\end{array} \\right. $$ Jumlah dari I _ { i _ { 1 } i _ { 2 } \\ldots i _ { d } } I _ { i _ { 1 } i _ { 2 } \\ldots i _ { d } } terhadap semua n n titik dalam sample menghasilkan jumlah kejadian n _ { i _ { 1 } i _ { 2 } \\ldots i _ { d } } n _ { i _ { 1 } i _ { 2 } \\ldots i _ { d } } dari simbol vektor ( a _ { 1 i _ { 1 } } , a _ { 2 i _ { i } } , \\ldots , a _ { d i _ { d } } ) ( a _ { 1 i _ { 1 } } , a _ { 2 i _ { i } } , \\ldots , a _ { d i _ { d } } ) . Membagi kejadian dengan ukuran sample menghasilkan dalam probabilitas dari simbol simbol pengamatan. Dengan menggunakan notasi i = ( i _ { 1 } , i _ { 2 } , \\ldots , i _ { d } ) i = ( i _ { 1 } , i _ { 2 } , \\ldots , i _ { d } ) untuk menotasikan index baris, kita dapat menulis fungsi massa probabilitas empiris gabungan sebagai matrik d- d- dimensi \\widehat P \\widehat P dari ukuran m_1 \\times m_2 \\times ...\\times m_d= \\prod _ { i = 1 } ^ { d } m _ { i } m_1 \\times m_2 \\times ...\\times m_d= \\prod _ { i = 1 } ^ { d } m _ { i } dinyatakan dengan \\widehat { P } ( i ) = \\{ \\hat { p } _ { i } \\} \\widehat { P } ( i ) = \\{ \\hat { p } _ { i } \\} untuk semua indek baris i i dengan 1 \\leq i _ { 1 } \\leq m _ { 1 } , \\ldots , 1 \\leq i _ { d } \\leq m _ { d } 1 \\leq i _ { 1 } \\leq m _ { 1 } , \\ldots , 1 \\leq i _ { d } \\leq m _ { d } Tabel contingency d d - dimensi dinyatakan dengan $$ N = n \\times \\hat { P } = { n _ { i } } \\text { untuk semua indek baris i, dengan } 1 \\leq i _ { 1 } \\leq m _ { 1 } , \\ldots , 1 \\leq i _ { d } \\leq m _ { d } $$ Tabel contingency ditambah dengan vektor hitung marjinal N_i N_i untuk semua d d atribut X $$ N _ { i } = n \\hat { p } _ { i } = \\left( \\begin{array} { c } { n _ { 1 } ^ { i } } \\ { \\vdots } \\ { n _ { m _ { i } } ^ { i } } \\end{array} \\right) $$ dimana \\hat p_i \\hat p_i adalah fungsi massa probabilitas empiris untuk X_i X_i","title":"Analisa Contingency Multiway"},{"location":"Eksplorasi%20data%204_analisa%20Multivariate/#uji-chi2chi2","text":"Kita dapat menguji untuk sebesar d d kebergantungan antara d d atribut kategorikal menggunakan hipotesa nol H_0 H_0 dengan d-way saling bebas. Hipotesa lain H_1 H_1 yang mana $ d$ cara yang tidak saling bebas. Sehingga atribut tersebut saling bergantung. Perhatikan bahwa analisa contingency d d dimensi menyatakan apakah semua atribut bersama sama saling bebas atau tidak. Umumnya kita harus melakukan k- k- cara analisa contingency untuk menguji apakah sebagaian dari k k atribut adalah saling bebas atau tidak. Terhadap hipotesa nol, jumlah harapan dari kejadian baris simbol ( a _ { 1 i _ { 1 } } , a _ { 2 i _ { 2 } } , \\ldots , a _ { d i _ { d } } ) ( a _ { 1 i _ { 1 } } , a _ { 2 i _ { 2 } } , \\ldots , a _ { d i _ { d } } ) dinyatakan dengan $$ e _ { i } = n \\cdot \\hat { p } _ { i } = n \\cdot \\prod _ { i = 1 } ^ { d } \\hat { p } _ { i j } ^ { j } = \\frac { n _ { i 1 } ^ { 1 } n _ { i _ { 2 } } ^ { 2 } \\cdots n _ { i _ { d } } ^ { d } } { n ^ { d - 1 } } $$ Ukuran statistik chi-squared mengukur selisih antara banyaknya pengamatan n_i n_i dan banyaknya yang diharapkan e_i e_i : $$ \\chi ^ { 2 } = \\sum _ { i } \\frac { ( n _ { i } - e _ { i } ) ^ { 2 } } { e _ { i } } = \\sum _ { i _ { 1 } = 1 } ^ { m _ { 1 } } \\sum _ { i _ { 2 } = 1 } ^ { m _ { 2 } } \\cdots \\sum _ { i _ { d } = 1 } ^ { m _ { d } } \\frac { ( n _ { i _ { 1 } , i _ { 2 } , \\ldots , i _ { d } } - e _ { i _ { 1 } , i _ { 2 } , \\ldots , i _ { d } } ) ^ { 2 } } { e _ { i _ { 1 } , i _ { 2 } \\ldots , i _ { d } } } $$ Statistik \\chi^2 \\chi^2 mengikuti fungsi padat chi-squared dengan derajat kebebasan q q Untuk tabel kontingensi d-cara kita dapat menghitung q q dengan mencatat bahwa seolah-olah ada \\prod _ { i = 1 } ^ { d } | d o m ( X _ { i } ) | \\prod _ { i = 1 } ^ { d } | d o m ( X _ { i } ) | parameter saling bebas (jumlah).Namun, kita harus menghapus \\prod _ { i = 1 } ^ { d } | d o m ( X _ { i } ) | \\prod _ { i = 1 } ^ { d } | d o m ( X _ { i } ) | derajat kebebasan karena jumlah marginal vektor masing masing dimensi X_i X_i harus sama dengn N_i N_i . Akan tetapi, dengan menghapus salah satu parameter d d kali, kita perlu menambahkan lagi d-1 d-1 ke jumlah parameter bebas. Jumlah total derajat kebebasan dinyatakan dengan $$ q = \\prod _ { i = 1 } ^ { d } | \\operatorname { dom } ( X _ { i } ) | - \\sum _ { i = 1 } ^ { d } | \\operatorname { dom } ( X _ { i } ) | + ( d - 1 ) $$ = \\left ( \\prod _ { i = 1 } ^ { d } m _ { i } \\right) - \\left( \\sum _ { i = 1 } ^ { d } m _ { i } \\right) + d - 1 = \\left ( \\prod _ { i = 1 } ^ { d } m _ { i } \\right) - \\left( \\sum _ { i = 1 } ^ { d } m _ { i } \\right) + d - 1 Untuk menolak hipotesa nol, kita harus mengecek apakah p p -value dari nilai \\chi^2 \\chi^2 pengamatan adalah lebih kecil dari tingkat signifikan yang diharapkan \\alpha \\alpha (katatakan \\alpha=0.01) \\alpha=0.01) menggukana pada chi-squared dengan q q derajat kebebasan (persamaan 3.16) Contoh Perhatikan tabel contingency 3-cara dalam gambar 3.4. Itu menunjukkan jumlah pengamatan untuk setiap baris (a_{1i},a_{2j},a_{3k}) (a_{1i},a_{2j},a_{3k}) untuk tiga atribut sepal length ( X_1) X_1) , sepal width ( X_2) X_2) dan class ( X_3 X_3 ). Dari jumlah tepi funtuk X_1 X_1 dan X_2 X_2 dalam tabel 3.5 dan fakta bahwa semua dari tiga nilai dari X_3 X_3 terjadi 50 kali, kita dapat menghitung jumlah harapan [persamaan 3.19] untu semua sel. Misalkan $$ e_{(4,1,1)}= \\frac {n 1_4.n 2_1.n 3_1}{150 2}=\\frac{45.47.50}{150.150}=4.7 $$ Banyaknya harapan adalah sama untuk semua dari tiga nilai daru X_3 X_3 dan dinyatakan dalam tabel 3.7. Nilai dari statistik \\chi^2 \\chi^2 [persamaan 3.20] dinyatakan dengan $$ \\chi^2 =231.06 $$ Dengan menggunakan persamaan (3.21) banyaknya derajat kebebasan dinyatakan dengan $$ q=3.3.3-(4+3+3)+2=36-10+2=28 $$ Dalam gambar 3.4 jumlah dalam bold adalah parameter terikat. Untuk semua jumlah lain adalah bebas. Dalam faktanya 8 sel berbeda telah dipilih sebagai parameter terikat. Untuk tingkat signifikan \\alpha =0.01 \\alpha =0.01 nilai kritis dari distribusi chi-square ada;aj z=48.28 z=48.28 . Nilai pengamatan dari \\chi^2=231.06 \\chi^2=231.06 lebh besar dari z z dan itu sangat tidak mungkin terjadi dibawah hipotesa nol. Kita dapat memberikan kesimpulan bahwa ketiga atribut adalah bukan 3 cara bebas tetapi saling berganting diantaranya.","title":"Uji \\chi^2\\chi^2"},{"location":"Eksplorasi%20data%204_analisa%20Multivariate/#jarak-dan-sudut","text":"Dengan memodelkan atribut kategorikal sebagai variabel Bernolli multivariate, maka memungkinkan untuk menghitung jarak atau sudat antara dua titik x_i x_i dan x_j x_j : $$ x_i = \\left( \\begin{array} { c } { e _ { 1i_1 } } \\ { \\vdots } \\ { e _ { di_d } } \\end{array} \\right) \\space \\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space x_j = \\left( \\begin{array} { c } { e _ { 1j_1 } } \\ { \\vdots } \\ { e _ { dj_d } } \\end{array} \\right) $$ Berbagai ukuran jaran dan similaritas bergantung pada banyaknya nilai yang cocok dan tidak cocok (simbol ) diantara d d atribut X_k X_k . Misalkan, kita dapat menghingung banyaknya nilai yang cocok s s melalui perkalian titik $$ s = x _ { i } ^ { T } x _ { j } = \\sum _ { k = 1 } ^ { d } ( e _ { k i _ { k } } ) ^ { T } e _ { k j _ { k } } $$ Dengan kata lain, banyaknya yang tidak cocok adalah d-s d-s . Juga berguna norm dari setiap titik $$ | x _ { i } | ^ { 2 } = x _ { i } ^ { T } x _ { i } = d $$","title":"Jarak dan Sudut"},{"location":"Eksplorasi%20data%204_analisa%20Multivariate/#jarak-euclidian","text":"Jarak euclidian antara x_i x_i dan x_j x_j dinyatakan dengan $$ \\delta ( x _ { i } , x _ { j } ) = | x _ { i } - x _ { j } | = \\sqrt { x _ { i } ^ { T } x _ { i } - 2 x _ { i } x _ { j } + x _ { j } ^ { T } x _ { j } } = \\sqrt { 2 ( d - s ) } $$ Sehingga, maximum jarak Euclidian antara dua titik adalah \\sqrt{2d} \\sqrt{2d} yang terjadi bila tidak ada simbol umum diantaranya, maka s=0 s=0","title":"Jarak Euclidian"},{"location":"Eksplorasi%20data%204_analisa%20Multivariate/#jarak-hamming","text":"Jarak Hamming diantara x_i x_i dan x_j x_j dinyatakan sebagai banyaknya dari nilai ketidakcocokan $$ \\delta _ { H } ( x _ { i } , x _ { j } ) = d - s = \\frac { 1 } { 2 } \\delta ( x _ { i } , x _ { j } ) ^ { 2 } $$ Jarak Hamming adalah kemudian ekivalen dengan setengah kuadrat jarak Euclidian","title":"Jarak Hamming"},{"location":"Eksplorasi%20data%204_analisa%20Multivariate/#cosine-similarity","text":"Cosinus dari sudut antara x_i x_i dan x_j x_j dinyatakan dengan $$ \\operatorname { cos } \\theta = \\frac { x _ { i } ^ { T } x _ { j } } { | x _ { i } | \\cdot | x _ { j } | } = \\frac { s } { d } $$","title":"Cosine Similarity"},{"location":"Eksplorasi%20data%204_analisa%20Multivariate/#koefisien-jaccard","text":"Koefisien Jaccard (Jaccard Coefficient) adalah ukuran kesamaan yang umum digunakan antara dua titik kategori. Ini didefinisikan sebagai rasio dari jumlah nilai yang cocok dengan jumlah nilai berbeda yang muncul di kedua x_i x_i dan x_j x_j , dari semua atribut d d : $$ J ( x _ { i } , x _ { j } ) = \\frac { s } { 2 ( d - s ) + s } = \\frac { s } { 2 d - s } $$ dimana kita menggunakan pengamatan dimana ketika dua titik tidak cocok untuk dimensi k k . mereka berkontribis 2 ke banyaknya simbol yang berbeda , sebaliknya jika mereka cocok, jumlah simbol yang berbeda naik 1. Atas ketidakcocokan d - s dan kecocokan, jumlah simbol yang berbeda adalah 2 (d - s) + s 2 (d - s) + s . Contoh Perhatikan data kategorikal 3-dimensi dari contoh 3.11. Titik simbolik (Short, Medium, iris versicolor) dimodelkan sebagai vektor $$ x _ { 1 } = \\left( \\begin{array} { l } { e _ { 12 } } \\ { e _ { 22 } } \\ { e _ { 31 } } \\end{array} \\right) = ( 0,1,0,0 | 0,1,0 | 1,0,0 ) ^ { T } \\in \\mathbb R ^ { 10 } $$ dan titik simbol (veryShort, Medium, iris-setoso) dimodelkan dengan $$ x _ { 1 } = \\left( \\begin{array} { l } { e _ { 12 } } \\ { e _ { 22 } } \\ { e _ { 31 } } \\end{array} \\right) = ( 0,1,0,0 | 0,1,0 | 1,0,0 ) ^ { T } \\in R ^ { 10 } $$ Banyaknya simbol yang cocok dinyatakan dengan $$ s = x _ { 1 } ^ { T } x _ { 2 } = ( e _ { 12 } ) ^ { T } e _ { 11 } + ( e _ { 22 } ) ^ { T } e _ { 22 } + ( e _ { 31 } ) ^ { T } e _ { 32 } $$ = \\left( \\begin{array} { l l l l } { 0 } & { 1 } & { 0 } & { 0 } \\end{array} \\right) \\left( \\begin{array} { l } { 1 } \\\\ { 0 } \\\\ { 0 } \\\\ { 0 } \\end{array} \\right) + \\left( \\begin{array} { l l l } { 0 } & { 1 } & { 0 } \\end{array} \\right) \\left( \\begin{array} { l } { 0 } \\\\ { 1 } \\\\ { 0 } \\end{array} \\right) + \\left( \\begin{array} { l l l } { 1 } & { 0 } & { 0 } \\end{array} \\right) \\left( \\begin{array} { l } { 0 } \\\\ { 1 } \\\\ { 0 } \\end{array} \\right) =0+1+0=1 = \\left( \\begin{array} { l l l l } { 0 } & { 1 } & { 0 } & { 0 } \\end{array} \\right) \\left( \\begin{array} { l } { 1 } \\\\ { 0 } \\\\ { 0 } \\\\ { 0 } \\end{array} \\right) + \\left( \\begin{array} { l l l } { 0 } & { 1 } & { 0 } \\end{array} \\right) \\left( \\begin{array} { l } { 0 } \\\\ { 1 } \\\\ { 0 } \\end{array} \\right) + \\left( \\begin{array} { l l l } { 1 } & { 0 } & { 0 } \\end{array} \\right) \\left( \\begin{array} { l } { 0 } \\\\ { 1 } \\\\ { 0 } \\end{array} \\right) =0+1+0=1 Jarak Euclidian dan Jarak Hamming dinyatakan dengan $$ \\left. \\begin{array}{l}{ \\delta ( x _ { 1 } , x _ { 2 } ) = \\sqrt { 2 ( d - s ) } = \\sqrt { 2 \\cdot 2 } = \\sqrt { 4 } = 2 }\\{ \\delta _ { H } ( x _ { 1 } , x _ { 2 } ) = d - s = 3 - 1 = 2 }\\end{array} \\right. $$ Jarak cosinus dan jarak Jaccard dinyatakan dengan $$ \\left. \\begin{array}{l}{ \\operatorname { cos } \\theta = \\frac { s } { d } = \\frac { 1 } { 3 } = 0.333 }\\{ J ( x _ { 1 } , x _ { 2 } ) = \\frac { s } { 2 d - s } = \\frac { 1 } { 5 } = 0.2 }\\end{array} \\right. $$","title":"Koefisien Jaccard"},{"location":"Eksplorasi%20data-2/","text":"Sampel Mean Disumsikan bahwa setiap titk simbol x_i \\in D x_i \\in D dipetakan ke variabel x_i=X(x_i) x_i=X(x_i) . Data yang telah dipetakan x_1,x_2,....x_n x_1,x_2,....x_n adalah kemudian diasumsikan sampel acak IID dengan X X . Kita dapat menghitung sampel mean dengan menempatkan massa proabilitas dari $ \\frac {1}{n}$ pada setiap titik $$ \\hat { \\mu } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } = \\sum _ { i = 1 } ^ { m } \\frac { n _ { i } } { n } e _ { i } = \\left( \\begin{array} { c } { n _ { 1 } / n } \\ { n _ { 2 } / n } \\ { \\vdots } \\ { n _ { m } / n } \\end{array} \\right) = \\left( \\begin{array} { c } { \\hat { p } _ { 1 } } \\ { \\hat { p } _ { 2 } } \\ { \\vdots } \\ { \\hat { p } _ { m } } \\end{array} \\right) = \\hat { p } $$ dimana n_i n_i adalah banyaknya kejadian dari nilai vektor e_i e_i dalam sampel, yang ekivalen dengan banyaknya kejadian dari simbol a_i a_i . Selanjutnya, kita memiliki \\sum_{i=1}^m n_i=n \\sum_{i=1}^m n_i=n , yang mengikuti dari fakta bahwa X X hanya dapat diperoleh pada m m yang berbeda e_i e_i dan perhitungan setiap nilai haru ditambahkan hingga ke ukuran sampel n n Contoh3.4. Sampel Mean . Perthatikan jumlah yang diamati untuk setiap nilai a_i a_i (e_i) (e_i) dari diskritisasi atribut sepal length dalam tabel 3.1. Karena jumlah sampel adalah n=150 n=150 , dari sini kita dapat estimasi \\hat p_i \\hat p_i sebagai berikut $$ \\left. \\begin{array} { l } { \\hat { p } _ { 1 } = 45 / 150 = 0.3 } \\ { \\hat { p } _ { 2 } = 50 / 150 = 0.333 } \\ { \\hat { p } _ { 3 } = 43 / 150 = 0.287 } \\ { \\hat { p } _ { 4 } = 12 / 150 = 0.08 } \\end{array} \\right. $$ Fungsi Massa Probabilias diplot dalam gambar 3.1 dan sample mean untuk X dinyatakan dengan $$ \\hat { \\mu } = \\hat { p } = \\left( \\begin{array} { c } { 0.3 } \\ { 0.333 } \\ { 0.287 } \\ { 0.08 } \\end{array} \\right) $$ Matrik Covarian Perhatikan lagi bahwa m m -dimensi variabel multivariate Bernouli adalah sederhananya vektor dari m m variabel Bernoulli. Misalkan X=(A_1,A_2,...A_m)^T X=(A_1,A_2,...A_m)^T dimana A_i A_i adalah variabel Bernoulli yang terkait dengan simbol a_i a_i . Informasi variansi covarian antara unsur-unsur variabel Bernoully yang menghasilkan matrik untuk X X Marilah kita pertama kita perhatikan variansi dari setiap variabel Bernoulli A_i A_i . Dengan persamaan (3.1),kita segera memiliki $$ \\sigma _ { i } ^ { 2 } = \\operatorname { var } ( A _ { i } ) = p _ { i } ( 1 - p _ { i } ) $$ Berikutnya perhatikan covariasi antara A_1 A_1 dan A_j A_j . Dengan memanfaatkan identitas (2.21) kita miliki $$ \\sigma _ { i j } = E [ A _ { i } A _ { j } ] - E [ A _ { i } ] \\cdot E [ A _ { j } ] = 0 - p _ { i } p _ { j } = - p _ { i } p _ { j } $$ yang mengikuti dari fakta bahwa E[A_iA_j]=0 E[A_iA_j]=0 sehingga A_1 A_1 dan A_2 A_2 keduanya tidak sama dengan 1 dan kemudian perkalian A_iA_j=0 A_iA_j=0 . Fakta yang sama ini terkait dengan relasi negatif antara A_i A_i dan A_j A_j . Yang menarik adalah bahwa derajat keterkaitan negatif adalah proporsional pada perkalian dari nilai mean A_i A_i dan A_j A_j . Dari eskperesi sebelumnya untuk varian dan covarian, m\\times m m\\times m matrik covarian untuk X X dinyatakan dengan $$ \\Sigma = \\left( \\begin{array} { c c c c } { \\sigma _ { 1 } ^ { 2 } } & { \\sigma _ { 12 } } & { \\dots } & { \\sigma _ { 1 m } } \\ { \\sigma _ { 12 } } & { \\sigma _ { 2 } ^ { 2 } } & { \\dots } & { \\sigma _ { 2 m } } \\ { \\vdots } & { \\vdots } & { \\ddots } & { \\vdots } \\ { \\sigma _ { 1 m } } & { \\sigma _ { 2 m } } & { \\dots } & { \\sigma _ { m } ^ { 2 } } \\end{array} \\right) = \\left( \\begin{array} { c c c c } { p _ { 1 } ( 1 - p _ { 1 } ) } & { - p _ { 1 } p _ { 2 } } & { \\dots } & { - p _ { 1 } p _ { m } } \\ { - p _ { 1 } p _ { 2 } } & { p _ { 2 } ( 1 - p _ { 2 } ) } & { \\dots } & { - p _ { 2 } p _ { m } } \\ { \\vdots } & { \\vdots } & { \\ddots } & { \\vdots } \\ { - p _ { 1 } p _ { m } } & { - p _ { 2 } p _ { m } } & { \\cdots } & { p _ { m } ( 1 - p _ { m } ) } \\end{array} \\right) $$ Perhatikan bagaimana setiap baris dalam \\Sigma \\Sigma adalah nol. Misalkan, untuk baris i i kita punya $$ - p _ { i } p _ { 1 } - p _ { i } p _ { 2 } - \\cdots + p _ { i } ( 1 - p _ { i } ) - \\cdots - p _ { i } p _ { m } = p _ { i } - p _ { i } \\sum _ { l = 1 } ^ { m } p _ { j } = p _ { i } - p _ { i } = 0 $$ Karna \\Sigma \\Sigma adalah simetris, maka memungkinkan setiap kolom jumlahnya adalah nol. Definisi P sebagai m\\times m m\\times m matrik diagonal: $$ P = \\operatorname { diag } ( p ) = \\operatorname { diag } ( p _ { 1 } , p _ { 2 } , \\ldots , p _ { m } ) = \\left( \\begin{array} { c c c c } { p _ { 1 } } & { 0 } & { \\cdots } & { 0 } \\ { 0 } & { p _ { 2 } } & { \\cdots } & { 0 } \\ { \\vdots } & { \\vdots } & { \\ddots } & { \\vdots } \\ { 0 } & { 0 } & { \\cdots } & { p _ { m } } \\end{array} \\right) $$ Kita dapat menulis matrik kovarian X X dengan $$ \\Sigma = P - p \\cdot p ^ { T } $$ Matrik Kovarian Sampel Matrik kovarian sample dapat diperoleh dari (3.8) dengan jelas yaitu $$ \\hat { \\Sigma } = \\hat { P } - \\hat { p } \\cdot \\hat { p } ^ { T }dimana \\hat P=diag(\\hat p) \\hat P=diag(\\hat p) dan $\\hat p=\\hat \\mu =(hat) $$ dimana \\hat P=diag(\\hat p) \\hat P=diag(\\hat p) dan \\hat p=\\hat \\mu =(\\hat p_1,\\hat p_2,...\\hat p_m) \\hat p=\\hat \\mu =(\\hat p_1,\\hat p_2,...\\hat p_m) menyatakan fungsi massa k probabilitas empiris untuk X X . Contoh. Dari hasil diskritisasi atribut sepal length dalam contoh 3.4 kita telah memiliki $ \\hat { \\mu } = \\hat { p } = ( 0.3,0.333,0.287,0.08 ) ^ { T }$ Mtrik kovarian sample dinyatakan dengan $$ \\left. \\begin{array}{l}{ \\hat { \\Sigma } = \\hat { P } - \\hat { p } \\cdot \\hat { p } ^ { T } }\\\\hspace{5mm}{ = \\left( \\begin{array} { c c c c } { 0.3 } & { 0 } & { 0 } & { 0 } \\ { 0 } & { 0.333 } & { 0 } & { 0 } \\ { 0 } & { 0 } & { 0.287 } & { 0 } \\ { 0 } & { 0 } & { 0 } & { 0.08 } \\end{array} \\right) - \\left( \\begin{array} { c } { 0.3 } \\ { 0.333 } \\ { 0.287 } \\ { 0.08 } \\end{array} \\right) \\left( \\begin{array} { l l l l } { 0.3 } & { 0.333 } & { 0.287 } & { 0.08 } \\end{array} \\right) }\\end{array} \\right.\\ \\left. \\begin{array} { l } { = \\left( \\begin{array} { c c c c } { 0.3 } & { 0 } & { 0 } & { 0 } \\ { 0 } & { 0.333 } & { 0 } & { 0 } \\ { 0 } & { 0 } & { 0.287 } & { 0 } \\ { 0 } & { 0 } & { 0 } & { 0.08 } \\end{array} \\right) - \\left( \\begin{array} { c c c c } { 0.09 } & { 0.1 } & { 0.086 } & { 0.024 } \\ { 0.1 } & { 0.111 } & { 0.096 } & { 0.027 } \\ { 0.086 } & { 0.096 } & { 0.082 } & { 0.023 } \\ { 0.024 } & { 0.027 } & { 0.023 } & { 0.006 } \\end{array} \\right) } \\ { = \\left( \\begin{array} { r r r r } { 0.21 } & { - 0.1 } & { - 0.086 } & { - 0.024 } \\ { - 0.1 } & { 0.222 } & { - 0.096 } & { - 0.027 } \\ { - 0.086 } & { - 0.096 } & { 0.204 } & { - 0.023 } \\ { - 0.024 } & { - 0.027 } & { - 0.023 } & { 0.074 } \\end{array} \\right) } \\end{array} \\right. $$ Mean sample persamaan (3.6 ) adalah $$ \\hat { \\mu } = \\hat { p } = ( 2 / 5,3 / 5 ) ^ { T } = ( 0.4,0.6 ) ^ { T } $$ dan matrik kovarian sample (3.9) adalah $$ \\hat { \\Sigma } = \\hat { P } - \\hat { p } \\hat { p } ^ { T } = \\left( \\begin{array} { c c } { 0.4 } & { 0 } \\ { 0 } & { 0.6 } \\end{array} \\right) - \\left( \\begin{array} { l } { 0.4 } \\ { 0.6 } \\end{array} \\right) \\left( \\begin{array} { l l } { 0.4 } & { 0.6 } \\end{array} \\right)\\ \\hspace{50mm}= \\left( \\begin{array} { c c } { 0.4 } & { 0 } \\ { 0 } & { 0.6 } \\end{array} \\right) - \\left( \\begin{array} { c c } { 0.16 } & { 0.24 } \\ { 0.24 } & { 0.36 } \\end{array} \\right) = \\left( \\begin{array} { r r } { 0.24 } & { - 0.24 } \\ { - 0.24 } & { 0.24 } \\end{array} \\right) $$ Tabel 3.2 (a) Dataset Kategorical (b) dataset yang telah dipetakan ke biner \u00a9 dataset yang telah dicentering Untuk menunjukkan bahwa hasilnya sema yang telah diperoleh dengan analisa standar numerik, kita memetakan atribut kategorical X X menjadi dua atribut Bernoulli A_1 A_1 dan A_2 A_2 masing masing dengan simbol Long dan Short masing masing. Dataset yang dipetakan ditunjukkan dalam tabel 3.2b. Mean sampel sederhana diperolah dengan $$ \\hat { \\mu } = \\frac { 1 } { 5 } \\sum _ { i = 1 } ^ { 5 } x _ { i } = \\frac { 1 } { 5 } ( 2,3 ) ^ { T } = ( 0.4,0.6 ) ^ { T } $$ Selanjutnyaang te, kita centerkan dataset dengan mengurangkan mean dari masing masing atribut. Setelah dicentering, dataset dipetakan seperti yang ditunjukkan dalam tabel 3.2 dengan atribut Z_i Z_i seperti atrbut yang telah dicenter A_i A_i . KIta dapat menghitung matrik covarian dengan menggunakan inner product [persamaan2.30] pada kolom yang telah dipusatkan. Kita miliki $$ \\left. \\begin{array}{l}{ \\sigma _ { 1 } ^ { 2 } = \\frac { 1 } { 5 } Z _ { 1 } ^ { T } Z _ { 1 } = 1.2 / 5 = 0.24 }\\{ \\sigma _ { 2 } ^ { 2 } = \\frac { 1 } { 5 } Z _ { 2 } ^ { T } Z _ { 2 } = 1.2 / 5 = 0.24 }\\{ \\sigma _ { 12 } = \\frac { 1 } { 5 } Z _ { 1 } ^ { T } Z _ { 2 } = - 1.2 / 5 = - 0.24 }\\end{array} \\right. $$ Kemudian matrik kovarian sample dinyatakan dengn $$ \\hat { \\Sigma } = \\left( \\begin{array} { r r } { 0.24 } & { - 0.24 } \\ { - 0.24 } & { 0.24 } \\end{array} \\right) $$ yang sesuai dengan hasil yang diperoleh dengan menggunakan pendekatan model Bernoulli multivariate. Analisa Bivariate \u00b6 Asumsikan data terdiri dari dua atribut kategorikal X_1 X_1 dan X_2 X_2 dengan $$ \\left. \\begin{array} { l } { \\operatorname { dom } ( X _ { 1 } ) = { a _ { 11 } , a _ { 12 } , \\ldots , a _ { 1 m _ { 1 } } } } \\ { \\operatorname { dom } ( X _ { 2 } ) = { a _ { 21 } , a _ { 22 } , \\ldots , a _ { 2 m _ { 2 } } } } \\end{array} \\right. $$ Kita telah memberikan n n titik kategorical dari bentuk $x_i=(x_{i1},x_{i2})^T $ dengan x_{i1} \\in dom(X_1) x_{i1} \\in dom(X_1) dan x_{i2} \\in dom(X_2) x_{i2} \\in dom(X_2) . Dataset adalah matrik simbolik n\\times 2 n\\times 2 yaitu $$ D = \\left( \\begin{array} { c c } { X _ { 1 } } & { X _ { 2 } } \\ \\hline x _ { 11 } & { x _ { 12 } } \\ { x _ { 21 } } & { x _ { 22 } } \\ { \\vdots } & { \\vdots } \\ { x _ { n 1 } } & { x _ { n 2 } } \\end{array} \\right) $$ Kita dapat memodelkan X_1 X_1 dan X_2 X_2 sebagai variabel Bernoulli multivariate X1 X1 dan $ X_2$ dengan dimensi m_1 m_1 dan m_2 m_2 . Fungsi massa probabilitas untuk X_1 X_1 dan X_2 X_2 dinyatakan dengan sesuai persamaan (3.4) $$ \\left. \\begin{array} { l } { P ( X _ { 1 } = e _ { 1 i } ) = f _ { 1 } ( e _ { 1 i } ) = p _ { i } ^ { 1 } = \\prod _ { k = 1 } ^ { m _ { 1 } } ( p _ { i } ^ { 1 } ) ^ { c _ { i k } ^ { 1 } } } \\ { P ( X _ { 2 } = e _ { 2 j } ) = f _ { 2 } ( e _ { 2 j } ) = p _ { j } ^ { 2 } = \\prod _ { k = 1 } ^ { m _ { 2 } } ( p _ { j } ^ { 2 } ) ^ { e _ { j k } ^ { 2 } } } \\end{array} \\right. $$ dimana e_{1j} e_{1j} adalah vektor standar basis ke j j dalam \\mathbb R^{m_1} \\mathbb R^{m_1} (untuk atribut X_1 X_1 ) dimana komponen ke k k adalah e_{ik}^1 e_{ik}^1 dan e_{2j} e_{2j} adalah vektor basis standar ke j j dalam \\mathbb R^{m_1} \\mathbb R^{m_1} (untuk atribut X_2 X_2 ) dimana komponen ke k k adalah e_{jk}^2 e_{jk}^2 . Selanjutnya, parameter p_i^1 p_i^1 menotasikan probabilitas, simbol pengamatan a_{1i} a_{1i} dan p_j^2 p_j^2 menotasikan probabilitas pengamatan simbol a_{2j} a_{2j} . Secara bersamaan hal itu harus memenhui kondisi \\sum_{1=1}^{m_1}p_i^1=1 \\sum_{1=1}^{m_1}p_i^1=1 dan \\sum_{1=1}^{m_1}p_j^2=1 \\sum_{1=1}^{m_1}p_j^2=1 Distribusi gabungan dari X_1 X_1 dan X_2 X_2 dimodelkan dengan d ^ { \\prime } = m _ { 1 } + m _ { 2 } d ^ { \\prime } = m _ { 1 } + m _ { 2 } dimensi vektor variabel X = \\left( \\begin{array} { l } { X _ { 1 } } \\\\ { X _ { 2 } } \\end{array} \\right) X = \\left( \\begin{array} { l } { X _ { 1 } } \\\\ { X _ { 2 } } \\end{array} \\right) spesifik dengan memetakan $$ X ( ( v _ { 1 } , v _ { 2 } ) ^ { T } ) = \\left( \\begin{array} { l } { X _ { 1 } ( v _ { 1 } ) } \\ { X _ { 2 } ( v _ { 2 } ) } \\end{array} \\right) = \\left( \\begin{array} { l } { e _ { 1 i } } \\ { e _ { 2 j } } \\end{array} \\right) $$ disediakan bahwa v_1=a_{1i} v_1=a_{1i} dan v_2=a_{2j} v_2=a_{2j} . Jangkauan dari X X terdiri dari m_1 \\times m_2 m_1 \\times m_2 pasangan berbeda dari nilai nilai vektor \\{ ( e _ { 1 i } , e _ { 2 j } ) ^ { T } \\} \\{ ( e _ { 1 i } , e _ { 2 j } ) ^ { T } \\} dengan 1 \\leq i \\leq m _ { 1 } 1 \\leq i \\leq m _ { 1 } dan 1 \\leq j \\leq m _ { 2 } 1 \\leq j \\leq m _ { 2 } . Fungsi massa probabilitas gabungan dari X X adalah dinyatakan dengan $$ P ( X = ( e _ { 1 i } , e _ { 2 j } ) ^ { T } ) = f ( e _ { 1 i } , e _ { 2 j } ) = p _ { i j } = \\prod \\prod _ { i j } ^ { m _ { 1 } } \\frac { e _ { i r } ^ { 1 } \\cdot e _ { j s } ^ { 2 } } { 1 } $$ dimana p_{ij} p_{ij} adalah probabilitas dari pengamatan pasangan simbol (a_{1i},a_{2j}) (a_{1i},a_{2j}) . Parameter probabilitas ini harus memenuhi kondisi \\sum _ { i = 1 } ^ { m _ { 1 } } \\sum _ { i = 1 } ^ { m _ { 2 } } p _ { i j } = 1 \\sum _ { i = 1 } ^ { m _ { 1 } } \\sum _ { i = 1 } ^ { m _ { 2 } } p _ { i j } = 1 . Fungsi Massa Probabiltias gabungan untuk X X dinyatakan dengan m_1 \\times m_2 m_1 \\times m_2 matrik $$ P _ { 12 } = \\left( \\begin{array} { c c c c } { p _ { 11 } } & { p _ { 12 } } & { \\dots } & { p _ { 1 m _ { 2 } } } \\ { p _ { 21 } } & { p _ { 22 } } & { \\dots } & { p _ { 2 m _ { 2 } } } \\ { \\vdots } & { \\vdots } & { \\ddots } & { \\vdots } \\ { p _ { m _ { 1 } 1 } } & { p _ { m _ { 1 } 2 } } & { \\dots } & { p _ { m _ { 1 } m _ { 2 } } } \\end{array} \\right) $$ Contoh. Perhatikan atribut sepal length(atribut X_1 X_1 ) yang telah diskritisasi dalam tabel 3.1. Kita juga telah mendiskritisasi sepal width (atribut X_2 X_2 ) kedalam tiga nilai seperti yang ditunjukkan dalam tabel 3.3. Kita lalu punya $$ \\hspace{29mm}dom(X_1)={a_{11}=VeryShort,a_{12}=short,a_{13}=Long,a_{14}=Very}\\ dom(X_1)={a_{21}=Short,a_{22}=Medium,a_{23}=Long,} $$ Simbol titik x=\\{Short,Long=(a_{12},a_{23})\\} x=\\{Short,Long=(a_{12},a_{23})\\} dipetakan ke vektor $$ X ( x ) = \\left( \\begin{array} { l } { e _ { 12 } } \\ { e _ { 23 } } \\end{array} \\right) = ( 0,1,0,0 | 0,0,1 ) ^ { T } \\in \\mathbb R ^ { 7 } $$ dimana kita gunakan | untuk membatasi dua subvektor e _ { 12 } = ( 0,1,0,0 ) ^ { T } \\in \\mathbb R^4 e _ { 12 } = ( 0,1,0,0 ) ^ { T } \\in \\mathbb R^4 dan e_{23}=(0,0,1)^T \\in \\mathbb R^3 e_{23}=(0,0,1)^T \\in \\mathbb R^3 bersesuaian dengan simbol atribut sepal length dan sepal width. Catatan bahwa e_{12} e_{12} adalah standar kedua basis vektor dalam \\mathbb R^4 \\mathbb R^4 untuk X_1 X_1 dan e_{23 } e_{23 } adalah standar ketiga basis vektor dalam $ \\mathbb R^3$ untuk X_2 X_2 Mean \u00b6 Mean bivariate dapat dengan mudah digeneralisasi dari (3.5) sebagai berikut $$ \\mu = E [ X ] = E [ \\left( \\begin{array} { l } { X _ { 1 } } \\ { X _ { 2 } } \\end{array} \\right) ] = \\left( \\begin{array} { l } { E [ X _ { 1 } ] } \\ { E [ X _ { 2 } ] } \\end{array} \\right) = \\left( \\begin{array} { l } { \\mu _ { 1 } } \\ { \\mu _ { 2 } } \\end{array} \\right) = \\left( \\begin{array} { l } { p _ { 1 } } \\ { p _ { 2 } } \\end{array} \\right) $$ dimana \\mu _ { 1 } = p _ { 1 } = ( p _ { 1 } ^ { 1 } , \\ldots , p _ { m _ { 1 } } ^ { 1 } ) ^ { T } \\mu _ { 1 } = p _ { 1 } = ( p _ { 1 } ^ { 1 } , \\ldots , p _ { m _ { 1 } } ^ { 1 } ) ^ { T } dan \\mu _ { 2 } = p _ { 2 } = ( p _ { 1 } ^ { 2 } , \\ldots , p _ { m _ { 2 } } ^ { 2 } ) ^ { T } \\mu _ { 2 } = p _ { 2 } = ( p _ { 1 } ^ { 2 } , \\ldots , p _ { m _ { 2 } } ^ { 2 } ) ^ { T } adalah vektor mean untuk X_1 X_1 dan X_2 X_2 . Vektor p_1 p_1 dan p_2 p_2 juga menyatakan fungsi massa probabilitas untuk X_1 X_1 dan X_2 X_2 Mean sample \u00b6 Mean sampel dapat digeneralisasi dari persamaan (3.6) dengan mengganti massa probabilitas dari \\frac{1}{n} \\frac{1}{n} pada setiap titik data $$ \\hat { \\mu } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } = \\frac { 1 } { n } \\left( \\begin{array} { c } { \\sum _ { i = 1 } ^ { m _ { 1 } } n _ { i } ^ { 1 } e _ { 1 i } } \\ { \\sum _ { j = 1 } ^ { m _ { 2 } } n _ { j } ^ { 2 } e _ { 2 j } } \\end{array} \\right) = \\frac { 1 } { n } \\left( \\begin{array} { c } { n _ { 1 } ^ { 1 } } \\ { \\vdots } \\ { n _ { m _ { 1 } } ^ { 1 } } \\ { n _ { 1 } ^ { 2 } } \\ { \\vdots } \\ { n _ { m _ { 2 } } ^ { 2 } } \\end{array} \\right) = \\left( \\begin{array} { c } { \\hat { p } _ { 1 } ^ { 1 } } \\ { \\vdots } \\ { \\hat { p } _ { m _ { 1 } } ^ { 1 } } \\ { \\hat { p } _ { 1 } ^ { 2 } } \\ { \\vdots } \\ { \\hat { p } _ { m _ { 2 } } ^ { 2 } } \\end{array} \\right) = \\left( \\begin{array} { c } { \\hat { p } _ { 1 } } \\ { \\hat { p } _ { 2 } } \\end{array} \\right) = \\left( \\begin{array} { c } { \\hat { \\mu } _ { 1 } } \\ { \\hat { \\mu } _ { 2 } } \\end{array} \\right) $$ dimana n_j^i n_j^i adalah frekwensi pengamatan dari simbol a_{ij} a_{ij} dalam sampel ukuran n n dan \\hat \\mu_i=\\hat p_i=(p_1^i,p_2^i...p_m^i)^T \\hat \\mu_i=\\hat p_i=(p_1^i,p_2^i...p_m^i)^T adalah mean vektor sampel untuk X_i X_i , yang juga fungsi massa probabilitas empiris untuk atribut X_i X_i Matrik kovarian \u00b6 Matrik kovarian untuk X X adalah matrik d ^ { \\prime } \\times d ^ { \\prime } = ( m _ { 1 } + m _ { 2 } ) \\times ( m _ { 1 } + m _ { 2 } ) d ^ { \\prime } \\times d ^ { \\prime } = ( m _ { 1 } + m _ { 2 } ) \\times ( m _ { 1 } + m _ { 2 } ) yang dinyatakan dengan $$ \\Sigma = \\left( \\begin{array} { l l } { \\Sigma _ { 11 } } & { \\Sigma _ { 12 } } \\ { \\Sigma _ { 12 } ^ { T } } & { \\Sigma _ { 22 } } \\end{array} \\right) $$ dimana $\\sum_{11} $ adalah matrik kovarian m_1\\times m_1 m_1\\times m_1 untuk X-1 X-1 dan \\sum_{22} \\sum_{22} adalah matrik kovarian m_2\\times m_2 m_2\\times m_2 untuk matrik X_2 X_2 yang dapat dihitung menggunakan persamaan (3.8). Oleh karena itu $$ \\left. \\begin{array} { l } { \\Sigma _ { 11 } = P _ { 1 } - p _ { 1 } p _ { 1 } ^ { T } } \\ { \\Sigma _ { 22 } = P _ { 2 } - p _ { 2 } p _ { 2 } ^ { T } } \\end{array} \\right. $$ dimana P_1=diag(p_1) P_1=diag(p_1) dan P_2=diag(p_2) P_2=diag(p_2) . Oleh karena itu, \\sum_{12} \\sum_{12} adalah matrik kovarian m_1 \\times m_2 m_1 \\times m_2 antara variabel X_1 X_1 dan X2 X2 dinyatakan dengan $$ \\left. \\begin{array}{l}{ \\Sigma _ { 12 } = E [ ( X _ { 1 } - \\mu _ { 1 } ) ( X _ { 2 } - \\mu _ { 2 } ) ^ { T } ] }\\{ = E [ X _ { 1 } X _ { 2 } ^ { T } ] - E [ X _ { 1 } ] E [ X _ { 2 } ] ^ { T } }\\{ = P _ { 12 } - \\mu _ { 1 } \\mu _ { 2 } ^ { T } }\\{ = P _ { 12 } - p _ { 1 } p _ { 2 } ^ { T } }\\end{array} \\right. $$ = \\left( \\begin{array} { c c c c } { p _ { 11 } - p _ { 1 } ^ { 1 } p _ { 1 } ^ { 2 } } & { p _ { 12 } - p _ { 1 } ^ { 1 } p _ { 2 } ^ { 2 } } & { \\cdots } & { p _ { 1 m _ { 2 } } - p _ { 1 } ^ { 1 } p _ { m _ { 2 } } ^ { 2 } } \\\\ { p _ { 21 } - p _ { 2 } ^ { 1 } p _ { 1 } ^ { 2 } } & { p _ { 22 } - p _ { 2 } ^ { 1 } p _ { 2 } ^ { 2 } } & { \\cdots } & { p _ { 2 m _ { 2 } } - p _ { 2 } ^ { 1 } p _ { m _ { 2 } } ^ { 2 } } \\\\ { \\vdots } & { \\vdots } & { \\ddots } & { \\vdots } \\\\ { p _ { m _ { 1 } 1 } - p _ { m _ { 1 } } ^ { 1 } p _ { 1 } ^ { 2 } } & { p _ { m _ { 1 } 2 } - p _ { m _ { 1 } } ^ { 1 } p _ { 2 } ^ { 2 } } & { \\cdots } & { p _ { m _ { 1 } m _ { 2 } } - p _ { m _ { 1 } } ^ { 1 } p _ { m _ { 2 } } ^ { 2 } } \\end{array} \\right) = \\left( \\begin{array} { c c c c } { p _ { 11 } - p _ { 1 } ^ { 1 } p _ { 1 } ^ { 2 } } & { p _ { 12 } - p _ { 1 } ^ { 1 } p _ { 2 } ^ { 2 } } & { \\cdots } & { p _ { 1 m _ { 2 } } - p _ { 1 } ^ { 1 } p _ { m _ { 2 } } ^ { 2 } } \\\\ { p _ { 21 } - p _ { 2 } ^ { 1 } p _ { 1 } ^ { 2 } } & { p _ { 22 } - p _ { 2 } ^ { 1 } p _ { 2 } ^ { 2 } } & { \\cdots } & { p _ { 2 m _ { 2 } } - p _ { 2 } ^ { 1 } p _ { m _ { 2 } } ^ { 2 } } \\\\ { \\vdots } & { \\vdots } & { \\ddots } & { \\vdots } \\\\ { p _ { m _ { 1 } 1 } - p _ { m _ { 1 } } ^ { 1 } p _ { 1 } ^ { 2 } } & { p _ { m _ { 1 } 2 } - p _ { m _ { 1 } } ^ { 1 } p _ { 2 } ^ { 2 } } & { \\cdots } & { p _ { m _ { 1 } m _ { 2 } } - p _ { m _ { 1 } } ^ { 1 } p _ { m _ { 2 } } ^ { 2 } } \\end{array} \\right) dimana P_{12} P_{12} menyatakan fungsi massa probabilitas gabungan untuk X X dinyatakan dalam persamaan (3.10) Kadang kala, setiap baris dan seitap kolom dari \\sum_{12} \\sum_{12} menjumlahkan ke nol. Misalkan, perhatikan baris i i dan kolom j j $$ \\left. \\begin{array} { l } { \\sum _ { k = 1 } ^ { m _ { 2 } } ( p _ { i k } - p _ { i } ^ { 1 } p _ { k } ^ { 2 } ) = ( \\sum _ { k = 1 } ^ { m _ { 2 } } p _ { i k } ) - p _ { i } ^ { 1 } = p _ { i } ^ { 1 } - p _ { i } ^ { 1 } = 0 } \\ { \\sum _ { k = 1 } ^ { m _ { 1 } } ( p _ { k j } - p _ { k } ^ { 1 } p _ { j } ^ { 2 } ) = ( \\sum _ { k = 1 } ^ { m _ { 1 } } p _ { k j } ) - p _ { j } ^ { 2 } = p _ { j } ^ { 2 } - p _ { j } ^ { 2 } = 0 } \\end{array} \\right. $$ yang mengikuti dari fakta bahwa menjumlahkan fungsi massa gabungan terhadap semua nilai X_2 X_2 , menghasilkan distribusi marginal $ X_1$, dan menjumlahkannya terhadap semua nilai X_1 X_1 menghasilkan distribusi marginal untuk X_ 2 X_ 2 Dikombinasikan dengan fakta bahwa \\sum_{11} \\sum_{11} dan \\sum_{22} \\sum_{22} juga memiliki baris dan jumlah kolom sama dengan nol melalui Persamaan. (3.7), matriks kovarian yang lengkap \\Sigma \\Sigma memiliki baris dan kolom yang berjumlah hingga nol. Matrik Kovarian Sample \u00b6 Matrik kovarian sampel dinyatakan dengan $$ \\hat { \\Sigma } = \\left( \\begin{array} { l l } { \\hat { \\Sigma } _ { 11 } } & { \\hat { \\Sigma } _ { 12 } } \\ { \\hat { \\Sigma } _ { 12 } ^ { T } } & { \\hat { \\Sigma } _ { 22 } } \\end{array} \\right) $$ dimana $$ \\left. \\begin{array} { l } { \\hat { \\Sigma } _ { 11 } = \\hat { P } _ { 1 } - \\hat { p } _ { 1 } \\hat { p } _ { 1 } ^ { T } } \\ { \\hat { \\Sigma } _ { 22 } = \\hat { P } _ { 2 } - \\hat { p } _ { 2 } \\hat { p } _ { 2 } ^ { T } } \\ { \\hat { \\Sigma } _ { 12 } = \\hat { P } _ { 12 } - \\hat { p } _ { 1 } \\hat { p } _ { 2 } ^ { T } } \\end{array} \\right. $$ Disini $\\hat P_1=diag(\\hat p_1) $ dan \\hat P_2=diag(\\hat p_2) \\hat P_2=diag(\\hat p_2) dan \\hat p_1 \\hat p_1 dan \\hat p_2 \\hat p_2 menyatakan masing masing fungsi massa probabilitas empiris untuk X_1 X_1 dan X_2 X_2 . Selanjutnya $ \\hat P_{12}$ menyatakan fungsi massa probabilitas gabungan empiris untuk X_1 X_1 dan X_2 X_2 ,dinyatakan dengan $$ \\hat { P } _ { 12 } ( i , j ) = \\hat { f } ( e _ { 1 i } , e _ { 2 j } ) = \\frac { 1 } { n } \\sum _ { k = 1 } ^ { n } I _ { i j } ( x _ { k } ) = \\frac { n _ { i j } } { n } = \\hat { p } _ { i j } $$ dimana I_{ij} I_{ij} adalah variabel indikator $$ I _ { i j } ( x _ { k } ) = \\left{ \\begin{array} { l l } { 1 } & { \\text { jika } x _ { k 1 } = e _ { i _ { 1 } } \\text { dan } x _ { k 2 } = e _ { j 2 } } \\ { 0 } & { \\text { lainnya } } \\end{array} \\right. $$ Contoh Kita terustkan dengan atribut kategorikal bivariate X_1 X_1 dan X_2 X_2 dalam contoh 3.7. Dari contoh 3.4 dan dari banyaknya kejadian untuk setiap nilai dari sepal width dalam tabel 3.3, kita miliki $$ \\hat { \\mu } _ { 1 } = \\hat { p } _ { 1 } = \\left( \\begin{array} { c } { 0.3 } \\ { 0.333 } \\ { 0.287 } \\ { 0.08 } \\end{array} \\right) \\quad \\hat { \\mu } _ { 2 } = \\hat { p } _ { 2 } = \\frac { 1 } { 150 } \\left( \\begin{array} { c } { 47 } \\ { 88 } \\ { 15 } \\end{array} \\right) = \\left( \\begin{array} { c } { 0.313 } \\ { 0.587 } \\ { 0.1 } \\end{array} \\right) $$ Kemudian, mean untuk X = \\left( \\begin{array} { l } { X _ { 1 } } \\\\ { X _ { 2 } } \\end{array} \\right) X = \\left( \\begin{array} { l } { X _ { 1 } } \\\\ { X _ { 2 } } \\end{array} \\right) dinyatakan dengan $$ \\hat { \\mu } = \\left( \\begin{array} { l } { \\hat { \\mu } _ { 1 } } \\ { \\hat { \\mu } _ { 2 } } \\end{array} \\right) = \\left( \\begin{array} { l } { \\hat { p } _ { 1 } } \\ { \\hat { p } _ { 2 } } \\end{array} \\right) = ( 0.3,0.333,0.287,0.08 | 0.313,0.587,0.1 ) ^ { T } $$ dari persamaan 3.5 kita miliki $$ \\hat { \\Sigma } _ { 11 } = \\left( \\begin{array} { r r r r } { 0.21 } & { - 0.1 } & { - 0.086 } & { - 0.024 } \\ { - 0.1 } & { 0.222 } & { - 0.096 } & { - 0.027 } \\ { - 0.086 } & { - 0.096 } & { 0.204 } & { - 0.023 } \\ { - 0.024 } & { - 0.027 } & { - 0.023 } & { 0.074 } \\end{array} \\right) $$ Dengan cara sama kita dapatkan $$ \\hat { \\Sigma } _ { 22 } = \\left( \\begin{array} { r r r } { 0.215 } & { - 0.184 } & { - 0.031 } \\ { - 0.184 } & { 0.242 } & { - 0.059 } \\ { - 0.031 } & { - 0.059 } & { 0.09 } \\end{array} \\right) $$ Selanjutnya kita gunakan banyaknya pengamatan dalam tabel 3.4 untuk mendapatkan fungsi massa probabilitas gabungan empiris untuk X_1 X_1 dan X_2 X_2 menggunakan persamaan 3.13, seperti yang telah diplot dalam gambar 3.2. Dari probabilitas ini kita dapatkan $$ E [ X _ { 1 } X _ { 2 } ^ { T } ] = \\hat { P } _ { 12 } = \\frac { 1 } { 150 } \\left( \\begin{array} { c c c } { 7 } & { 33 } & { 5 } \\ { 24 } & { 18 } & { 8 } \\ { 13 } & { 30 } & { 0 } \\ { 3 } & { 7 } & { 2 } \\end{array} \\right) = \\left( \\begin{array} { c c c } { 0.047 } & { 0.22 } & { 0.033 } \\ { 0.16 } & { 0.12 } & { 0.053 } \\ { 0.087 } & { 0.2 } & { 0 } \\ { 0.02 } & { 0.047 } & { 0.013 } \\end{array} \\right) $$ Selanjutnya kita memiliki $$ \\left. \\begin{array}{l}{ E [ X _ { 1 } ] E [ X _ { 2 } ] ^ { T } = \\hat { \\mu } _ { 1 } \\hat { \\mu } _ { 2 } ^ { T } = \\hat { p } _ { 1 } \\hat { p } _ { 2 } ^ { T } }\\{ = \\left( \\begin{array} { c } { 0.3 } \\ { 0.333 } \\ { 0.287 } \\ { 0.08 } \\end{array} \\right) \\left( \\begin{array} { c c c } { 0.313 } & { 0.587 } & { 0.1 } \\end{array} \\right) }\\{ = \\left( \\begin{array} { c c c } { 0.094 } & { 0.176 } & { 0.03 } \\ { 0.104 } & { 0.196 } & { 0.033 } \\ { 0.09 } & { 0.168 } & { 0.029 } \\ { 0.025 } & { 0.047 } & { 0.008 } \\end{array} \\right) }\\end{array} \\right. $$ Kita sekarang menghitung matrik kovarian sample antar atribut \\hat { \\Sigma } _ { 12} \\hat { \\Sigma } _ { 12} untuk X_1 X_1 dan X_2 X_2 menggunakan persamaan (3.11) sebagai berikut $$ \\left. \\begin{array}{l}{ \\hat { \\Sigma } _ { 12 } = \\hat { P } _ { 12 } - \\hat { p } _ { 1 } \\hat { p } _ { 2 } ^ { T } }\\{ = \\left( \\begin{array} { r r r } { - 0.047 } & { 0.044 } & { 0.003 } \\ { 0.056 } & { - 0.076 } & { 0.02 } \\ { - 0.003 } & { 0.032 } & { - 0.029 } \\ { - 0.005 } & { 0 } & { 0.005 } \\end{array} \\right) }\\end{array} \\right. $$ Kita dapat mengamati bahwa setiap baris dan kolam dalam \\hat { \\Sigma } _ { 12} \\hat { \\Sigma } _ { 12} jumlahnya adalah nol. Dengan menempatkan semuanya dari \\hat { \\Sigma } _ { 11} \\hat { \\Sigma } _ { 11} , \\hat { \\Sigma } _ { 22} \\hat { \\Sigma } _ { 22} ,dan \\hat { \\Sigma } _ { 12} \\hat { \\Sigma } _ { 12} kita dapatkan matrik kovarian sample sebagai berikut $$ \\hat { \\Sigma } = \\left( \\begin{array} { l l } { \\hat { \\Sigma } _ { 11 } } & { \\hat { \\Sigma } _ { 12 } } \\ { \\hat { \\Sigma } _ { 12 } ^ { T } } & { \\hat { \\Sigma } _ { 22 } } \\end{array} \\right)\\ = \\left( \\begin{array} { r r r r | r r r } { 0.21 } & { - 0.1 } & { - 0.086 } & { - 0.024 } & { - 0.047 } & { 0.044 } & { 0.003 } \\ { - 0.1 } & { 0.222 } & { - 0.096 } & { - 0.027 } & { 0.056 } & { - 0.076 } & { 0.02 } \\ { - 0.086 } & { - 0.096 } & { 0.204 } & { - 0.023 } & { - 0.003 } & { 0.032 } & { - 0.029 } \\ { - 0.024 } & { - 0.027 } & { - 0.023 } & { 0.074 } & { - 0.005 } & { 0 } & { 0.005 } \\ \\hline - 0.047 & { 0.056 } & { - 0.003 } & { - 0.005 } & { 0.215 } & { - 0.184 } & { - 0.031 } \\ { 0.044 } & { - 0.076 } & { 0.032 } & { 0 } & { - 0.184 } & { 0.242 } & { - 0.059 } \\ { 0.003 } & { 0.02 } & { - 0.029 } & { 0.005 } & { - 0.031 } & { - 0.059 } & { 0.09 } \\end{array} \\right) $$ Dalam \\hat \\Sigma \\hat \\Sigma , setiap baris dan kolom juga jumlahnya sama dengan nol Tabel 3.4 Jumlah pengamatan (n_{ij}) (n_{ij}) sepal length dan sepal width Gambar 3.2 Fungsi massa probabilitas gabungan empiris sepal length dan sepal width","title":"Eksplorasi data 2"},{"location":"Eksplorasi%20data-2/#analisa-bivariate","text":"Asumsikan data terdiri dari dua atribut kategorikal X_1 X_1 dan X_2 X_2 dengan $$ \\left. \\begin{array} { l } { \\operatorname { dom } ( X _ { 1 } ) = { a _ { 11 } , a _ { 12 } , \\ldots , a _ { 1 m _ { 1 } } } } \\ { \\operatorname { dom } ( X _ { 2 } ) = { a _ { 21 } , a _ { 22 } , \\ldots , a _ { 2 m _ { 2 } } } } \\end{array} \\right. $$ Kita telah memberikan n n titik kategorical dari bentuk $x_i=(x_{i1},x_{i2})^T $ dengan x_{i1} \\in dom(X_1) x_{i1} \\in dom(X_1) dan x_{i2} \\in dom(X_2) x_{i2} \\in dom(X_2) . Dataset adalah matrik simbolik n\\times 2 n\\times 2 yaitu $$ D = \\left( \\begin{array} { c c } { X _ { 1 } } & { X _ { 2 } } \\ \\hline x _ { 11 } & { x _ { 12 } } \\ { x _ { 21 } } & { x _ { 22 } } \\ { \\vdots } & { \\vdots } \\ { x _ { n 1 } } & { x _ { n 2 } } \\end{array} \\right) $$ Kita dapat memodelkan X_1 X_1 dan X_2 X_2 sebagai variabel Bernoulli multivariate X1 X1 dan $ X_2$ dengan dimensi m_1 m_1 dan m_2 m_2 . Fungsi massa probabilitas untuk X_1 X_1 dan X_2 X_2 dinyatakan dengan sesuai persamaan (3.4) $$ \\left. \\begin{array} { l } { P ( X _ { 1 } = e _ { 1 i } ) = f _ { 1 } ( e _ { 1 i } ) = p _ { i } ^ { 1 } = \\prod _ { k = 1 } ^ { m _ { 1 } } ( p _ { i } ^ { 1 } ) ^ { c _ { i k } ^ { 1 } } } \\ { P ( X _ { 2 } = e _ { 2 j } ) = f _ { 2 } ( e _ { 2 j } ) = p _ { j } ^ { 2 } = \\prod _ { k = 1 } ^ { m _ { 2 } } ( p _ { j } ^ { 2 } ) ^ { e _ { j k } ^ { 2 } } } \\end{array} \\right. $$ dimana e_{1j} e_{1j} adalah vektor standar basis ke j j dalam \\mathbb R^{m_1} \\mathbb R^{m_1} (untuk atribut X_1 X_1 ) dimana komponen ke k k adalah e_{ik}^1 e_{ik}^1 dan e_{2j} e_{2j} adalah vektor basis standar ke j j dalam \\mathbb R^{m_1} \\mathbb R^{m_1} (untuk atribut X_2 X_2 ) dimana komponen ke k k adalah e_{jk}^2 e_{jk}^2 . Selanjutnya, parameter p_i^1 p_i^1 menotasikan probabilitas, simbol pengamatan a_{1i} a_{1i} dan p_j^2 p_j^2 menotasikan probabilitas pengamatan simbol a_{2j} a_{2j} . Secara bersamaan hal itu harus memenhui kondisi \\sum_{1=1}^{m_1}p_i^1=1 \\sum_{1=1}^{m_1}p_i^1=1 dan \\sum_{1=1}^{m_1}p_j^2=1 \\sum_{1=1}^{m_1}p_j^2=1 Distribusi gabungan dari X_1 X_1 dan X_2 X_2 dimodelkan dengan d ^ { \\prime } = m _ { 1 } + m _ { 2 } d ^ { \\prime } = m _ { 1 } + m _ { 2 } dimensi vektor variabel X = \\left( \\begin{array} { l } { X _ { 1 } } \\\\ { X _ { 2 } } \\end{array} \\right) X = \\left( \\begin{array} { l } { X _ { 1 } } \\\\ { X _ { 2 } } \\end{array} \\right) spesifik dengan memetakan $$ X ( ( v _ { 1 } , v _ { 2 } ) ^ { T } ) = \\left( \\begin{array} { l } { X _ { 1 } ( v _ { 1 } ) } \\ { X _ { 2 } ( v _ { 2 } ) } \\end{array} \\right) = \\left( \\begin{array} { l } { e _ { 1 i } } \\ { e _ { 2 j } } \\end{array} \\right) $$ disediakan bahwa v_1=a_{1i} v_1=a_{1i} dan v_2=a_{2j} v_2=a_{2j} . Jangkauan dari X X terdiri dari m_1 \\times m_2 m_1 \\times m_2 pasangan berbeda dari nilai nilai vektor \\{ ( e _ { 1 i } , e _ { 2 j } ) ^ { T } \\} \\{ ( e _ { 1 i } , e _ { 2 j } ) ^ { T } \\} dengan 1 \\leq i \\leq m _ { 1 } 1 \\leq i \\leq m _ { 1 } dan 1 \\leq j \\leq m _ { 2 } 1 \\leq j \\leq m _ { 2 } . Fungsi massa probabilitas gabungan dari X X adalah dinyatakan dengan $$ P ( X = ( e _ { 1 i } , e _ { 2 j } ) ^ { T } ) = f ( e _ { 1 i } , e _ { 2 j } ) = p _ { i j } = \\prod \\prod _ { i j } ^ { m _ { 1 } } \\frac { e _ { i r } ^ { 1 } \\cdot e _ { j s } ^ { 2 } } { 1 } $$ dimana p_{ij} p_{ij} adalah probabilitas dari pengamatan pasangan simbol (a_{1i},a_{2j}) (a_{1i},a_{2j}) . Parameter probabilitas ini harus memenuhi kondisi \\sum _ { i = 1 } ^ { m _ { 1 } } \\sum _ { i = 1 } ^ { m _ { 2 } } p _ { i j } = 1 \\sum _ { i = 1 } ^ { m _ { 1 } } \\sum _ { i = 1 } ^ { m _ { 2 } } p _ { i j } = 1 . Fungsi Massa Probabiltias gabungan untuk X X dinyatakan dengan m_1 \\times m_2 m_1 \\times m_2 matrik $$ P _ { 12 } = \\left( \\begin{array} { c c c c } { p _ { 11 } } & { p _ { 12 } } & { \\dots } & { p _ { 1 m _ { 2 } } } \\ { p _ { 21 } } & { p _ { 22 } } & { \\dots } & { p _ { 2 m _ { 2 } } } \\ { \\vdots } & { \\vdots } & { \\ddots } & { \\vdots } \\ { p _ { m _ { 1 } 1 } } & { p _ { m _ { 1 } 2 } } & { \\dots } & { p _ { m _ { 1 } m _ { 2 } } } \\end{array} \\right) $$ Contoh. Perhatikan atribut sepal length(atribut X_1 X_1 ) yang telah diskritisasi dalam tabel 3.1. Kita juga telah mendiskritisasi sepal width (atribut X_2 X_2 ) kedalam tiga nilai seperti yang ditunjukkan dalam tabel 3.3. Kita lalu punya $$ \\hspace{29mm}dom(X_1)={a_{11}=VeryShort,a_{12}=short,a_{13}=Long,a_{14}=Very}\\ dom(X_1)={a_{21}=Short,a_{22}=Medium,a_{23}=Long,} $$ Simbol titik x=\\{Short,Long=(a_{12},a_{23})\\} x=\\{Short,Long=(a_{12},a_{23})\\} dipetakan ke vektor $$ X ( x ) = \\left( \\begin{array} { l } { e _ { 12 } } \\ { e _ { 23 } } \\end{array} \\right) = ( 0,1,0,0 | 0,0,1 ) ^ { T } \\in \\mathbb R ^ { 7 } $$ dimana kita gunakan | untuk membatasi dua subvektor e _ { 12 } = ( 0,1,0,0 ) ^ { T } \\in \\mathbb R^4 e _ { 12 } = ( 0,1,0,0 ) ^ { T } \\in \\mathbb R^4 dan e_{23}=(0,0,1)^T \\in \\mathbb R^3 e_{23}=(0,0,1)^T \\in \\mathbb R^3 bersesuaian dengan simbol atribut sepal length dan sepal width. Catatan bahwa e_{12} e_{12} adalah standar kedua basis vektor dalam \\mathbb R^4 \\mathbb R^4 untuk X_1 X_1 dan e_{23 } e_{23 } adalah standar ketiga basis vektor dalam $ \\mathbb R^3$ untuk X_2 X_2","title":"Analisa Bivariate"},{"location":"Eksplorasi%20data-2/#mean","text":"Mean bivariate dapat dengan mudah digeneralisasi dari (3.5) sebagai berikut $$ \\mu = E [ X ] = E [ \\left( \\begin{array} { l } { X _ { 1 } } \\ { X _ { 2 } } \\end{array} \\right) ] = \\left( \\begin{array} { l } { E [ X _ { 1 } ] } \\ { E [ X _ { 2 } ] } \\end{array} \\right) = \\left( \\begin{array} { l } { \\mu _ { 1 } } \\ { \\mu _ { 2 } } \\end{array} \\right) = \\left( \\begin{array} { l } { p _ { 1 } } \\ { p _ { 2 } } \\end{array} \\right) $$ dimana \\mu _ { 1 } = p _ { 1 } = ( p _ { 1 } ^ { 1 } , \\ldots , p _ { m _ { 1 } } ^ { 1 } ) ^ { T } \\mu _ { 1 } = p _ { 1 } = ( p _ { 1 } ^ { 1 } , \\ldots , p _ { m _ { 1 } } ^ { 1 } ) ^ { T } dan \\mu _ { 2 } = p _ { 2 } = ( p _ { 1 } ^ { 2 } , \\ldots , p _ { m _ { 2 } } ^ { 2 } ) ^ { T } \\mu _ { 2 } = p _ { 2 } = ( p _ { 1 } ^ { 2 } , \\ldots , p _ { m _ { 2 } } ^ { 2 } ) ^ { T } adalah vektor mean untuk X_1 X_1 dan X_2 X_2 . Vektor p_1 p_1 dan p_2 p_2 juga menyatakan fungsi massa probabilitas untuk X_1 X_1 dan X_2 X_2","title":"Mean"},{"location":"Eksplorasi%20data-2/#mean-sample","text":"Mean sampel dapat digeneralisasi dari persamaan (3.6) dengan mengganti massa probabilitas dari \\frac{1}{n} \\frac{1}{n} pada setiap titik data $$ \\hat { \\mu } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } = \\frac { 1 } { n } \\left( \\begin{array} { c } { \\sum _ { i = 1 } ^ { m _ { 1 } } n _ { i } ^ { 1 } e _ { 1 i } } \\ { \\sum _ { j = 1 } ^ { m _ { 2 } } n _ { j } ^ { 2 } e _ { 2 j } } \\end{array} \\right) = \\frac { 1 } { n } \\left( \\begin{array} { c } { n _ { 1 } ^ { 1 } } \\ { \\vdots } \\ { n _ { m _ { 1 } } ^ { 1 } } \\ { n _ { 1 } ^ { 2 } } \\ { \\vdots } \\ { n _ { m _ { 2 } } ^ { 2 } } \\end{array} \\right) = \\left( \\begin{array} { c } { \\hat { p } _ { 1 } ^ { 1 } } \\ { \\vdots } \\ { \\hat { p } _ { m _ { 1 } } ^ { 1 } } \\ { \\hat { p } _ { 1 } ^ { 2 } } \\ { \\vdots } \\ { \\hat { p } _ { m _ { 2 } } ^ { 2 } } \\end{array} \\right) = \\left( \\begin{array} { c } { \\hat { p } _ { 1 } } \\ { \\hat { p } _ { 2 } } \\end{array} \\right) = \\left( \\begin{array} { c } { \\hat { \\mu } _ { 1 } } \\ { \\hat { \\mu } _ { 2 } } \\end{array} \\right) $$ dimana n_j^i n_j^i adalah frekwensi pengamatan dari simbol a_{ij} a_{ij} dalam sampel ukuran n n dan \\hat \\mu_i=\\hat p_i=(p_1^i,p_2^i...p_m^i)^T \\hat \\mu_i=\\hat p_i=(p_1^i,p_2^i...p_m^i)^T adalah mean vektor sampel untuk X_i X_i , yang juga fungsi massa probabilitas empiris untuk atribut X_i X_i","title":"Mean sample"},{"location":"Eksplorasi%20data-2/#matrik-kovarian","text":"Matrik kovarian untuk X X adalah matrik d ^ { \\prime } \\times d ^ { \\prime } = ( m _ { 1 } + m _ { 2 } ) \\times ( m _ { 1 } + m _ { 2 } ) d ^ { \\prime } \\times d ^ { \\prime } = ( m _ { 1 } + m _ { 2 } ) \\times ( m _ { 1 } + m _ { 2 } ) yang dinyatakan dengan $$ \\Sigma = \\left( \\begin{array} { l l } { \\Sigma _ { 11 } } & { \\Sigma _ { 12 } } \\ { \\Sigma _ { 12 } ^ { T } } & { \\Sigma _ { 22 } } \\end{array} \\right) $$ dimana $\\sum_{11} $ adalah matrik kovarian m_1\\times m_1 m_1\\times m_1 untuk X-1 X-1 dan \\sum_{22} \\sum_{22} adalah matrik kovarian m_2\\times m_2 m_2\\times m_2 untuk matrik X_2 X_2 yang dapat dihitung menggunakan persamaan (3.8). Oleh karena itu $$ \\left. \\begin{array} { l } { \\Sigma _ { 11 } = P _ { 1 } - p _ { 1 } p _ { 1 } ^ { T } } \\ { \\Sigma _ { 22 } = P _ { 2 } - p _ { 2 } p _ { 2 } ^ { T } } \\end{array} \\right. $$ dimana P_1=diag(p_1) P_1=diag(p_1) dan P_2=diag(p_2) P_2=diag(p_2) . Oleh karena itu, \\sum_{12} \\sum_{12} adalah matrik kovarian m_1 \\times m_2 m_1 \\times m_2 antara variabel X_1 X_1 dan X2 X2 dinyatakan dengan $$ \\left. \\begin{array}{l}{ \\Sigma _ { 12 } = E [ ( X _ { 1 } - \\mu _ { 1 } ) ( X _ { 2 } - \\mu _ { 2 } ) ^ { T } ] }\\{ = E [ X _ { 1 } X _ { 2 } ^ { T } ] - E [ X _ { 1 } ] E [ X _ { 2 } ] ^ { T } }\\{ = P _ { 12 } - \\mu _ { 1 } \\mu _ { 2 } ^ { T } }\\{ = P _ { 12 } - p _ { 1 } p _ { 2 } ^ { T } }\\end{array} \\right. $$ = \\left( \\begin{array} { c c c c } { p _ { 11 } - p _ { 1 } ^ { 1 } p _ { 1 } ^ { 2 } } & { p _ { 12 } - p _ { 1 } ^ { 1 } p _ { 2 } ^ { 2 } } & { \\cdots } & { p _ { 1 m _ { 2 } } - p _ { 1 } ^ { 1 } p _ { m _ { 2 } } ^ { 2 } } \\\\ { p _ { 21 } - p _ { 2 } ^ { 1 } p _ { 1 } ^ { 2 } } & { p _ { 22 } - p _ { 2 } ^ { 1 } p _ { 2 } ^ { 2 } } & { \\cdots } & { p _ { 2 m _ { 2 } } - p _ { 2 } ^ { 1 } p _ { m _ { 2 } } ^ { 2 } } \\\\ { \\vdots } & { \\vdots } & { \\ddots } & { \\vdots } \\\\ { p _ { m _ { 1 } 1 } - p _ { m _ { 1 } } ^ { 1 } p _ { 1 } ^ { 2 } } & { p _ { m _ { 1 } 2 } - p _ { m _ { 1 } } ^ { 1 } p _ { 2 } ^ { 2 } } & { \\cdots } & { p _ { m _ { 1 } m _ { 2 } } - p _ { m _ { 1 } } ^ { 1 } p _ { m _ { 2 } } ^ { 2 } } \\end{array} \\right) = \\left( \\begin{array} { c c c c } { p _ { 11 } - p _ { 1 } ^ { 1 } p _ { 1 } ^ { 2 } } & { p _ { 12 } - p _ { 1 } ^ { 1 } p _ { 2 } ^ { 2 } } & { \\cdots } & { p _ { 1 m _ { 2 } } - p _ { 1 } ^ { 1 } p _ { m _ { 2 } } ^ { 2 } } \\\\ { p _ { 21 } - p _ { 2 } ^ { 1 } p _ { 1 } ^ { 2 } } & { p _ { 22 } - p _ { 2 } ^ { 1 } p _ { 2 } ^ { 2 } } & { \\cdots } & { p _ { 2 m _ { 2 } } - p _ { 2 } ^ { 1 } p _ { m _ { 2 } } ^ { 2 } } \\\\ { \\vdots } & { \\vdots } & { \\ddots } & { \\vdots } \\\\ { p _ { m _ { 1 } 1 } - p _ { m _ { 1 } } ^ { 1 } p _ { 1 } ^ { 2 } } & { p _ { m _ { 1 } 2 } - p _ { m _ { 1 } } ^ { 1 } p _ { 2 } ^ { 2 } } & { \\cdots } & { p _ { m _ { 1 } m _ { 2 } } - p _ { m _ { 1 } } ^ { 1 } p _ { m _ { 2 } } ^ { 2 } } \\end{array} \\right) dimana P_{12} P_{12} menyatakan fungsi massa probabilitas gabungan untuk X X dinyatakan dalam persamaan (3.10) Kadang kala, setiap baris dan seitap kolom dari \\sum_{12} \\sum_{12} menjumlahkan ke nol. Misalkan, perhatikan baris i i dan kolom j j $$ \\left. \\begin{array} { l } { \\sum _ { k = 1 } ^ { m _ { 2 } } ( p _ { i k } - p _ { i } ^ { 1 } p _ { k } ^ { 2 } ) = ( \\sum _ { k = 1 } ^ { m _ { 2 } } p _ { i k } ) - p _ { i } ^ { 1 } = p _ { i } ^ { 1 } - p _ { i } ^ { 1 } = 0 } \\ { \\sum _ { k = 1 } ^ { m _ { 1 } } ( p _ { k j } - p _ { k } ^ { 1 } p _ { j } ^ { 2 } ) = ( \\sum _ { k = 1 } ^ { m _ { 1 } } p _ { k j } ) - p _ { j } ^ { 2 } = p _ { j } ^ { 2 } - p _ { j } ^ { 2 } = 0 } \\end{array} \\right. $$ yang mengikuti dari fakta bahwa menjumlahkan fungsi massa gabungan terhadap semua nilai X_2 X_2 , menghasilkan distribusi marginal $ X_1$, dan menjumlahkannya terhadap semua nilai X_1 X_1 menghasilkan distribusi marginal untuk X_ 2 X_ 2 Dikombinasikan dengan fakta bahwa \\sum_{11} \\sum_{11} dan \\sum_{22} \\sum_{22} juga memiliki baris dan jumlah kolom sama dengan nol melalui Persamaan. (3.7), matriks kovarian yang lengkap \\Sigma \\Sigma memiliki baris dan kolom yang berjumlah hingga nol.","title":"Matrik kovarian"},{"location":"Eksplorasi%20data-2/#matrik-kovarian-sample","text":"Matrik kovarian sampel dinyatakan dengan $$ \\hat { \\Sigma } = \\left( \\begin{array} { l l } { \\hat { \\Sigma } _ { 11 } } & { \\hat { \\Sigma } _ { 12 } } \\ { \\hat { \\Sigma } _ { 12 } ^ { T } } & { \\hat { \\Sigma } _ { 22 } } \\end{array} \\right) $$ dimana $$ \\left. \\begin{array} { l } { \\hat { \\Sigma } _ { 11 } = \\hat { P } _ { 1 } - \\hat { p } _ { 1 } \\hat { p } _ { 1 } ^ { T } } \\ { \\hat { \\Sigma } _ { 22 } = \\hat { P } _ { 2 } - \\hat { p } _ { 2 } \\hat { p } _ { 2 } ^ { T } } \\ { \\hat { \\Sigma } _ { 12 } = \\hat { P } _ { 12 } - \\hat { p } _ { 1 } \\hat { p } _ { 2 } ^ { T } } \\end{array} \\right. $$ Disini $\\hat P_1=diag(\\hat p_1) $ dan \\hat P_2=diag(\\hat p_2) \\hat P_2=diag(\\hat p_2) dan \\hat p_1 \\hat p_1 dan \\hat p_2 \\hat p_2 menyatakan masing masing fungsi massa probabilitas empiris untuk X_1 X_1 dan X_2 X_2 . Selanjutnya $ \\hat P_{12}$ menyatakan fungsi massa probabilitas gabungan empiris untuk X_1 X_1 dan X_2 X_2 ,dinyatakan dengan $$ \\hat { P } _ { 12 } ( i , j ) = \\hat { f } ( e _ { 1 i } , e _ { 2 j } ) = \\frac { 1 } { n } \\sum _ { k = 1 } ^ { n } I _ { i j } ( x _ { k } ) = \\frac { n _ { i j } } { n } = \\hat { p } _ { i j } $$ dimana I_{ij} I_{ij} adalah variabel indikator $$ I _ { i j } ( x _ { k } ) = \\left{ \\begin{array} { l l } { 1 } & { \\text { jika } x _ { k 1 } = e _ { i _ { 1 } } \\text { dan } x _ { k 2 } = e _ { j 2 } } \\ { 0 } & { \\text { lainnya } } \\end{array} \\right. $$ Contoh Kita terustkan dengan atribut kategorikal bivariate X_1 X_1 dan X_2 X_2 dalam contoh 3.7. Dari contoh 3.4 dan dari banyaknya kejadian untuk setiap nilai dari sepal width dalam tabel 3.3, kita miliki $$ \\hat { \\mu } _ { 1 } = \\hat { p } _ { 1 } = \\left( \\begin{array} { c } { 0.3 } \\ { 0.333 } \\ { 0.287 } \\ { 0.08 } \\end{array} \\right) \\quad \\hat { \\mu } _ { 2 } = \\hat { p } _ { 2 } = \\frac { 1 } { 150 } \\left( \\begin{array} { c } { 47 } \\ { 88 } \\ { 15 } \\end{array} \\right) = \\left( \\begin{array} { c } { 0.313 } \\ { 0.587 } \\ { 0.1 } \\end{array} \\right) $$ Kemudian, mean untuk X = \\left( \\begin{array} { l } { X _ { 1 } } \\\\ { X _ { 2 } } \\end{array} \\right) X = \\left( \\begin{array} { l } { X _ { 1 } } \\\\ { X _ { 2 } } \\end{array} \\right) dinyatakan dengan $$ \\hat { \\mu } = \\left( \\begin{array} { l } { \\hat { \\mu } _ { 1 } } \\ { \\hat { \\mu } _ { 2 } } \\end{array} \\right) = \\left( \\begin{array} { l } { \\hat { p } _ { 1 } } \\ { \\hat { p } _ { 2 } } \\end{array} \\right) = ( 0.3,0.333,0.287,0.08 | 0.313,0.587,0.1 ) ^ { T } $$ dari persamaan 3.5 kita miliki $$ \\hat { \\Sigma } _ { 11 } = \\left( \\begin{array} { r r r r } { 0.21 } & { - 0.1 } & { - 0.086 } & { - 0.024 } \\ { - 0.1 } & { 0.222 } & { - 0.096 } & { - 0.027 } \\ { - 0.086 } & { - 0.096 } & { 0.204 } & { - 0.023 } \\ { - 0.024 } & { - 0.027 } & { - 0.023 } & { 0.074 } \\end{array} \\right) $$ Dengan cara sama kita dapatkan $$ \\hat { \\Sigma } _ { 22 } = \\left( \\begin{array} { r r r } { 0.215 } & { - 0.184 } & { - 0.031 } \\ { - 0.184 } & { 0.242 } & { - 0.059 } \\ { - 0.031 } & { - 0.059 } & { 0.09 } \\end{array} \\right) $$ Selanjutnya kita gunakan banyaknya pengamatan dalam tabel 3.4 untuk mendapatkan fungsi massa probabilitas gabungan empiris untuk X_1 X_1 dan X_2 X_2 menggunakan persamaan 3.13, seperti yang telah diplot dalam gambar 3.2. Dari probabilitas ini kita dapatkan $$ E [ X _ { 1 } X _ { 2 } ^ { T } ] = \\hat { P } _ { 12 } = \\frac { 1 } { 150 } \\left( \\begin{array} { c c c } { 7 } & { 33 } & { 5 } \\ { 24 } & { 18 } & { 8 } \\ { 13 } & { 30 } & { 0 } \\ { 3 } & { 7 } & { 2 } \\end{array} \\right) = \\left( \\begin{array} { c c c } { 0.047 } & { 0.22 } & { 0.033 } \\ { 0.16 } & { 0.12 } & { 0.053 } \\ { 0.087 } & { 0.2 } & { 0 } \\ { 0.02 } & { 0.047 } & { 0.013 } \\end{array} \\right) $$ Selanjutnya kita memiliki $$ \\left. \\begin{array}{l}{ E [ X _ { 1 } ] E [ X _ { 2 } ] ^ { T } = \\hat { \\mu } _ { 1 } \\hat { \\mu } _ { 2 } ^ { T } = \\hat { p } _ { 1 } \\hat { p } _ { 2 } ^ { T } }\\{ = \\left( \\begin{array} { c } { 0.3 } \\ { 0.333 } \\ { 0.287 } \\ { 0.08 } \\end{array} \\right) \\left( \\begin{array} { c c c } { 0.313 } & { 0.587 } & { 0.1 } \\end{array} \\right) }\\{ = \\left( \\begin{array} { c c c } { 0.094 } & { 0.176 } & { 0.03 } \\ { 0.104 } & { 0.196 } & { 0.033 } \\ { 0.09 } & { 0.168 } & { 0.029 } \\ { 0.025 } & { 0.047 } & { 0.008 } \\end{array} \\right) }\\end{array} \\right. $$ Kita sekarang menghitung matrik kovarian sample antar atribut \\hat { \\Sigma } _ { 12} \\hat { \\Sigma } _ { 12} untuk X_1 X_1 dan X_2 X_2 menggunakan persamaan (3.11) sebagai berikut $$ \\left. \\begin{array}{l}{ \\hat { \\Sigma } _ { 12 } = \\hat { P } _ { 12 } - \\hat { p } _ { 1 } \\hat { p } _ { 2 } ^ { T } }\\{ = \\left( \\begin{array} { r r r } { - 0.047 } & { 0.044 } & { 0.003 } \\ { 0.056 } & { - 0.076 } & { 0.02 } \\ { - 0.003 } & { 0.032 } & { - 0.029 } \\ { - 0.005 } & { 0 } & { 0.005 } \\end{array} \\right) }\\end{array} \\right. $$ Kita dapat mengamati bahwa setiap baris dan kolam dalam \\hat { \\Sigma } _ { 12} \\hat { \\Sigma } _ { 12} jumlahnya adalah nol. Dengan menempatkan semuanya dari \\hat { \\Sigma } _ { 11} \\hat { \\Sigma } _ { 11} , \\hat { \\Sigma } _ { 22} \\hat { \\Sigma } _ { 22} ,dan \\hat { \\Sigma } _ { 12} \\hat { \\Sigma } _ { 12} kita dapatkan matrik kovarian sample sebagai berikut $$ \\hat { \\Sigma } = \\left( \\begin{array} { l l } { \\hat { \\Sigma } _ { 11 } } & { \\hat { \\Sigma } _ { 12 } } \\ { \\hat { \\Sigma } _ { 12 } ^ { T } } & { \\hat { \\Sigma } _ { 22 } } \\end{array} \\right)\\ = \\left( \\begin{array} { r r r r | r r r } { 0.21 } & { - 0.1 } & { - 0.086 } & { - 0.024 } & { - 0.047 } & { 0.044 } & { 0.003 } \\ { - 0.1 } & { 0.222 } & { - 0.096 } & { - 0.027 } & { 0.056 } & { - 0.076 } & { 0.02 } \\ { - 0.086 } & { - 0.096 } & { 0.204 } & { - 0.023 } & { - 0.003 } & { 0.032 } & { - 0.029 } \\ { - 0.024 } & { - 0.027 } & { - 0.023 } & { 0.074 } & { - 0.005 } & { 0 } & { 0.005 } \\ \\hline - 0.047 & { 0.056 } & { - 0.003 } & { - 0.005 } & { 0.215 } & { - 0.184 } & { - 0.031 } \\ { 0.044 } & { - 0.076 } & { 0.032 } & { 0 } & { - 0.184 } & { 0.242 } & { - 0.059 } \\ { 0.003 } & { 0.02 } & { - 0.029 } & { 0.005 } & { - 0.031 } & { - 0.059 } & { 0.09 } \\end{array} \\right) $$ Dalam \\hat \\Sigma \\hat \\Sigma , setiap baris dan kolom juga jumlahnya sama dengan nol Tabel 3.4 Jumlah pengamatan (n_{ij}) (n_{ij}) sepal length dan sepal width Gambar 3.2 Fungsi massa probabilitas gabungan empiris sepal length dan sepal width","title":"Matrik Kovarian Sample"},{"location":"Eksplorasi%20data-3%2BAtribut%20Depedency/","text":"Kebergantungan Atribut: Analisa Contingency \u00b6 Pengujian untuk kesalingbebasan dari dua variabel acak X_1 X_1 dan X_2 X_2 dapat dilakukan dengan analisa tabel contingency. Ide utamannya adalah untuk menetapkan kerangka pengusian hipotesa, dimana hipotesa null H_0 H_0 adalah X_1 X_1 dan X_2 X_2 adalah saling bebas, dan hipotesa lainnya H_1 H_1 adalah X_1 X_1 dan X_2 X_2 adalah saling bebas. Kita kemudian menghitung nilai statistik chi-square \\chi^2 \\chi^2 terhadap hipotesa null. Bergantung pada p p -value, kita apakah menerima atau menolak hipotesa null. Tabel Contigency Tabel contingency untuk X_1 X_1 dan X_2 X_2 adalah matrik m_1 \\times m_2 m_1 \\times m_2 dengan n_{ij} n_{ij} semua pasangan nilai ( e_{1j} e_{1j} , e_{2j} e_{2j} ) dalam sampel ukuran n n didefinisikan dengan $$ N _ { 12 } = n \\cdot \\hat { P } _ { 12 } = \\left( \\begin{array} { c c c c } { n _ { 11 } } & { n _ { 12 } } & { \\cdots } & { n _ { 1 m _ { 2 } } } \\ { n _ { 21 } } & { n _ { 22 } } & { \\cdots } & { n _ { 2 m _ { 2 } } } \\ { \\vdots } & { \\vdots } & { \\ddots } & { \\vdots } \\ { n _ { m _ { 1 } 1 } } & { n _ { m _ { 1 } 2 } } & { \\cdots } & { n _ { m _ { 1 } m _ { 2 } } } \\end{array} \\right) $$ Tabel 3.5 Tabel Contingency sepal length dengan sepal width dimana \\hat P_{12} \\hat P_{12} adalah fungsi massa probabilitas gabungan empiris untuk X_1 X_1 dan X_2 X_2 , dihitung dengan persamaaan (3.13). Tabel contingency adalah kemudian diperbesar dengan banyaknya baris dan kolom seperti berikut $$ N _ { 1 } = n \\cdot \\hat { p } _ { 1 } = \\left( \\begin{array} { c } { n _ { 1 } ^ { 1 } } \\ { \\vdots } \\ { n _ { m _ { 1 } } ^ { 1 } } \\end{array} \\right) \\quad N _ { 2 } = n \\cdot \\hat { p } _ { 2 } = \\left( \\begin{array} { c } { n _ { 1 } ^ { 2 } } \\ { \\vdots } \\ { n _ { m _ { 2 } } ^ { 2 } } \\end{array} \\right) $$ Perhatikan bahwa setiap entri baris dan kolom dan ukuran sampel memenuhi batasan berikut $$ n _ { i } ^ { 1 } = \\sum _ { j = 1 } ^ { m _ { 2 } } n _ { i j } \\quad n _ { j } ^ { 2 } = \\sum _ { j = 1 } ^ { m _ { 1 } } n _ { i j } \\quad n = \\sum _ { j = 1 } ^ { m _ { 2 } } n _ { j } ^ { 1 } = \\sum _ { l = 1 } ^ { m _ { 1 } } n _ { i } ^ { 2 } = \\sum _ { j = 1 } ^ { m _ { 1 } } \\sum _ { j = 1 } ^ { m _ { 2 } } n _ { i j } $$ Perlu dicatat bahwa kiedua N_1 N_1 dan N_2 N_2 masing masing memiliki distribusi multinomial dengan paramter p_1 =(p_{1}^1, p_{1}^1,....p_{m1}^1) p_1 =(p_{1}^1, p_{1}^1,....p_{m1}^1) dan p_2=(p_1 =(p_{1}^1, p_{1}^1,....p_{m1}^2) p_2=(p_1 =(p_{1}^1, p_{1}^1,....p_{m1}^2) . Selanjutnya N_{12} N_{12} juga memiliki distribusi multinomial dengan parameter P_{12}=\\{p_{ij}\\} P_{12}=\\{p_{ij}\\} untuk 1\\leq i\\leq m_1 1\\leq i\\leq m_1 dan 1\\leq j\\leq m_2 1\\leq j\\leq m_2 Contoh 3.9. Tabel Contingency Tabel 3.4 menunjukkan banyaknya pengamatan untuk diskritisasi atribut sepal length ( X_1 X_1 ) dan sepal width ( X_2 X_2 ). Pengembangan tabel dengan jumlah baris dan kolom dan ukuran sampel menghasilkan tabel contingency seperti yang ditunjukkan dalam tabel 3.5 Statistik \\chi^2 \\chi^2 dan Uji Hipotesa Di bawah hipotesis nol X_1 X_1 dan$ X _2 $ diasumsikan independen, yang berarti bahwa fungsi massa probabilitas gabungannya diberikan dengan $$ \\hat { p } _ { i j } = \\hat { p } _ { i } ^ { 1 } \\cdot \\hat { p } _ { j } ^ { 2 } $$ Terhadap asumsi independen, frekwensi harapatan untuk setiap pasangan dari nilai diberikan dengan $$ e _ { i j } = n \\cdot p _ { i j } = n \\cdot \\hat { p } _ { i } ^ { 1 } \\cdot \\hat { p } _ { j } ^ { 2 } = n \\cdot \\frac { n _ { i } ^ { 1 } } { n } \\cdot \\frac { n _ { j } ^ { 2 } } { n } = \\frac { n _ { i } ^ { 1 } n _ { j } ^ { 2 } } { n } $$ Akan tetapi dari sample kita telah memilik frekwensi pengamatan dari setiap pasangan nilai n_{ij} n_{ij} . Kita ingin menentukan apakah ada perbedaan signifikan dalam pengamatan dan frekwensi harapan untuk setiap pasangan nilai. Jika tidak ada perbedaan signifikan, maka asumsi independen adalah valid dan kita menerima hipotesa nol dimana atribut atribut tersebut adalah independen. Dengan kata lain jika ada perbedaan signifikan, maka hipotesa null akan ditolak dan kita menyimpulkan bahwa atribut atribut saling bergantung. Statistik \\chi^2 \\chi^2 , menentukan perbedaan nilai antara besarnya pengamatan dan besarnya harapan untuk setiap pasangan nilai, didefinisikan dengan sebagai berikut $$ \\chi ^ { 2 } = \\sum _ { i = 1 } ^ { m _ { 1 } } \\sum _ { j = 1 } ^ { m _ { 2 } } \\frac { ( n _ { i j } - e _ { i j } ) ^ { 2 } } { e _ { i j } } $$ Pada poin ini, kita perlu menentukan probabilitas dari nilai \\chi^2 \\chi^2 yang dihutung. Secara umum ini dapat lebih sulit jika kita tidak mengetahui distribusi sample dari statistik yang yang ada. Untungnya, untuk statistik \\chi^2 \\chi^2 distribusi samplingnya mengikuti fungsi padat chi-squared dengan q q derajat kebebasan $$ f ( x | q ) = \\frac { 1 } { 2 ^ { q / 2 } \\Gamma ( q / 2 ) } x ^ { \\frac { q } { 2 } - 1 } e ^ { - \\frac { x } { 2 } } $$ dimana fungsi gamma \\Gamma \\Gamma didefinsikan sebagai $$ \\Gamma ( k > 0 ) = \\int _ { 0 } ^ { \\infty } x ^ { k - 1 } e ^ { - x } d x $$ Derajat kebebasan q q menyatakan banyaknya parameter bebas. Dalam tabel contingency terdapat m_1 \\times m_2 m_1 \\times m_2 banyaknya pengamatan n_{ij} n_{ij} . Akan tetapi, perhatikan bahwa setiap baris i i dan setiap kolom j j harus jumlahnya sama dengan $n_{i}^1 $ dan n_j^2 n_j^2 . Oleh karena jumlah tepi baris dan kolom harus juga ditambahkan ke n n , maka kita harus menghapus parameter (m_1 + m_2) (m_1 + m_2) dari beberapa parameter bebas. Akan tetapi degan menghapus satu dari parameter katakan n_{m_1m_2} n_{m_1m_2} dua kali, sehingga kita harus menambahkan lagi satu ke jumlah. Besar derajat kebebasannya adalah $$ \\left. \\begin{array}{l}{ q = | \\operatorname { dom } ( X _ { 1 } ) | \\times | \\operatorname { dom } ( X _ { 2 } ) | - ( | \\operatorname { dom } ( X _ { 1 } ) | + | \\operatorname { dom } ( X _ { 2 } ) | ) + 1 }\\{ = m _ { 1 } m _ { 2 } - m _ { 1 } - m _ { 2 } + 1 }\\{ = ( m _ { 1 } - 1 ) ( m _ { 2 } - 1 ) }\\end{array} \\right. $$ p p -value \u00b6 p-value p-value dari statistik \\theta \\theta didefinisikan sebagai probabilitas mendapatkan nilai setidaknya sama dengan ekstrim dari nilai yang diamati, katakanlah z, di bawah hipotesis nol, didefinisikan sebagai $$ p - \\text { value } ( z ) = P ( \\theta \\geq z ) = 1 - F ( \\theta ) $$ dimana $ F ( \\theta )$ adalah distribusi probabilitas kumulatif untuk statistik Nilai p-value p-value ukuran seberapa mengejutkan nilai statistik yang diamati. Jika nilai yang diamati terletak di wilayah probabilitas rendah, maka nilainya lebih mengejutkan.Secara umum, semakin rendah nilai p, semakin mengejutkan nilai yang diamati, dan semakin menyakinkan untuk menolak hipotesa nol. Hipotesa nol ditolak jika p-value p-value diawah tingkat signifikan \\alpha \\alpha . Misalkan, jika \\alpha =0.01 \\alpha =0.01 maka kita menolak hipotesa nol jika p-value \\leq \\alpha p-value \\leq \\alpha . Tingkat signifikansi \\alpha \\alpha berkaitan dengan probabilitas penolakan hipotesa nol jika itu benar. Untuk tingkat signifikasn \\alpha \\alpha nilai dari statistik tes katakan z z dengan p p -value dari p p -value(z) = \\alpha \\alpha disebut dengan nilai kritis (critical value). Tes alternatif untuk menolak hipotesa nol adalah mengecek apaka \\chi^2 > z \\chi^2 > z seperti dalam kasus itu nilai p dari yang diamati \\chi^2 \\chi^2 nilai dibatasi oleh \\alpha \\alpha , p-value ( \\chi^2 \\leq p \\chi^2 \\leq p -value(z)= \\alpha \\alpha ) Nilai 1 - \\alpha 1 - \\alpha juga disebut dengan confidence level Contoh Perhatikan tabel contingency untuk sepal length dan sepal widht dalam tabel 3.5 . Kita menghitung besarnya harapan menggunakan persamaan (3.14). Nilai ini ditunjukkan dalam tabel 3.6 Misalkan kita mempunyai $$ e _ { 11 } = \\frac { n _ { 1 } ^ { 1 } n _ { 1 } ^ { 2 } } { n } = \\frac { 45 \\cdot 47 } { 150 } = \\frac { 2115 } { 150 } = 14.1 $$ Selanjutnya kita menggunaka persamaan 3.15 untuk menghitung nilai statistik \\chi^2 \\chi^2 yang diberikan dengan \\chi^2=21.8 \\chi^2=21.8 Oleh karena, jumlah derajat kebebasan dinyatakan dengan $$ q = ( m _ { 1 } - 1 ) \\cdot ( m _ { 2 } - 1 ) = 3 \\cdot 2 = 6 $$ Ploting dari fungsi padat \\chi^2 \\chi^2 dengan 6 derajat kebebasan adalah ditunjukkan dalam gambar 3.3. Dari distribusi \\chi^2 \\chi^2 kumulatif, kita dapatkan $$ p-value( 21.8 ) = 1 - F ( 21.8 | 6 ) = 1 - 0.9987 = 0.0013 $$ Pada tingkat signifikansi \u03b1 = 0,01, kita tentu akan dibenarkan dalam menolak hipotesis nol karena nilai besar dari statistik \\chi^2 \\chi^2 dan memang mengejutkan. Lebih lanjut, pada tingkat signifikansi 0,01, nilai kritis statistik adalah $$ z = F ^ { - 1 } ( 1 - 0.01 | 6 ) = F ^ { - 1 } ( 0.99 | 6 ) = 16.81 $$ Nilai kritis ini juga ditunjukkan dalam gambar 3.3, dan kita dapat dengan jelas melihat bahwa nilai pengamatan 21.8 adalah dalam daerah penolakan sehingga 21.8 > z =16.81. Oleh karena itu kita menolak hipotesa nol sehingga sepal length dan sepal width adalah saling bebas, dan menerima hipotesa lainnya bahwa keduanya adalah saling bergantung.","title":"Eksplorasi data 3+Atribut Depedency"},{"location":"Eksplorasi%20data-3%2BAtribut%20Depedency/#kebergantungan-atribut-analisa-contingency","text":"Pengujian untuk kesalingbebasan dari dua variabel acak X_1 X_1 dan X_2 X_2 dapat dilakukan dengan analisa tabel contingency. Ide utamannya adalah untuk menetapkan kerangka pengusian hipotesa, dimana hipotesa null H_0 H_0 adalah X_1 X_1 dan X_2 X_2 adalah saling bebas, dan hipotesa lainnya H_1 H_1 adalah X_1 X_1 dan X_2 X_2 adalah saling bebas. Kita kemudian menghitung nilai statistik chi-square \\chi^2 \\chi^2 terhadap hipotesa null. Bergantung pada p p -value, kita apakah menerima atau menolak hipotesa null. Tabel Contigency Tabel contingency untuk X_1 X_1 dan X_2 X_2 adalah matrik m_1 \\times m_2 m_1 \\times m_2 dengan n_{ij} n_{ij} semua pasangan nilai ( e_{1j} e_{1j} , e_{2j} e_{2j} ) dalam sampel ukuran n n didefinisikan dengan $$ N _ { 12 } = n \\cdot \\hat { P } _ { 12 } = \\left( \\begin{array} { c c c c } { n _ { 11 } } & { n _ { 12 } } & { \\cdots } & { n _ { 1 m _ { 2 } } } \\ { n _ { 21 } } & { n _ { 22 } } & { \\cdots } & { n _ { 2 m _ { 2 } } } \\ { \\vdots } & { \\vdots } & { \\ddots } & { \\vdots } \\ { n _ { m _ { 1 } 1 } } & { n _ { m _ { 1 } 2 } } & { \\cdots } & { n _ { m _ { 1 } m _ { 2 } } } \\end{array} \\right) $$ Tabel 3.5 Tabel Contingency sepal length dengan sepal width dimana \\hat P_{12} \\hat P_{12} adalah fungsi massa probabilitas gabungan empiris untuk X_1 X_1 dan X_2 X_2 , dihitung dengan persamaaan (3.13). Tabel contingency adalah kemudian diperbesar dengan banyaknya baris dan kolom seperti berikut $$ N _ { 1 } = n \\cdot \\hat { p } _ { 1 } = \\left( \\begin{array} { c } { n _ { 1 } ^ { 1 } } \\ { \\vdots } \\ { n _ { m _ { 1 } } ^ { 1 } } \\end{array} \\right) \\quad N _ { 2 } = n \\cdot \\hat { p } _ { 2 } = \\left( \\begin{array} { c } { n _ { 1 } ^ { 2 } } \\ { \\vdots } \\ { n _ { m _ { 2 } } ^ { 2 } } \\end{array} \\right) $$ Perhatikan bahwa setiap entri baris dan kolom dan ukuran sampel memenuhi batasan berikut $$ n _ { i } ^ { 1 } = \\sum _ { j = 1 } ^ { m _ { 2 } } n _ { i j } \\quad n _ { j } ^ { 2 } = \\sum _ { j = 1 } ^ { m _ { 1 } } n _ { i j } \\quad n = \\sum _ { j = 1 } ^ { m _ { 2 } } n _ { j } ^ { 1 } = \\sum _ { l = 1 } ^ { m _ { 1 } } n _ { i } ^ { 2 } = \\sum _ { j = 1 } ^ { m _ { 1 } } \\sum _ { j = 1 } ^ { m _ { 2 } } n _ { i j } $$ Perlu dicatat bahwa kiedua N_1 N_1 dan N_2 N_2 masing masing memiliki distribusi multinomial dengan paramter p_1 =(p_{1}^1, p_{1}^1,....p_{m1}^1) p_1 =(p_{1}^1, p_{1}^1,....p_{m1}^1) dan p_2=(p_1 =(p_{1}^1, p_{1}^1,....p_{m1}^2) p_2=(p_1 =(p_{1}^1, p_{1}^1,....p_{m1}^2) . Selanjutnya N_{12} N_{12} juga memiliki distribusi multinomial dengan parameter P_{12}=\\{p_{ij}\\} P_{12}=\\{p_{ij}\\} untuk 1\\leq i\\leq m_1 1\\leq i\\leq m_1 dan 1\\leq j\\leq m_2 1\\leq j\\leq m_2 Contoh 3.9. Tabel Contingency Tabel 3.4 menunjukkan banyaknya pengamatan untuk diskritisasi atribut sepal length ( X_1 X_1 ) dan sepal width ( X_2 X_2 ). Pengembangan tabel dengan jumlah baris dan kolom dan ukuran sampel menghasilkan tabel contingency seperti yang ditunjukkan dalam tabel 3.5 Statistik \\chi^2 \\chi^2 dan Uji Hipotesa Di bawah hipotesis nol X_1 X_1 dan$ X _2 $ diasumsikan independen, yang berarti bahwa fungsi massa probabilitas gabungannya diberikan dengan $$ \\hat { p } _ { i j } = \\hat { p } _ { i } ^ { 1 } \\cdot \\hat { p } _ { j } ^ { 2 } $$ Terhadap asumsi independen, frekwensi harapatan untuk setiap pasangan dari nilai diberikan dengan $$ e _ { i j } = n \\cdot p _ { i j } = n \\cdot \\hat { p } _ { i } ^ { 1 } \\cdot \\hat { p } _ { j } ^ { 2 } = n \\cdot \\frac { n _ { i } ^ { 1 } } { n } \\cdot \\frac { n _ { j } ^ { 2 } } { n } = \\frac { n _ { i } ^ { 1 } n _ { j } ^ { 2 } } { n } $$ Akan tetapi dari sample kita telah memilik frekwensi pengamatan dari setiap pasangan nilai n_{ij} n_{ij} . Kita ingin menentukan apakah ada perbedaan signifikan dalam pengamatan dan frekwensi harapan untuk setiap pasangan nilai. Jika tidak ada perbedaan signifikan, maka asumsi independen adalah valid dan kita menerima hipotesa nol dimana atribut atribut tersebut adalah independen. Dengan kata lain jika ada perbedaan signifikan, maka hipotesa null akan ditolak dan kita menyimpulkan bahwa atribut atribut saling bergantung. Statistik \\chi^2 \\chi^2 , menentukan perbedaan nilai antara besarnya pengamatan dan besarnya harapan untuk setiap pasangan nilai, didefinisikan dengan sebagai berikut $$ \\chi ^ { 2 } = \\sum _ { i = 1 } ^ { m _ { 1 } } \\sum _ { j = 1 } ^ { m _ { 2 } } \\frac { ( n _ { i j } - e _ { i j } ) ^ { 2 } } { e _ { i j } } $$ Pada poin ini, kita perlu menentukan probabilitas dari nilai \\chi^2 \\chi^2 yang dihutung. Secara umum ini dapat lebih sulit jika kita tidak mengetahui distribusi sample dari statistik yang yang ada. Untungnya, untuk statistik \\chi^2 \\chi^2 distribusi samplingnya mengikuti fungsi padat chi-squared dengan q q derajat kebebasan $$ f ( x | q ) = \\frac { 1 } { 2 ^ { q / 2 } \\Gamma ( q / 2 ) } x ^ { \\frac { q } { 2 } - 1 } e ^ { - \\frac { x } { 2 } } $$ dimana fungsi gamma \\Gamma \\Gamma didefinsikan sebagai $$ \\Gamma ( k > 0 ) = \\int _ { 0 } ^ { \\infty } x ^ { k - 1 } e ^ { - x } d x $$ Derajat kebebasan q q menyatakan banyaknya parameter bebas. Dalam tabel contingency terdapat m_1 \\times m_2 m_1 \\times m_2 banyaknya pengamatan n_{ij} n_{ij} . Akan tetapi, perhatikan bahwa setiap baris i i dan setiap kolom j j harus jumlahnya sama dengan $n_{i}^1 $ dan n_j^2 n_j^2 . Oleh karena jumlah tepi baris dan kolom harus juga ditambahkan ke n n , maka kita harus menghapus parameter (m_1 + m_2) (m_1 + m_2) dari beberapa parameter bebas. Akan tetapi degan menghapus satu dari parameter katakan n_{m_1m_2} n_{m_1m_2} dua kali, sehingga kita harus menambahkan lagi satu ke jumlah. Besar derajat kebebasannya adalah $$ \\left. \\begin{array}{l}{ q = | \\operatorname { dom } ( X _ { 1 } ) | \\times | \\operatorname { dom } ( X _ { 2 } ) | - ( | \\operatorname { dom } ( X _ { 1 } ) | + | \\operatorname { dom } ( X _ { 2 } ) | ) + 1 }\\{ = m _ { 1 } m _ { 2 } - m _ { 1 } - m _ { 2 } + 1 }\\{ = ( m _ { 1 } - 1 ) ( m _ { 2 } - 1 ) }\\end{array} \\right. $$","title":"Kebergantungan Atribut: Analisa Contingency"},{"location":"Eksplorasi%20data-3%2BAtribut%20Depedency/#pp-value","text":"p-value p-value dari statistik \\theta \\theta didefinisikan sebagai probabilitas mendapatkan nilai setidaknya sama dengan ekstrim dari nilai yang diamati, katakanlah z, di bawah hipotesis nol, didefinisikan sebagai $$ p - \\text { value } ( z ) = P ( \\theta \\geq z ) = 1 - F ( \\theta ) $$ dimana $ F ( \\theta )$ adalah distribusi probabilitas kumulatif untuk statistik Nilai p-value p-value ukuran seberapa mengejutkan nilai statistik yang diamati. Jika nilai yang diamati terletak di wilayah probabilitas rendah, maka nilainya lebih mengejutkan.Secara umum, semakin rendah nilai p, semakin mengejutkan nilai yang diamati, dan semakin menyakinkan untuk menolak hipotesa nol. Hipotesa nol ditolak jika p-value p-value diawah tingkat signifikan \\alpha \\alpha . Misalkan, jika \\alpha =0.01 \\alpha =0.01 maka kita menolak hipotesa nol jika p-value \\leq \\alpha p-value \\leq \\alpha . Tingkat signifikansi \\alpha \\alpha berkaitan dengan probabilitas penolakan hipotesa nol jika itu benar. Untuk tingkat signifikasn \\alpha \\alpha nilai dari statistik tes katakan z z dengan p p -value dari p p -value(z) = \\alpha \\alpha disebut dengan nilai kritis (critical value). Tes alternatif untuk menolak hipotesa nol adalah mengecek apaka \\chi^2 > z \\chi^2 > z seperti dalam kasus itu nilai p dari yang diamati \\chi^2 \\chi^2 nilai dibatasi oleh \\alpha \\alpha , p-value ( \\chi^2 \\leq p \\chi^2 \\leq p -value(z)= \\alpha \\alpha ) Nilai 1 - \\alpha 1 - \\alpha juga disebut dengan confidence level Contoh Perhatikan tabel contingency untuk sepal length dan sepal widht dalam tabel 3.5 . Kita menghitung besarnya harapan menggunakan persamaan (3.14). Nilai ini ditunjukkan dalam tabel 3.6 Misalkan kita mempunyai $$ e _ { 11 } = \\frac { n _ { 1 } ^ { 1 } n _ { 1 } ^ { 2 } } { n } = \\frac { 45 \\cdot 47 } { 150 } = \\frac { 2115 } { 150 } = 14.1 $$ Selanjutnya kita menggunaka persamaan 3.15 untuk menghitung nilai statistik \\chi^2 \\chi^2 yang diberikan dengan \\chi^2=21.8 \\chi^2=21.8 Oleh karena, jumlah derajat kebebasan dinyatakan dengan $$ q = ( m _ { 1 } - 1 ) \\cdot ( m _ { 2 } - 1 ) = 3 \\cdot 2 = 6 $$ Ploting dari fungsi padat \\chi^2 \\chi^2 dengan 6 derajat kebebasan adalah ditunjukkan dalam gambar 3.3. Dari distribusi \\chi^2 \\chi^2 kumulatif, kita dapatkan $$ p-value( 21.8 ) = 1 - F ( 21.8 | 6 ) = 1 - 0.9987 = 0.0013 $$ Pada tingkat signifikansi \u03b1 = 0,01, kita tentu akan dibenarkan dalam menolak hipotesis nol karena nilai besar dari statistik \\chi^2 \\chi^2 dan memang mengejutkan. Lebih lanjut, pada tingkat signifikansi 0,01, nilai kritis statistik adalah $$ z = F ^ { - 1 } ( 1 - 0.01 | 6 ) = F ^ { - 1 } ( 0.99 | 6 ) = 16.81 $$ Nilai kritis ini juga ditunjukkan dalam gambar 3.3, dan kita dapat dengan jelas melihat bahwa nilai pengamatan 21.8 adalah dalam daerah penolakan sehingga 21.8 > z =16.81. Oleh karena itu kita menolak hipotesa nol sehingga sepal length dan sepal width adalah saling bebas, dan menerima hipotesa lainnya bahwa keduanya adalah saling bergantung.","title":"pp-value"},{"location":"Eksplorasi%20data/","text":"Ekplorasi data \u00b6 Oleh Mulaab Atribut Data numerik \u00b6 Dalam bab ini, kita membahas metode statistik dasar untuk analisis ekploarasi data atribut numerik. Kita membahas ukuran kecenderungan pusat (central tendency), ukuran dispersi atau sebaran, dan ukuran ketergantungan linier atau hubungan antara atribut. Kita menekankan hubungan antara probabilistik dan geometris dan aljabar dari sudut pandang data matriks Analisa univariat \u00b6 Analisis univariat dilakukan pada atribut tunggal ( X X ); dengan demikian matriks data D bisa dianggap sebagai matriks n \u00d7 1 n \u00d7 1 , atau sebagai vektor kolom, yang dianyatakan dengan $$ D = \\left( \\begin{array} { c } { X } \\ \\hline x _ { 1 } \\ { x _ { 2 } } \\ { \\vdots } \\ { x _ { n } } \\end{array} \\right) $$ dimana X X adalah atribut numerik yang dimaksudkan, dengan $ x _ { i } \\in \\mathbb{R} $. X X diasumsikan adalah variabel random, dengan setiap titik $ x _ { i } ( 1 \\leq i \\leq n ) $ , merupakan variabel acak. Kita asumsikan bawa data pengamatan adalah. Kami berasumsi bahwa data yang diamati adalah sampel acak yang diambil dari X X , artinya, setiap variabel x_i x_i adalah saling bebas dan berdistribus sama (iid). Dalam sudut pandang vektor, kami memperlakukan sampel sebagai vektor n-dimensi, dan menulis $ X \\in \\mathbb R ^ { n } $ Secara umum, fungsi padat probabilitas atau fungsi mass f(x) f(x) dan fungsi distribusi kumulatif $ F(x),$ untuk atribut X X keduanya tidak diketahui. Akan tetapi, kita dapat mengestimasi distribusi ini langsung dar data sample, juga juga memungkinkan kita untuk menghitung beberapa parameter penting populasi. Fungsi distribusi Kumulatif Empiris \u00b6 Fungsi distribusi kumulatif empiris (CDF ) dari X X dinyatakan dengan $$ \\hat { F } ( x ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i } \\leq x ) $$ dimana $$ I ( x _ { i } \\leq x ) = \\left{ \\begin{array} { l l } { 1 } & { \\text { if } x _ { i } \\leq x } \\ { 0 } & { \\text { if } x _ { i } > x } \\end{array} \\right. $$ adalah variabel indikator biner yang menyatakan variabel indikator biner yang menunjukkan apakah kondisi yang diberikan terpenuhi atau tidak. Fungsi distribusi kumulatif Invers \u00b6 Definisi fungsi distribusi kumulatif invers atau fungsi quantile untuk variabel acak X X sebagai berikut : $$ F ^ { - 1 } ( q ) = \\operatorname { min } { x | \\hat { F } ( x ) \\geq q } \\quad \\text { for } q \\in [ 0,1 ] $$ Fungsi distribusi kumulatif Invers empiris dapat diperoleh dari persamaan (2) Fungsi massa Probabilitas Empiris \u00b6 Fungsi massa probabilitas empiris dari X X dinyatakan dengan $$ \\hat { f } ( x ) = P ( X = x ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i } = x ) $$ dimana $$ I ( x _ { i } = x ) = \\left{ \\begin{array} { l l } { 1 } & { \\text { if } x _ { i } = x } \\ { 0 } & { \\text { if } x _ { i } \\neq x } \\end{array} \\right. $$ Fungsi massa probabilitas empiris juga menempatkan massa probabitas \\frac {1}{n} \\frac {1}{n} pada setipa titik x_i x_i Mengukur kecenduran terpusat \u00b6 Ukuran ini memberikan indikasi tentang konsentrasi massa probabilitas , nilai tengah dan lainnya. Mean \u00b6 Mean juga disebut dengan nilai harapan dari variabel acak X X adalah rata rata aritmetika dari nilai X X . Itu merupakan salah satu dari kecenderungan terpusat dari X X . Mean atau nilai harapan dari variabel acak X X didefinisikan dengan $$ \\mu = E [ X ] = \\sum _ { x } x f ( x ) $$ diman f(x) f(x) adalah fungsi massa probabilitas dari X X . Nilai harapan dari variabel acak kontinu X X dinyakan dengan $$ \\mu = E [ X ] = \\int _ { - \\infty } ^ { \\infty } x f ( x ) d x $$ dimana f(x) f(x) adalah fungsi padat probabilitas dari X X . Sample Mean . Sample mean adalah statistik, yaitu fungsi $ \\hat { \\mu } : { x _ { 1 } , x _ { 2 } , \\ldots , x _ { n } } \\rightarrow \\mathbb R$, didefinisikan sebagai nilai rata-rata dari x_i x_i : $$ \\hat { \\mu } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } $$ nilai adalah sebagai pengestimasi nilai mean yang tidak diketahui dari X X . Nilai tersebut diperoleh dengan memasukkan dalam fungsi massa probabilitas empiris dalam persamaan (7) $$ \\hat { \\mu } = \\sum _ { x } x \\hat { f } ( x ) = \\sum _ { x } x ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i } = x ) ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } $$ Sample mean adalah tidak bias . Estimator \\hat { \\theta } \\hat { \\theta } disebut dengan unbiased estimatore (stimator tidak bias) untuk parameter \\theta \\theta jika E[\\hat \\theta] = \\theta E[\\hat \\theta] = \\theta untuk setiap kemungkinan nilai dari \\theta \\theta . Sample mean \\hat \\mu \\hat \\mu adalah unbiased estimator untuk mean populasi \\mu \\mu sehingga $$ E [ \\hat { \\mu } ] = E [ \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } ] = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } E [ x _ { i } ] = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } \\mu = \\mu $$ dimana kita gunakan fakta bahwa variabel acak x_i x_i adalah IID sesuai dengan X X , yang berarti bahwa mereka memiliki rata-rata \\mu \\mu yang sama seperti X X , yaitu,$ E [x_i] =\\mu$ untuk semua x_i x_i . Kita juga menggunakan fakta bahwa fungsi ekpektasi E E adalah linier operator yaitu untuk suatu dua bilangan acak X X dan Y Y dan bilangan real a a dan b b , kita memiliki E [ a X + b Y ] = a E [ X ] + b E [ Y ] E [ a X + b Y ] = a E [ X ] + b E [ Y ] Robustnes Kita mengatakan bahwa statistik adalah robust jika tidak dipengaruhi oleh suatu nilai ekstrim ( misal outlier/pencilan) dalam data. Rata-rata sampel sayangnya tidak kuat karena ada satu nilai besar (outlier) dapat mejadikan rata-rata yang tidak sebenarnya. Ukuran yang lebih robust adalah trimmed mean yang didapatkan setalah mengabaikan sebagian kecil dari nilai nilai ekstrim pada salah satu ujungnya. Median Median dari suatu variabel acak didefinisikan dengan nilai m m sehingga $$ P ( X \\leq m ) \\geq \\frac { 1 } { 2 } \\text { and } P ( X \\geq m ) \\geq \\frac { 1 } { 2 } $$ Degan kata lain, median m m adalah nilai paling tengan (middle-most). Dalam istliah (invers) cumulatif distribution function , median m m dinyatakan dengan $$ F ( m ) = 0.5 \\text { or } m = F ^ { - 1 } ( 0.5 ) $$ Sample median dapat diperoleh dari Fungsi distribusi kumulatif invers atau fungsi distribusi kumulatif invers empiris dengan dihitung $$ \\hat { F } ( m ) = 0.5 \\text { atau } m = \\hat { F } ^ { - 1 } ( 0.5 ) $$ Pendekatan paling sederhan untuk menghitung sample median adalah pertama kai dari mengurutkan semua nilai x_i x_i (i \\in [1,n]) (i \\in [1,n]) dengan urutan naik. Jika n n adalah ganjil , media adalah nilai pada posisi \\frac {n+1}{2} \\frac {n+1}{2} . Jika n n adalah genap, nilai padan posisi \\frac {n}{2} \\frac {n}{2} dan \\frac {n}{2}+1 \\frac {n}{2}+1 adalah keduanaya median. Tidak seperti mean, media adalah robust, sehingga ia tidak dipengaruhi oleh banyak nilai extrim. Juga nilai tersebut terjadi dalam sample dan nilai yang bisa diasumsikan oleh variabel acak. Mode Nilai mode dari variabel acak adalah nilai dimana fungsi massa probabilitas atau fungsi padat probabilitas mencapai nilai maximumnya, bergantung pada apakah X X adalah diskrit atau kontinu. Sample mode adalah nila untuk fungsi probabilitas empiris mencapai nilai maksimum, dinyatakan dengan $$ mode(X) =\\arg \\underset{x}{max} {\\hat f(x)} $$ Mode ini mungkin bukan ukuran kecenderungan sentral yang sangat berguna untuk sampel, karena kemungkinan elemen yang tidak representatif menjadi elemen yang paling sering muncul. Selanjutnya, jika semua nilai dalam sampel berbeda, maka masing-masing akan menjadi mode Contoh . (Sample Mean, Median, dan Mode) . Perhatikan atribut sepal length (Xi) (Xi) dalam data iris. Data iris, dimana nilainya seperti yang ditunjukkan dalam tebel 1.2 . Sample mean dinyatakan dengan $$ \\hat { \\mu } = \\frac { 1 } { 150 } ( 5.9 + 6.9 + \\cdots + 7.7 + 5.1 ) = \\frac { 876.5 } { 150 } = 5.843 $$ Gambar 2.1 menunjukkan semua dari 150 nilai sepal length dan sample mean. Gambar 2.2a menunjukkan fungsi distribusi kumulatif empiri dan gambar 2.2b menunjukkan fungsi distribusi kumulatif empiris untuk sepal length Karena n=150 n=150 adalah genap, sample median adalah nilai pada posisi \\frac {n}{2}=75 \\frac {n}{2}=75 dan \\frac {n}{2}+1=76 \\frac {n}{2}+1=76 setelah diurutkan. Untuk sepal length kedua nilainya adalah 5.8, kemudian sample media adalah 5.8 . Dari fungsi distribusi kumulatif invers dalam gambar 2.2b, kita dapat melihat bahwa $$ \\hat { F } ( 5.8 ) = 0.5 \\text { or } 5.8 = \\hat { F } ^ { - 1 } ( 0.5 ) $$ Sample mode untuk sepal length adalah 5. yang dapat dilihat dari frequency dari 5 dalam gambar 2.1. Massa probabilitas empiris pada x=5 x=5 adalah $$ \\hat { f } ( 5 ) = \\frac { 10 } { 150 } = 0.067 $$ Mengukur sebaran (dispersion) \u00b6 Mengukur dispersi memberikan indikasi tentang sebaran atau variasi pada nilai nilai variabel acak. Jangkauan Jangkauan nilai atau secara sederhana jangkauan (range) variabel acak X X adalah perbedaan antara nilai maximum dan nilai minimum dari X X dinyatakan dengan $$ r = \\operatorname { max } { X } - \\operatorname { min } { X } $$ Sample range adalah statistik, dinyatakan dengan $$ \\ \\hat r = {\\overset{n}{\\underset{i=1}{max }}} {{{x_i}}}-{\\overset{n}{\\underset{i=1}{min }}} {{{x_i}}} $$ Dengan definisi, jangkauan adalah sensitif terhadap nilai extrime sehingga tidak robust. Jangkauan antar interquartile Quartile adalah nilai khusus dari fungsi quantile persaman (2.2) yang membagi data kedalam empat bagian. Furthermore quartile terkati dengan nilai-nilai quantile 0.25, 0.5, dan 0.74 dan 1.0. Quantile pertama adalah nilai q_1 =F^{-1}(0.25) q_1 =F^{-1}(0.25) 25% dari sebelah kiri rentang titik, kuartile ke dua adalah sama dengan nilai median q_2 =F^{-1}(0.5) q_2 =F^{-1}(0.5) , 50 % dari sebelah kiri data dan q_3=F^{-1}(0.75) q_3=F^{-1}(0.75) adalah nilai 75% dari sebelah kiri dan quantile ke empat adalah nilai maximum dari X X , 100 % sebelah kiri dari rentang data. Ukuran yang lebih robust dari seberan X X adalah jangkauan interquartile (IQR) dinyatakan dengan $$ I Q R = q _ { 3 } - q _ { 1 } = F ^ { - 1 } ( 0.75 ) - F ^ { - 1 } ( 0.25 ) $$ Variansi dan standar deviasi Variansi dari variabel acak X X memberikan pengukuran berapa banyak nilai nilai dari penyimpangan X X dari rata-rata atau nilai harapan dari X X . Lebih tepatnya variansi adalah nilai harapan dari penyimpangan dari mean yang dikuadratkan yang didefinisikan dengan $$ \\sigma ^ { 2 } = \\operatorname { var } ( X ) = E [ ( X - \\mu ) ^ { 2 } ] = \\left{ \\begin{array} { l l } { \\sum _ { x } ( x - \\mu ) ^ { 2 } f ( x ) } & { \\text {jika } X \\text { adalah diskrit } } \\ { \\int _ { - \\infty } ^ { \\infty } ( x - \\mu ) ^ { 2 } f ( x ) d x } & { \\text { jika } X \\text { adalah kontinu } } \\end{array} \\right. $$ Standar deviasi \\sigma \\sigma didefinisikan sebagai akar kuadrat positif dari variansi \\sigma^2 \\sigma^2 . Kita dapat juga menulis variansi sebagai selisih antara ekpektasi X^2 X^2 dan akar dari ekpektasi X X : Variansi Sampel Variansi sampel didefinisikan dengan $$ \\hat { \\sigma } ^ { 2 } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\hat { \\mu } ) ^ { 2 } $$ Standar deviasi adalah akar dari variansi sample yang dinyatakan dengan $$ \\hat { \\sigma } = \\sqrt { \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\hat { \\mu } ) ^ { 2 } } $$ Analisa Bivariate \u00b6 Dalam analisa bivariate, kita memandang dua atribut pada waktu yang sama. Kita fokus untuk memahami keterkaitan atau kebergantunga antara dua variabel atau atribut tersebut, jika ada. Kita lalu membatasi pada dua variabel X_1 X_1 dan X_2 X_2 , dengan D D dinyatakan sebagai matrik dengan ukuran $ n \\times 2$ $$ D = \\left( \\begin{array} { c c } { X _ { 1 } } & { X _ { 2 } } \\ \\hline x _ { 11 } & { x _ { 12 } } \\ { x _ { 21 } } & { x _ { 22 } } \\ { \\vdots } & { \\vdots } \\ { x _ { n 1 } } & { x _ { n 2 } } \\end{array} \\right) $$ Secara geometri, kita dapat memandang D D dalam dua cara. Itu dapat dianggap sebagai n n titik atau vektor dalam 2-ruang dimensi terhadap atribut X_1 X_1 dan X_2 X_2 yaitu x_i =(x_{i1},x_{i2})^T \\in \\mathbb R^2 x_i =(x_{i1},x_{i2})^T \\in \\mathbb R^2 .Selain itu dapat dilihat sebagai 2 titik atau vektor dalam n n -ruang dimensi yang berisi titik, yaitu setiap kolom adalah vektor dalam $ \\mathbb R^n$ sebagai berikut : $$ \\left. \\begin{array} { l } { X _ { 1 } = ( x _ { 11 } , x _ { 21 } , \\ldots , x _ { n 1 } ) ^ { T } } \\ { X _ { 2 } = ( x _ { 12 } , x _ { 22 } , \\ldots , x _ { n 2 } ) ^ { T } } \\end{array} \\right. $$ Dalam sudut pandang probabilistik, vektor kolom X=(X_1,X_2)^T X=(X_1,X_2)^T dianggapa variabel acak bivariate dan titik titik x _ { i } ( 1 \\leq i \\leq n ) x _ { i } ( 1 \\leq i \\leq n ) dinyatakan sebagai sampel acak yang diperoleh dari X X , yaitu x_i x_i dianggap independent and identically distributed (iid) seperti X X . Fungsi Massa Probabilitas Gabungan Empiris Fungsi Massa Probabilitas Gabungan Empiris untuk X X dinyatakan dengan $$ \\hat { f } ( x ) = P ( X = x ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i } = x ) $$ \\hat { f } ( x _ { 1 } , x _ { 2 } ) = P ( X _ { 1 } = x _ { 1 } , X _ { 2 } = x _ { 2 } ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i 1 } = x _ { 1 } , x _ { i 2 } = x _ { 2 } ) \\hat { f } ( x _ { 1 } , x _ { 2 } ) = P ( X _ { 1 } = x _ { 1 } , X _ { 2 } = x _ { 2 } ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i 1 } = x _ { 1 } , x _ { i 2 } = x _ { 2 } ) dimana I I adalah variabel indikator yang bernilai 1 jika argumen argumennya benar $$ I ( x _ { i } = x ) = \\left{ \\begin{array} { l l } { 1 } & { \\text { jika } x _ { i 1 } = x _ { 1 } \\text { dan } x _ { i 2 } = x _ { 2 } } \\ { 0 } & { \\text { untuk yang lainnya } } \\end{array} \\right. $$ Seperti dalam kasus univariate, fungsi probabilitas menempatkan massa probabilitas \\frac {1}{n} \\frac {1}{n} pada setiap objek dalam data sampel. Mengukur Dispersi \u00b6 Mean Rata rata bivariate didefinisikan sebagai nilai harapan dari variabel acak vektor X X , didefinisikan sebagai berikut : $$ \\mu = E [ X ] = E \\left[ \\left( \\begin{array} { l } { X _ { 1 } } \\ { X _ { 2 } } \\end{array} \\right) \\right] = \\left( \\begin{array} { l } { E [ X _ { 1 } ] } \\ { E [ X _ { 2 } ] } \\end{array} \\right) = \\left( \\begin{array} { l } { \\mu _ { 1 } } \\ { \\mu _ { 2 } } \\end{array} \\right) $$ Dengan kata lain, rata-rata bivariate adalah nilai harapan dari masing masing atribut. Rata-rata sampel dapat diperoleh dari \\hat f_{x_1} \\hat f_{x_1} dan \\hat f_{x_2} \\hat f_{x_2} , fungsi massa probabilitas empiris dari X_1 X_1 dan X_2 X_2 , menggunakan persamaan (2.5). Dapat juga dihitung dari gabungan fungsi massa probabilitas empiris dalam persamaan (2.17) $$ \\hat { \\mu } = \\sum _ { x } x \\hat { f } ( x ) = \\sum _ { x } x \\left( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i } = x )\\right ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } $$ Variansi Kita dapat menghitung variansi masing masing atribut, yaitu \\sigma_1^2 \\sigma_1^2 untuk X_1 X_1 dan \\sigma_2^2 \\sigma_2^2 untuk X_2 X_2 mengggunkan persamaan (2.8). Variansi secara keseluruhan (1.4) dinyatakan dengan $$ var(D)=\\sigma_1^2 +\\sigma_2^2 $$ Variansi sampel \\hat \\sigma_1^2 + \\hat \\sigma_2^2 \\hat \\sigma_1^2 + \\hat \\sigma_2^2 dapat diestimasi dengan menggunakanpersamaan (2.10) dan jumlah variansi sample adalah \\sigma_1^2 +\\sigma_2^2 \\sigma_1^2 +\\sigma_2^2 2.2.2. Mengukur keterkaitan Covarian Kovarian antara dua atribut X_1 X_1 dan X_2 X_2 mengukur keterkaitan antara kebergantungan linier diantaranya dan didefinisikan dengan $$ \\sigma _ { 12 } = E [ ( X _ { 1 } - \\mu _ { 1 } ) ( X _ { 2 } - \\mu _ { 2 } ) ] $$ Dengan linieraritas dari harapan, kita miliki $$ \\left. \\begin{array}{l}{ \\sigma _ { 12 } = E [ ( X _ { 1 } - \\mu _ { 1 } ) ( X _ { 2 } - \\mu _ { 2 } ) ] }\\{ = E [ X _ { 1 } X _ { 2 } - X _ { 1 } \\mu _ { 2 } - X _ { 2 } \\mu _ { 1 } + \\mu _ { 1 } \\mu _ { 2 } ] }\\{ = E [ X _ { 1 } X _ { 2 } ] - \\mu _ { 2 } E [ X _ { 1 } ] - \\mu _ { 1 } E [ X _ { 2 } ] + \\mu _ { 1 } \\mu _ { 2 } }\\{ = E [ X _ { 1 } X _ { 2 } ] - \\mu _ { 1 } \\mu _ { 2 } }\\{ = E [ X _ { 1 } X _ { 2 } ] - E [ X _ { 1 } ] E [ X _ { 2 } ] }\\end{array} \\right. $$ Persamaan (2.21) dapat dianggap sebagai generalisasi dari variansi univariate persamaan (2.9) pada kasus bivariate. Jika X_1 X_1 dan X_2 X_2 adalah variabel acak saling bebas, maka kita dapat simpulkan bahwa covariannya adalah nol. Ini karena jika X_1 X_1 dan X_2 X_2 adalah saling bebas, maka kita memiliki $$ E [ X _ { 1 } X _ { 2 } ] = E [ X _ { 1 } ] \\cdot E [ X _ { 2 } ] $$ yang pada akhirnya menyiratkan bahwa $$ \\sigma{12}= 0 $$ Namaun sebaliknya tidak benar. Yaitu jika \\sigma_{12}=0 \\sigma_{12}=0 , kita tidak dapat mengklaim bahwa $X_1 $ dan X_2 X_2 adalah saling bebas. Semuanya kita katakan bahwa tidak adalah kebergantung linier antara keduanya. Kovarian sampel antra X1 X1 dan X_2 X_2 dinyatakan dengan $$ \\hat { \\sigma } _ { 12 } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i 1 } - \\hat { \\mu } _ { 1 } ) ( x _ { i 2 } - \\hat { \\mu } _ { 2 } ) $$ Korelasi Korelasi antara variabel X_1 X_1 dan X_2 X_2 adalah standarisasi kovarian, yang didapatkan dengan menormalisasi kovarian dengan standar deviasi masing masing variabel dinyatakan dengan \\rho _ { 12 } = \\frac { \\sigma _ { 12 } } { \\sigma _ { 1 } \\sigma _ { 2 } } = \\frac { \\sigma _ { 12 } } { \\sqrt { \\sigma _ { 1 } ^ { 2 } \\sigma _ { 2 } ^ { 2 } } } $$ Korelasi sample untuk atribut $X_1$ dan $X_2$ dinyatakan dengan $$ \\hat { \\rho } _ { 12 } = \\frac { \\hat { \\sigma } _ { 12 } } { \\hat { \\sigma } _ { 1 } \\hat { \\sigma } _ { 2 } } = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i 1 } - \\hat { \\mu } _ { 1 } ) ( x _ { i 2 } - \\hat { \\mu } _ { 2 } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i 1 } - \\hat { \\mu } _ { 1 } ) ^ { 2 } \\sum _ { i = 1 } ^ { m } ( x _ { i 2 } - \\hat { \\mu } _ { 2 } ) ^ { 2 } } } \\rho _ { 12 } = \\frac { \\sigma _ { 12 } } { \\sigma _ { 1 } \\sigma _ { 2 } } = \\frac { \\sigma _ { 12 } } { \\sqrt { \\sigma _ { 1 } ^ { 2 } \\sigma _ { 2 } ^ { 2 } } } $$ Korelasi sample untuk atribut $X_1$ dan $X_2$ dinyatakan dengan $$ \\hat { \\rho } _ { 12 } = \\frac { \\hat { \\sigma } _ { 12 } } { \\hat { \\sigma } _ { 1 } \\hat { \\sigma } _ { 2 } } = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i 1 } - \\hat { \\mu } _ { 1 } ) ( x _ { i 2 } - \\hat { \\mu } _ { 2 } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i 1 } - \\hat { \\mu } _ { 1 } ) ^ { 2 } \\sum _ { i = 1 } ^ { m } ( x _ { i 2 } - \\hat { \\mu } _ { 2 } ) ^ { 2 } } } Matrik Kovarian Variansi dari untuk dua atribut X_1 X_1 dan X_2 X_2 dapat diringkas dalam matrik covarianse bujursangkar denga ukuran $2 \\times 2 $ dinyatakan dengan $$ \\left. \\begin{array}{l}{ \\Sigma = E [ ( X - \\mu ) ( X - \\mu ) ^ { T } ] }\\{ = E \\left[ \\left( \\begin{array} { c } { X _ { 1 } - \\mu _ { 1 } } \\ { X _ { 2 } - \\mu _ { 2 } } \\end{array} \\right) ( X _ { 1 } - \\mu _ { 1 } \\quad X _ { 2 } - \\mu _ { 2 } ) \\right ] }\\{ = \\left( \\begin{array} { c c } { E [ ( X _ { 1 } - \\mu _ { 1 } ) ( X _ { 1 } - \\mu _ { 1 } ) ] } & { E [ ( X _ { 1 } - \\mu _ { 1 } ) ( X _ { 2 } - \\mu _ { 2 } ) ] } \\ { E [ ( X _ { 2 } - \\mu _ { 2 } ) ( X _ { 1 } - \\mu _ { 1 } ) ] } & { E [ ( X _ { 2 } - \\mu _ { 2 } ) ( X _ { 2 } - \\mu _ { 2 } ) ] } \\end{array} \\right) }\\{ = \\left( \\begin{array} { c c } { \\sigma _ { 1 } ^ { 2 } } & { \\sigma _ { 12 } } \\ { \\sigma _ { 21 } } & { \\sigma _ { 2 } ^ { 2 } } \\end{array} \\right) }\\end{array} \\right. $$ Karena \\sigma_{12}=\\sigma_{21} \\sigma_{12}=\\sigma_{21} , $\\Sigma $ adalah matrik simetris. Matrik vovarian merekam variansi tertentu atribut pada diagonal utamanya, dan informasi covarian pada elemen element bukan diagonal. Total variance dari dua atribut dinyatakan sebagai jumlah elemen elemen diagonal dari $ \\Sigma $ , yang juga disebut trace dari $ \\Sigma $ dinyatakan dengan $$ \\operatorname { var } ( D ) = \\operatorname { tr } ( \\Sigma ) = \\sigma _ { 1 } ^ { 2 } + \\sigma _ { 2 } ^ { 2 } $$ Kita segera memiliki $ tr(\\Sigma)\\geq 0$ Secara umum covarian adalah non-negatif, karena $$ | \\Sigma | = \\operatorname { det } ( \\Sigma ) = \\sigma _ { 1 } ^ { 2 } \\sigma _ { 2 } ^ { 2 } - \\sigma _ { 12 } ^ { 2 } = \\sigma _ { 1 } ^ { 2 } \\sigma _ { 2 } ^ { 2 } - \\rho _ { 12 } ^ { 2 } \\sigma _ { 1 } ^ { 2 } \\sigma _ { 2 } ^ { 2 } = ( 1 - \\rho _ { 12 } ^ { 2 } ) \\sigma _ { 1 } ^ { 2 } \\sigma _ { 2 } ^ { 2 } $$ dimana kitu gunakan persamaan (2.23), yaiut \\rho_{12}\\sigma_1\\sigma_2 \\rho_{12}\\sigma_1\\sigma_2 . dengan |\\Sigma| |\\Sigma| adalah determinan dari matrik kovarian. Perhatikan bahwa |\\rho_{12}|\\leq 1 |\\rho_{12}|\\leq 1 menyebabkan \\rho_{12}^2 \\leq 1 \\rho_{12}^2 \\leq 1 sehingga det (\\Sigma) \\geq 1 (\\Sigma) \\geq 1 furthermore determinannya adalah non-negative. Matrik kovarian sampel dinyatakan dengan $$ \\hat { \\Sigma } = \\left( \\begin{array} { l l } { \\hat { \\sigma } _ { 1 } ^ { 2 } } & { \\hat { \\sigma } _ { 12 } } \\ { \\hat { \\sigma } _ { 12 } } & { \\hat { \\sigma } _ { 2 } ^ { 2 } } \\end{array} \\right) $$ Matrik kovarian sampe $ \\hat \\Sigma$ memilki karakteristik sama seperti \\Sigma \\Sigma , yaitu simetris dan |\\hat \\Sigma| \\geq 0 |\\hat \\Sigma| \\geq 0 dan itu dapat digunakan untum memudahkan mendapatkan total sampel dan variansi secara umum Contoh (Rata rata Sampel dan Covarian) Perhatikan atribut sepal length dan sepal width untuk data iris, seperti yang diplot dalam gambar 2.4. Ada n=150 data dalam d=2 d=2 ruang dimensi. Rata rata sampel adalah $$ \\hat { \\mu } = \\left( \\begin{array} { l } { 5.843 } \\ { 3.054 } \\end{array} \\right) $$ Matrik covarian dinyatakan dengan $$ \\hat { \\Sigma } = \\left( \\begin{array} { r r } { 0.681 } & { - 0.039 } \\ { - 0.039 } & { 0.187 } \\end{array} \\right) $$ Variansi untuk sepal length adalah \\hat \\sigma_1^2=0.681 \\hat \\sigma_1^2=0.681 dan sepal width adalah \\hat \\sigma_2^2=0.187 \\hat \\sigma_2^2=0.187 . Covarian antara dua atribut adalah \\hat \\sigma_{12}=-0.039 \\hat \\sigma_{12}=-0.039 dan korelasi antara dua atribut tersebut adalah $$ \\hat { \\rho } _ { 12 } = \\frac { - 0.039 } { \\sqrt { 0.681 \\cdot 0.187 } } = - 0.109 $$ Lalu, ada korelasi yang sangat lemah antara dua atribut tersebut Total variansi sampel dinyatakan dengan $$ \\operatorname { tr } ( \\hat { \\Sigma } ) = 0.681 + 0.187 = 0.868 $$ dan variansi secara umum dinyatakan dengan $$ \\hat { \\Sigma } | = \\operatorname { det } ( \\hat { \\Sigma } ) = 0.681 \\cdot 0.187 - ( - 0.039 ) ^ { 2 } = 0.126 $$ Analisa Multivariate \u00b6 Dalam analisa multivariate, kita melihat atribut numerik dengan d d dimensi X_1,X_2,...X_d X_1,X_2,...X_d . Data dinyatakan degan matrik n\\times d n\\times d seperti berikut $$ D = \\left( \\begin{array} { c c c c } { X _ { 1 } } & { X _ { 2 } } & { \\cdots } & { X _ { d } } \\ \\hline x _ { 11 } & { x _ { 12 } } & { \\cdots } & { x _ { 1 d } } \\ { x _ { 21 } } & { x _ { 22 } } & { \\cdots } & { x _ { 2 d } } \\ { \\vdots } & { \\vdots } & { \\ddots } & { \\vdots } \\ { x _ { n 1 } } & { x _ { n 2 } } & { \\cdots } & { x _ { n d } } \\end{array} \\right) $$ Jika dilihat dari baris data memiliki n n objek atatu vektor dalam d d ruang dimensi atribut $$ x _ { i } = ( x _ { i 1 } , x _ { i 2 } , \\ldots , x _ { i d } ) ^ { T } \\in \\mathbb R ^ { d } $$ Jika dilihat dari sudut pandang kolom, data diangga sebagai d d objek atau vektor dalam n n dimensi ruang dengan titik-titik data $$ X _ { j } = ( x _ { 1 j } , x _ { 2 j } , \\ldots , x _ { n j } ) ^ { T } \\in R ^ { n } $$ Jika dilihat dari sudut pandang probabilitas, d d atribut dimodelkan dengan variabel acak vektor X=(X_1,X_2,...X_d)^T X=(X_1,X_2,...X_d)^T dan titik titik x_i x_i dianggap sebagai sampel acak yang diperoleh dari X X , atribut atribut tersebut independent and identfically distributed dari X X (i.i.d X X ) Mean Generalisasi persamaan (2.18) rata-rata vektor multivariate diperoleh dari masing-masing atribut yang dinyatakan dengan $$ \\mu = E [ X ] = \\left( \\begin{array} { c } { E [ X _ { 1 } ] } \\ { E [ X _ { 2 } ] } \\ { \\vdots } \\ { E [ X _ { d } ] } \\end{array} \\right) = \\left( \\begin{array} { c } { \\mu _ { 1 } } \\ { \\mu _ { 2 } } \\ { \\vdots } \\ { \\mu _ { d } } \\end{array} \\right) $$ Generalisasi persamaan (2.19) rata-rata sampel dinyatakan dengan $$ \\hat { \\mu } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } $$ Matrik Kovarian Generalisasi persamaan (2.26) untuk d d dimensi, kovarian multicovariate di dinyatakan dengan matrik kovarian simetris $ d\\times d $yang menyatakan kovarian untuk setiap pasangan atribut $$ \\Sigma = E [ ( X - \\mu ) ( X - \\mu ) ^ { T } ] = \\left( \\begin{array} { c c c c } { \\sigma _ { 1 } ^ { 2 } } & { \\sigma _ { 12 } } & { \\cdots } & { \\sigma _ { 1 d } } \\ { \\sigma _ { 21 } } & { \\sigma _ { 2 } ^ { 2 } } & { \\cdots } & { \\sigma _ { 2 d } } \\ { \\cdots } & { \\cdots } & { \\cdots } & { \\cdots } \\ { \\sigma _ { d 1 } } & { \\sigma _ { d 2 } } & { \\cdots } & { \\sigma _ { d } ^ { 2 } } \\end{array} \\right) $$ Elemen diagonal $\\sigma_i^2 $ menyatakan variansi atribut X_i X_i , dimana elemen-elemen bukan diagonal \\sigma_{ij} = \\sigma_{ji} \\sigma_{ij} = \\sigma_{ji} menyatakan kovarian antara atribut pasangan X_i X_i dan X_j X_j . Matrik kovarian adalah positif semidefinite Contoh Rata-rata sample dan matrik covarian. Perhatikan semua atribut numerik untuk data iris, namanya sepal length, petal length, dan petal width. Rata rata multivarean dinyatakan dengan \\hat { \\mu } = ( 5.843 \\quad 3.054 \\quad 3.759 \\quad 1.199 ) ^ { T } $$ dan matrik covarian nya adalah $$ \\hat { \\Sigma } = \\left( \\begin{array} { r r r r } { 0.681 } & { - 0.039 } & { 1.265 } & { 0.513 } \\\\ { - 0.039 } & { 0.187 } & { - 0.320 } & { - 0.117 } \\\\ { 1.265 } & { - 0.320 } & { 3.092 } & { 1.288 } \\\\ { 0.513 } & { - 0.117 } & { 1.288 } & { 0.579 } \\end{array} \\right) $$ Jumlah variansi adalah $$ \\operatorname { var } ( D ) = \\operatorname { tr } ( \\hat { \\Sigma } ) = 0.681 + 0.187 + 3.092 + 0.579 = 4.539 \\hat { \\mu } = ( 5.843 \\quad 3.054 \\quad 3.759 \\quad 1.199 ) ^ { T } $$ dan matrik covarian nya adalah $$ \\hat { \\Sigma } = \\left( \\begin{array} { r r r r } { 0.681 } & { - 0.039 } & { 1.265 } & { 0.513 } \\\\ { - 0.039 } & { 0.187 } & { - 0.320 } & { - 0.117 } \\\\ { 1.265 } & { - 0.320 } & { 3.092 } & { 1.288 } \\\\ { 0.513 } & { - 0.117 } & { 1.288 } & { 0.579 } \\end{array} \\right) $$ Jumlah variansi adalah $$ \\operatorname { var } ( D ) = \\operatorname { tr } ( \\hat { \\Sigma } ) = 0.681 + 0.187 + 3.092 + 0.579 = 4.539 Contoh Perkalian dalam dan perkalian luar . Untuk mengdeskripsikan komputasi perkalian dalam dan perkalian luar dari matrik covarian, perhatikan data 2-dimensi $$ D = \\left( \\begin{array} { l l } { A _ { 1 } } & { A _ { 2 } } \\ \\hline 1 & { 0.8 } \\ { 5 } & { 2.4 } \\ { 9 } & { 5.5 } \\end{array} \\right) $$ Rata-rata vektor adalah sebagai berikut $$ \\hat { \\mu } = \\left( \\begin{array} { l } { \\hat { \\mu } _ { 1 } } \\ { \\hat { \\mu } _ { 2 } } \\end{array} \\right) = \\left( \\begin{array} { l } { 15 / 3 } \\ { 8.7 / 3 } \\end{array} \\right) = \\left( \\begin{array} { c } { 5 } \\ { 2.9 } \\end{array} \\right) $$ dan matrik data terpusat dinyatakan $$ Z = D - 1 \\cdot \\mu ^ { T } = \\left( \\begin{array} { l l } { 1 } & { 0.8 } \\ { 5 } & { 2.4 } \\ { 9 } & { 5.5 } \\end{array} \\right) - \\left( \\begin{array} { l } { 1 } \\ { 1 } \\ { 1 } \\end{array} \\right) \\left( \\begin{array} { l l } { 5 } & { 2.9 } \\end{array} \\right) = \\left( \\begin{array} { r r } { - 4 } & { - 2.1 } \\ { 0 } & { - 0.5 } \\ { 4 } & { 2.6 } \\end{array} \\right) $$ Pendekatan perkalian dalam [pers. 2.30] untuk menghitung matrik kovarian adalah $$ \\left. \\begin{array}{l}{ \\hat { \\Sigma } = \\frac { 1 } { n } Z ^ { T } Z = \\frac { 1 } { 3 } \\left( \\begin{array} { c c c } { - 4 } & { 0 } & { 4 } \\ { - 2.1 } & { - 0.5 } & { 2.6 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { - 4 } & { - 2.1 } \\ { 0 } & { - 0.5 } \\ { 4 } & { 2.6 } \\end{array} \\right) }\\{ = \\frac { 1 } { 3 } \\left( \\begin{array} { c c } { 32 } & { 18.8 } \\ { 18.8 } & { 11.42 } \\end{array} \\right) = \\left( \\begin{array} { c c } { 10.67 } & { 6.27 } \\ { 6.27 } & { 3.81 } \\end{array} \\right) }\\end{array} \\right. $$ Pendekatan lain yaitu dengan perkalian luar [pers. 2.31] dibyatakan dengan $$ \\hat { \\Sigma } = \\frac { 1 } { n } \\sum _ { j = 1 } ^ { n } z _ { i } \\cdot z _ { i } ^ { T } $$ = \\frac { 1 } { 3 } \\left [ \\left( \\begin{array} { c } { - 4 } \\\\ { - 2.1 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { - 4 } & { - 2.1 } \\end{array} \\right) + \\left( \\begin{array} { r r } { 0 } \\\\ { - 0.5 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { 0 } & { - 0.5 } \\end{array} \\right) + \\left( \\begin{array} { c } { 4 } \\\\ { 2.6 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { 4 } & { 2.6 } \\end{array} \\right)\\right ] = \\frac { 1 } { 3 } \\left [ \\left( \\begin{array} { c } { - 4 } \\\\ { - 2.1 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { - 4 } & { - 2.1 } \\end{array} \\right) + \\left( \\begin{array} { r r } { 0 } \\\\ { - 0.5 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { 0 } & { - 0.5 } \\end{array} \\right) + \\left( \\begin{array} { c } { 4 } \\\\ { 2.6 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { 4 } & { 2.6 } \\end{array} \\right)\\right ] \\left. \\begin{array} { l } { = \\frac { 1 } { 3 } [ \\left( \\begin{array} { c c } { 16.0 } & { 8.4 } \\\\ { 8.4 } & { 4.41 } \\end{array} \\right) + \\left( \\begin{array} { c c } { 0.0 } & { 0.0 } \\\\ { 0.0 } & { 0.25 } \\end{array} \\right) + \\left( \\begin{array} { c c } { 16.0 } & { 10.4 } \\\\ { 10.4 } & { 6.76 } \\end{array} \\right) ] } \\\\ { = \\frac { 1 } { 3 } \\left( \\begin{array} { c c } { 32.0 } & { 18.8 } \\\\ { 18.8 } & { 11.42 } \\end{array} \\right) = \\left( \\begin{array} { c c } { 10.67 } & { 6.27 } \\\\ { 6.27 } & { 3.81 } \\end{array} \\right) } \\end{array} \\right. \\left. \\begin{array} { l } { = \\frac { 1 } { 3 } [ \\left( \\begin{array} { c c } { 16.0 } & { 8.4 } \\\\ { 8.4 } & { 4.41 } \\end{array} \\right) + \\left( \\begin{array} { c c } { 0.0 } & { 0.0 } \\\\ { 0.0 } & { 0.25 } \\end{array} \\right) + \\left( \\begin{array} { c c } { 16.0 } & { 10.4 } \\\\ { 10.4 } & { 6.76 } \\end{array} \\right) ] } \\\\ { = \\frac { 1 } { 3 } \\left( \\begin{array} { c c } { 32.0 } & { 18.8 } \\\\ { 18.8 } & { 11.42 } \\end{array} \\right) = \\left( \\begin{array} { c c } { 10.67 } & { 6.27 } \\\\ { 6.27 } & { 3.81 } \\end{array} \\right) } \\end{array} \\right. dimana data terpusat z_i z_i adalah baris dari Z Z Atribut Kategorikal \u00b6 Kita asumsikan bahwa data terdiri dari satu atribut X X . Domain dari X X terdiri dari m m nilai simbolis dom(X)={a_1,a_2,...a_m} dom(X)={a_1,a_2,...a_m} . Data D D adalah n\\times 1 n\\times 1 matrik data simbolis yang dinyatakan dengan $$ D = \\left( \\begin{array} { c } { X } \\ { x _ { 1 } } \\ { x _ { 2 } } \\ { \\vdots } \\ { x _ { n } } \\end{array} \\right) $$ dimana setiap nilai x_i \\in dom(X) x_i \\in dom(X) Variabel Bernouli \u00b6 Marilah kita lihat kasus ketika atribut kategorikal X X memililik domain $ {a_1,a_2}$ dengan m=2 m=2 . Kita dapat memodelkan X X sebagai variabel acak Bernouli, yang didasarkan pada dua nilai berbeda yaitu 1 dan 0, sesuai dengan pemetaan $$ X ( v ) = \\left{ \\begin{array} { l l } { 1 } & { \\text { if } v = a _ { 1 } } \\ { 0 } & { \\text { if } v = a _ { 2 } } \\end{array} \\right. $$ Fungsi massa probabilitas (PMF) dari X X dinyatakan dengan $$ P ( X = x ) = f ( x ) = \\left{ \\begin{array} { l l } { p _ { 1 } } & { \\text { if } x = 1 } \\ { p _ { 0 } } & { \\text { if } x = 0 } \\end{array} \\right. $$ dimana p_1 p_1 dan p_0 p_0 adalah parameter distribusi, yang harus memenuhi kondisi $$ p_1+p_0=1 $$ Karena hanya ada satu parameter bebas, biasanya menotasikan p_1=p p_1=p maka p_0=1-p p_0=1-p . Fungsi Massa Probabilitas dari variabel acak Bernouli X X dapat kemudian ditulis dengan $$ P ( X = x ) = f ( x ) = p ^ { x } ( 1 - p ) ^ { 1 - x } $$ Kita dapat melihat bahwa P ( X = 1 ) = p ^ { 1 } ( 1 - p ) ^ { 0 } = p \\text { and } P ( X = 0 ) = p ^ { 0 } ( 1 - p ) ^ { 1 } = 1 - p P ( X = 1 ) = p ^ { 1 } ( 1 - p ) ^ { 0 } = p \\text { and } P ( X = 0 ) = p ^ { 0 } ( 1 - p ) ^ { 1 } = 1 - p seperti yand diharapkan Mean dan Variansi Nilai harapan dari X X dinyatakan dengan $$ \\mu = E [ X ] = 1 \\cdot p + 0 \\cdot ( 1 - p ) = p $$ dan variansi dari X X dinyatakan dengan $$ \\left. \\begin{array}{l}{ \\sigma ^ { 2 } = \\operatorname { var } ( X ) = E [ X ^ { 2 } ] - ( E [ X ] ) ^ { 2 } }\\ \\hspace{7mm}= ( 1 ^ { 2 } \\cdot p + 0 ^ { 2 } \\cdot ( 1 - p ) ) - p ^ { 2 } = p - p ^ { 2 } = p ( 1 - p ) \\\\end{array} \\right. $$ Rata-rata sampel dan Variansi Untuk mengestimasi parameter dari variabel Bernouli X X , kita asumsikan bahwa setiap simbol dipetakan ke nilai biner. Sehingga, sekumpulan nilai {x_1,x_2,...x_n} {x_1,x_2,...x_n} diasumsikan menjadi sampel acak yang diperoleh dari X X (yaitu setiap $ x_i$ adalah IID dengan X X . Rata-rata sampel dinyatakan dengan $$ \\hat { \\mu } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } = \\frac { n _ { 1 } } { n } = \\hat { p } $$ dimana n_1 n_1 adalah banyaknya titik dengan x_1=1 x_1=1 dalam sampel acak (sama dengan banyak kejadian dari simbol a_1 a_1 ) Misal n_0=n-n_1 n_0=n-n_1 menyatakan banyak titik dengan x_i=0 x_i=0 dalam sampel acak. Variansi sample dinyatakan dengan $$ \\left. \\begin{array}{l}{ \\hat { \\sigma } ^ { 2 } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\hat { \\mu } ) ^ { 2 } }\\ \\hspace{7mm}{ = \\frac { n _ { 1 } } { n } ( 1 - \\hat { p } ) ^ { 2 } + \\frac { n - n _ { 1 } } { n } ( - \\hat { p } ) ^ { 2 } }\\\\hspace{7mm}{ = \\hat { p } ( 1 - \\hat { p } ) ^ { 2 } + ( 1 - \\hat { p } ) \\hat { p } ^ { 2 } }\\\\hspace{7mm}{ = \\hat { p } ( 1 - \\hat { p } ) ( 1 - \\hat { p } + \\hat { p } ) }\\\\hspace{7mm}{ = \\hat { p } ( 1 - \\hat { p } ) }\\end{array} \\right. $$ Variansi sampel dapat juga diperoleh langsung dari persamaan(3.1) dengan mensubsitusikan \\hat p \\hat p untuk p p . Contoh Perhatikan atribut sepal length ( X X ) untuk dataset iris dalam tabel 1.1. Marilah kita definisikan bunga iris dengan Long jika bunga itu sepal length dalam range [7, \\infty ] [7, \\infty ] , dan short jika sepal length dalam range [-\\infty,7] [-\\infty,7] . Kemudian X_1 X_1 dapat dinyatakan dengan atribut kategorikan dengan domain {Long,Short}. Dari sampel yang diamati ukuran n=150 n=150 , kita menemukan 13 iris long. Rata-rata sampel dari X_1 X_1 adalah $$ \\hat { \\mu } = \\hat { p } = 13 / 150 = 0.087 $$ dan variansinya adalah $$ \\hat { \\sigma } ^ { 2 } = \\hat { p } ( 1 - \\hat { p } ) = 0.087 ( 1 - 0.087 ) = 0.087 \\cdot 0.913 = 0.079 $$ Ditribusi binomial : banyaknya kejadian Diberikan variabel Bernoulli X X , misal \\{x_1,x_2,...x_n\\} \\{x_1,x_2,...x_n\\} menyatakan sampel acak dari ukuran n n yang diperoleh dari X X . Misal N N adalah variabel acak yang menyatakan numlah kejadi dari simbol a_1 a_1 (nilai X=1 X=1 ) dalam sampe. N adalah distribusi binomial yang dinyatakan dengan $$ f ( N = n _ { 1 } | n , p ) = \\left( \\begin{array} { l } { n } \\ { n _ { 1 } } \\end{array} \\right) p ^ { n _ { 1 } } ( 1 - p ) ^ { n - n _ { 1 } } $$ Dalam kenyataannya, N N adalah jumlah dari n n variabel acak Bernoulli x_i x_i yang saling bebas dan (IID) dengan X X yaitu N=\\sum_{i=1}^n x_i N=\\sum_{i=1}^n x_i . Dengan liniearitas dari ekpektasi, mean atau jumlah harapan dari kejadian simbol a_i a_i dinyatakan dengan $$ \\mu _ { N } = E [ N ] = E \\left[ \\sum _ { i = 1 } ^ { n } x _ { i } \\right] = \\sum _ { i = 1 } ^ { n } E [ x _ { i } ] = \\sum _ { i = 1 } ^ { n } p = n p $$ Karena x_i x_i adalah semuanya saling bebas, variansi dari N N dinyatakan dengan $$ \\sigma _ { N } ^ { 2 } = \\operatorname { var } ( N ) = \\sum _ { i = 1 } ^ { n } \\operatorname { var } ( x _ { i } ) = \\sum _ { i = 1 } ^ { n } p ( 1 - p ) = n p ( 1 - p ) $$ Contoh 3.2. Dengan meneruskan contoh 3.1, kita dapat menggunakan parameter yang telah diestimasi \\hat p=0.087 \\hat p=0.087 untuk menghitung banyaknya kejadian yang diharapkan N long dari sepal length. distribusi binomial Iris $$ E [ N ] = n \\hat { p } = 150 \\cdot 0.087 = 13 $$ Dalam kasus ini, karena p p dihitung dari sample melalui \\hat p \\hat p , tidak mengherankan bahwa jumlah kejadian diharapkan dari Long Iris sama dengan kejadian yang sebenarnya. Akan tetapi yang lebih menarik adalah kita dapat menghitung variansi jumlah kejadian $$ \\operatorname { var } ( N ) = n \\hat { p } ( 1 - \\hat { p } ) = 150 \\cdot 0.079 = 11.9 $$ Meningkatnya ukuran sample, distribusi binomial seperti yang diberikan dapalam persamaan 3.3 cenderung ke distribusi normal dengan \\mu=13 \\mu=13 dan \\sigma=\\sqrt{11.9}=3.45 \\sigma=\\sqrt{11.9}=3.45 . Sehingga dengan kepercaan lebih besar dari 95%, kita dapat mengklam bahwa jumlah kejadian dari a_i a_i akan terletak dalam rentang \\mu \\pm 2 \\sigma = [ 9.55,16.45 ] \\mu \\pm 2 \\sigma = [ 9.55,16.45 ] yang mengikuti dari fakta bahwa untuk distribusi normal 95,45% dari massa probabilitas terletak dalam dua standar deviasi dari rata-rata. Variable multivariate Bernoulli \u00b6 Sekarang kita memandang kasus umum ketika X X adalah atribut kategorical dengan domain \\{a_1,a_2,...a_m\\} \\{a_1,a_2,...a_m\\} . Kita dapat memodelkan X X sebagai variabel acak Bernoulli m m -dimensi X = ( A _ { 1 } , A _ { 2 } , \\ldots , A _ { m } ) ^ { T } X = ( A _ { 1 } , A _ { 2 } , \\ldots , A _ { m } ) ^ { T } dimana setiap A_i A_i adalah variabel Bernoulli dengan parameter p_i p_i yang menotasikan probabilitas dari pengamatan simbol a_i a_i . Akan tetapi karena X X dapat mengasumsikan hanya satu dari nilai simbolik pada suatu waktum jika X=a_i X=a_i maka A_i=1 A_i=1 dan A_j=0 A_j=0 untuk semua j \\neq i j \\neq i . Variabel acak X \\in {0,1}^m X \\in {0,1}^m , dan jika X=a_i X=a_i , maka X=e_i X=e_i , dimana e_i e_i adalah standar vektor basis ke i, e_i\\in\\mathbb R^m e_i\\in\\mathbb R^m yang dinyatakan dengan $$ e _ { i } = ( \\overbrace { 0 , \\ldots , 0 } ^ { i - 1 } , 1 , \\overbrace { 0 , \\ldots , 0 } ^ { m - i } ) ^ { T } $$ Pada e_i e_i hanya elemen ke i adalah 1 ( e_{ii}=1 e_{ii}=1 ) , sedangkan semua elemen yang lain adalah nol, ( e_{ij}=0, \\forall j \\neq i e_{ij}=0, \\forall j \\neq i ). Disini, definis yang lebih tepat dari variabel Bernoulli multivariate , yaitu generalisasi dari variabel Bernoullii dari dua hasil ke m m hasil. Kita kemudian memodelkan atribut kategorical X X sebagai variabel Bernoulli multivariate X X didefinisikan dengan $$ X ( v ) = e _ { i } \\text { if } v = a _ { i } $$ Rentang dari X X terdiri dari m m nilai vektor berbeda \\{e_1,e_2,...e_m\\} \\{e_1,e_2,...e_m\\} dengan fungsi massa probabilitas dari X X dinyatakan dengan $$ P ( X = e _ { i } ) = f ( e _ { i } ) = p _ { i } $$ dimana p_i p_i adalah probabilitas dari nilai pengamatan a_i a_i . Parameter ini harus memenuhi kondisi $$ \\sum _ { i = 1 } ^ { m } p _ { i } = 1 $$ Fungsi massa prababilitas dapat ditulis secara utuh sebagai berikut $$ P ( X = e _ { i } ) = f ( e _ { i } ) = \\prod _ { j = 1 } ^ { m } p _ { j } ^ { e _ { i j } }Ka $$ Kareana e_ii=1 e_ii=1 dan e_ij=0 e_ij=0 funtuk $ j\\neq i$, kita dapat melihat bahwa, seperti yang diharapkan, kita miliki $$ f ( e _ { i } ) = \\prod _ { j=1 } ^ { m } p _ { j } ^ { e _ { i j } } = p _ { 1 } ^ { e _ { i 0 } } \\times \\cdots p _ { i } ^ { e _ { i i } } \\cdots \\times p _ { m } ^ { e _ { i m } } = p _ { 1 } ^ { 0 } \\times \\cdots p _ { i } ^ { 1 } \\cdots \\times p _ { m } ^ { 0 } = p _ { i } $$ \\left. \\begin{array} { | l | l | l | } \\hline \\text { Bins } & { { \\text { Domain } } } & { { \\text { Counts } } } \\\\ \\hline [ 4.3,5.2 ] & { \\text { Very Short } ( a _ { 1 } ) } & { n _ { 1 } = 45 } \\\\ { ( 5.2,6.1 ] } & { \\text { Short } ( a _ { 2 } ) } & { n _ { 2 } = 50 } \\\\ { ( 6.1,7.0 ] } & { \\text { Long } ( a _ { 3 } ) } & { n _ { 3 } = 43 } \\\\ { ( 7.0,7.9 ] } & { \\text { Very Long } ( a _ { 4 } ) } & { n _ { 4 } = 12 } \\\\ \\hline \\end{array} \\right. \\left. \\begin{array} { | l | l | l | } \\hline \\text { Bins } & { { \\text { Domain } } } & { { \\text { Counts } } } \\\\ \\hline [ 4.3,5.2 ] & { \\text { Very Short } ( a _ { 1 } ) } & { n _ { 1 } = 45 } \\\\ { ( 5.2,6.1 ] } & { \\text { Short } ( a _ { 2 } ) } & { n _ { 2 } = 50 } \\\\ { ( 6.1,7.0 ] } & { \\text { Long } ( a _ { 3 } ) } & { n _ { 3 } = 43 } \\\\ { ( 7.0,7.9 ] } & { \\text { Very Long } ( a _ { 4 } ) } & { n _ { 4 } = 12 } \\\\ \\hline \\end{array} \\right. Contoh : Marilah kita lihat atribut sepal length ( X_1 X_1 ) untuk data Iris seperti yang ditunjukkan dalam tabel 1.2. Kita membagi sepal length kedalam empat interval yang sama, dan memberikan nama untuk setiap interval seperti yang diunjukkan dalam tabel 3.1. Kita lihat X_1 X_1 sebagai atribut kategorical dengan domain $$ {a _ { 2 } = \\text { VeryShort, } a _ { 2 } = \\text { Short, } a _ { 3 } = \\operatorname { Long } , a _ { 4 } = \\operatorname{Very Long}} $$ Kita memodelkan atribut kategorical X_1 X_1 sebagai variabel X X Bernoulli multivariate, didefinisikan dengan $$ X ( v ) = \\left{ \\begin{array} { l l } { e _ { 1 } = ( 1,0,0,0 ) } & { \\text { jika } v = a _ { 1 } } \\ { e _ { 2 } = ( 0,1,0,0 ) } & { \\text { jika } v = a _ { 2 } } \\ { e _ { 3 } = ( 0,0,1,0 ) } & { \\text { jika } v = a _ { 3 } } \\ { e _ { 4 } = ( 0,0,0,1 ) } & { \\text { jika } v = a _ { 4 } } \\end{array} \\right. $$ Misalkan, simbol x_1=Short=a_2 x_1=Short=a_2 dinyatakan dengan (0,1,0,0)^T=e_2 (0,1,0,0)^T=e_2 Mean Mean atau nilai harapan dari X X dapat diperoleh dengan $$ \\mu = E [ X ] = \\sum _ { i = 1 } ^ { m } e _ { i } f ( e _ { i } ) = \\sum _ { i = 1 } ^ { m } e _ { i } p _ { i } = \\left( \\begin{array} { l } { 1 } \\ { 0 } \\ { \\vdots } \\ { 0 } \\end{array} \\right) p _ { 1 } + \\cdots + \\left( \\begin{array} { l } { 0 } \\ { 0 } \\ { \\vdots } \\ { 1 } \\end{array} \\right) p _ { m } = \\left( \\begin{array} { c } { p _ { 1 } } \\ { p _ { 2 } } \\ { \\vdots } \\ { p _ { m } } \\end{array} \\right) = p $$","title":"Ekplorasi data"},{"location":"Eksplorasi%20data/#ekplorasi-data","text":"Oleh Mulaab","title":"Ekplorasi data"},{"location":"Eksplorasi%20data/#atribut-data-numerik","text":"Dalam bab ini, kita membahas metode statistik dasar untuk analisis ekploarasi data atribut numerik. Kita membahas ukuran kecenderungan pusat (central tendency), ukuran dispersi atau sebaran, dan ukuran ketergantungan linier atau hubungan antara atribut. Kita menekankan hubungan antara probabilistik dan geometris dan aljabar dari sudut pandang data matriks","title":"Atribut   Data numerik"},{"location":"Eksplorasi%20data/#analisa-univariat","text":"Analisis univariat dilakukan pada atribut tunggal ( X X ); dengan demikian matriks data D bisa dianggap sebagai matriks n \u00d7 1 n \u00d7 1 , atau sebagai vektor kolom, yang dianyatakan dengan $$ D = \\left( \\begin{array} { c } { X } \\ \\hline x _ { 1 } \\ { x _ { 2 } } \\ { \\vdots } \\ { x _ { n } } \\end{array} \\right) $$ dimana X X adalah atribut numerik yang dimaksudkan, dengan $ x _ { i } \\in \\mathbb{R} $. X X diasumsikan adalah variabel random, dengan setiap titik $ x _ { i } ( 1 \\leq i \\leq n ) $ , merupakan variabel acak. Kita asumsikan bawa data pengamatan adalah. Kami berasumsi bahwa data yang diamati adalah sampel acak yang diambil dari X X , artinya, setiap variabel x_i x_i adalah saling bebas dan berdistribus sama (iid). Dalam sudut pandang vektor, kami memperlakukan sampel sebagai vektor n-dimensi, dan menulis $ X \\in \\mathbb R ^ { n } $ Secara umum, fungsi padat probabilitas atau fungsi mass f(x) f(x) dan fungsi distribusi kumulatif $ F(x),$ untuk atribut X X keduanya tidak diketahui. Akan tetapi, kita dapat mengestimasi distribusi ini langsung dar data sample, juga juga memungkinkan kita untuk menghitung beberapa parameter penting populasi.","title":"Analisa univariat"},{"location":"Eksplorasi%20data/#fungsi-distribusi-kumulatif-empiris","text":"Fungsi distribusi kumulatif empiris (CDF ) dari X X dinyatakan dengan $$ \\hat { F } ( x ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i } \\leq x ) $$ dimana $$ I ( x _ { i } \\leq x ) = \\left{ \\begin{array} { l l } { 1 } & { \\text { if } x _ { i } \\leq x } \\ { 0 } & { \\text { if } x _ { i } > x } \\end{array} \\right. $$ adalah variabel indikator biner yang menyatakan variabel indikator biner yang menunjukkan apakah kondisi yang diberikan terpenuhi atau tidak.","title":"Fungsi distribusi  Kumulatif Empiris"},{"location":"Eksplorasi%20data/#fungsi-distribusi-kumulatif-invers","text":"Definisi fungsi distribusi kumulatif invers atau fungsi quantile untuk variabel acak X X sebagai berikut : $$ F ^ { - 1 } ( q ) = \\operatorname { min } { x | \\hat { F } ( x ) \\geq q } \\quad \\text { for } q \\in [ 0,1 ] $$ Fungsi distribusi kumulatif Invers empiris dapat diperoleh dari persamaan (2)","title":"Fungsi distribusi kumulatif Invers"},{"location":"Eksplorasi%20data/#fungsi-massa-probabilitas-empiris","text":"Fungsi massa probabilitas empiris dari X X dinyatakan dengan $$ \\hat { f } ( x ) = P ( X = x ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i } = x ) $$ dimana $$ I ( x _ { i } = x ) = \\left{ \\begin{array} { l l } { 1 } & { \\text { if } x _ { i } = x } \\ { 0 } & { \\text { if } x _ { i } \\neq x } \\end{array} \\right. $$ Fungsi massa probabilitas empiris juga menempatkan massa probabitas \\frac {1}{n} \\frac {1}{n} pada setipa titik x_i x_i","title":"Fungsi massa Probabilitas Empiris"},{"location":"Eksplorasi%20data/#mengukur-kecenduran-terpusat","text":"Ukuran ini memberikan indikasi tentang konsentrasi massa probabilitas , nilai tengah dan lainnya.","title":"Mengukur kecenduran terpusat"},{"location":"Eksplorasi%20data/#mean","text":"Mean juga disebut dengan nilai harapan dari variabel acak X X adalah rata rata aritmetika dari nilai X X . Itu merupakan salah satu dari kecenderungan terpusat dari X X . Mean atau nilai harapan dari variabel acak X X didefinisikan dengan $$ \\mu = E [ X ] = \\sum _ { x } x f ( x ) $$ diman f(x) f(x) adalah fungsi massa probabilitas dari X X . Nilai harapan dari variabel acak kontinu X X dinyakan dengan $$ \\mu = E [ X ] = \\int _ { - \\infty } ^ { \\infty } x f ( x ) d x $$ dimana f(x) f(x) adalah fungsi padat probabilitas dari X X . Sample Mean . Sample mean adalah statistik, yaitu fungsi $ \\hat { \\mu } : { x _ { 1 } , x _ { 2 } , \\ldots , x _ { n } } \\rightarrow \\mathbb R$, didefinisikan sebagai nilai rata-rata dari x_i x_i : $$ \\hat { \\mu } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } $$ nilai adalah sebagai pengestimasi nilai mean yang tidak diketahui dari X X . Nilai tersebut diperoleh dengan memasukkan dalam fungsi massa probabilitas empiris dalam persamaan (7) $$ \\hat { \\mu } = \\sum _ { x } x \\hat { f } ( x ) = \\sum _ { x } x ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i } = x ) ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } $$ Sample mean adalah tidak bias . Estimator \\hat { \\theta } \\hat { \\theta } disebut dengan unbiased estimatore (stimator tidak bias) untuk parameter \\theta \\theta jika E[\\hat \\theta] = \\theta E[\\hat \\theta] = \\theta untuk setiap kemungkinan nilai dari \\theta \\theta . Sample mean \\hat \\mu \\hat \\mu adalah unbiased estimator untuk mean populasi \\mu \\mu sehingga $$ E [ \\hat { \\mu } ] = E [ \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } ] = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } E [ x _ { i } ] = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } \\mu = \\mu $$ dimana kita gunakan fakta bahwa variabel acak x_i x_i adalah IID sesuai dengan X X , yang berarti bahwa mereka memiliki rata-rata \\mu \\mu yang sama seperti X X , yaitu,$ E [x_i] =\\mu$ untuk semua x_i x_i . Kita juga menggunakan fakta bahwa fungsi ekpektasi E E adalah linier operator yaitu untuk suatu dua bilangan acak X X dan Y Y dan bilangan real a a dan b b , kita memiliki E [ a X + b Y ] = a E [ X ] + b E [ Y ] E [ a X + b Y ] = a E [ X ] + b E [ Y ] Robustnes Kita mengatakan bahwa statistik adalah robust jika tidak dipengaruhi oleh suatu nilai ekstrim ( misal outlier/pencilan) dalam data. Rata-rata sampel sayangnya tidak kuat karena ada satu nilai besar (outlier) dapat mejadikan rata-rata yang tidak sebenarnya. Ukuran yang lebih robust adalah trimmed mean yang didapatkan setalah mengabaikan sebagian kecil dari nilai nilai ekstrim pada salah satu ujungnya. Median Median dari suatu variabel acak didefinisikan dengan nilai m m sehingga $$ P ( X \\leq m ) \\geq \\frac { 1 } { 2 } \\text { and } P ( X \\geq m ) \\geq \\frac { 1 } { 2 } $$ Degan kata lain, median m m adalah nilai paling tengan (middle-most). Dalam istliah (invers) cumulatif distribution function , median m m dinyatakan dengan $$ F ( m ) = 0.5 \\text { or } m = F ^ { - 1 } ( 0.5 ) $$ Sample median dapat diperoleh dari Fungsi distribusi kumulatif invers atau fungsi distribusi kumulatif invers empiris dengan dihitung $$ \\hat { F } ( m ) = 0.5 \\text { atau } m = \\hat { F } ^ { - 1 } ( 0.5 ) $$ Pendekatan paling sederhan untuk menghitung sample median adalah pertama kai dari mengurutkan semua nilai x_i x_i (i \\in [1,n]) (i \\in [1,n]) dengan urutan naik. Jika n n adalah ganjil , media adalah nilai pada posisi \\frac {n+1}{2} \\frac {n+1}{2} . Jika n n adalah genap, nilai padan posisi \\frac {n}{2} \\frac {n}{2} dan \\frac {n}{2}+1 \\frac {n}{2}+1 adalah keduanaya median. Tidak seperti mean, media adalah robust, sehingga ia tidak dipengaruhi oleh banyak nilai extrim. Juga nilai tersebut terjadi dalam sample dan nilai yang bisa diasumsikan oleh variabel acak. Mode Nilai mode dari variabel acak adalah nilai dimana fungsi massa probabilitas atau fungsi padat probabilitas mencapai nilai maximumnya, bergantung pada apakah X X adalah diskrit atau kontinu. Sample mode adalah nila untuk fungsi probabilitas empiris mencapai nilai maksimum, dinyatakan dengan $$ mode(X) =\\arg \\underset{x}{max} {\\hat f(x)} $$ Mode ini mungkin bukan ukuran kecenderungan sentral yang sangat berguna untuk sampel, karena kemungkinan elemen yang tidak representatif menjadi elemen yang paling sering muncul. Selanjutnya, jika semua nilai dalam sampel berbeda, maka masing-masing akan menjadi mode Contoh . (Sample Mean, Median, dan Mode) . Perhatikan atribut sepal length (Xi) (Xi) dalam data iris. Data iris, dimana nilainya seperti yang ditunjukkan dalam tebel 1.2 . Sample mean dinyatakan dengan $$ \\hat { \\mu } = \\frac { 1 } { 150 } ( 5.9 + 6.9 + \\cdots + 7.7 + 5.1 ) = \\frac { 876.5 } { 150 } = 5.843 $$ Gambar 2.1 menunjukkan semua dari 150 nilai sepal length dan sample mean. Gambar 2.2a menunjukkan fungsi distribusi kumulatif empiri dan gambar 2.2b menunjukkan fungsi distribusi kumulatif empiris untuk sepal length Karena n=150 n=150 adalah genap, sample median adalah nilai pada posisi \\frac {n}{2}=75 \\frac {n}{2}=75 dan \\frac {n}{2}+1=76 \\frac {n}{2}+1=76 setelah diurutkan. Untuk sepal length kedua nilainya adalah 5.8, kemudian sample media adalah 5.8 . Dari fungsi distribusi kumulatif invers dalam gambar 2.2b, kita dapat melihat bahwa $$ \\hat { F } ( 5.8 ) = 0.5 \\text { or } 5.8 = \\hat { F } ^ { - 1 } ( 0.5 ) $$ Sample mode untuk sepal length adalah 5. yang dapat dilihat dari frequency dari 5 dalam gambar 2.1. Massa probabilitas empiris pada x=5 x=5 adalah $$ \\hat { f } ( 5 ) = \\frac { 10 } { 150 } = 0.067 $$","title":"Mean"},{"location":"Eksplorasi%20data/#mengukur-sebaran-dispersion","text":"Mengukur dispersi memberikan indikasi tentang sebaran atau variasi pada nilai nilai variabel acak. Jangkauan Jangkauan nilai atau secara sederhana jangkauan (range) variabel acak X X adalah perbedaan antara nilai maximum dan nilai minimum dari X X dinyatakan dengan $$ r = \\operatorname { max } { X } - \\operatorname { min } { X } $$ Sample range adalah statistik, dinyatakan dengan $$ \\ \\hat r = {\\overset{n}{\\underset{i=1}{max }}} {{{x_i}}}-{\\overset{n}{\\underset{i=1}{min }}} {{{x_i}}} $$ Dengan definisi, jangkauan adalah sensitif terhadap nilai extrime sehingga tidak robust. Jangkauan antar interquartile Quartile adalah nilai khusus dari fungsi quantile persaman (2.2) yang membagi data kedalam empat bagian. Furthermore quartile terkati dengan nilai-nilai quantile 0.25, 0.5, dan 0.74 dan 1.0. Quantile pertama adalah nilai q_1 =F^{-1}(0.25) q_1 =F^{-1}(0.25) 25% dari sebelah kiri rentang titik, kuartile ke dua adalah sama dengan nilai median q_2 =F^{-1}(0.5) q_2 =F^{-1}(0.5) , 50 % dari sebelah kiri data dan q_3=F^{-1}(0.75) q_3=F^{-1}(0.75) adalah nilai 75% dari sebelah kiri dan quantile ke empat adalah nilai maximum dari X X , 100 % sebelah kiri dari rentang data. Ukuran yang lebih robust dari seberan X X adalah jangkauan interquartile (IQR) dinyatakan dengan $$ I Q R = q _ { 3 } - q _ { 1 } = F ^ { - 1 } ( 0.75 ) - F ^ { - 1 } ( 0.25 ) $$ Variansi dan standar deviasi Variansi dari variabel acak X X memberikan pengukuran berapa banyak nilai nilai dari penyimpangan X X dari rata-rata atau nilai harapan dari X X . Lebih tepatnya variansi adalah nilai harapan dari penyimpangan dari mean yang dikuadratkan yang didefinisikan dengan $$ \\sigma ^ { 2 } = \\operatorname { var } ( X ) = E [ ( X - \\mu ) ^ { 2 } ] = \\left{ \\begin{array} { l l } { \\sum _ { x } ( x - \\mu ) ^ { 2 } f ( x ) } & { \\text {jika } X \\text { adalah diskrit } } \\ { \\int _ { - \\infty } ^ { \\infty } ( x - \\mu ) ^ { 2 } f ( x ) d x } & { \\text { jika } X \\text { adalah kontinu } } \\end{array} \\right. $$ Standar deviasi \\sigma \\sigma didefinisikan sebagai akar kuadrat positif dari variansi \\sigma^2 \\sigma^2 . Kita dapat juga menulis variansi sebagai selisih antara ekpektasi X^2 X^2 dan akar dari ekpektasi X X : Variansi Sampel Variansi sampel didefinisikan dengan $$ \\hat { \\sigma } ^ { 2 } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\hat { \\mu } ) ^ { 2 } $$ Standar deviasi adalah akar dari variansi sample yang dinyatakan dengan $$ \\hat { \\sigma } = \\sqrt { \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\hat { \\mu } ) ^ { 2 } } $$","title":"Mengukur sebaran (dispersion)"},{"location":"Eksplorasi%20data/#analisa-bivariate","text":"Dalam analisa bivariate, kita memandang dua atribut pada waktu yang sama. Kita fokus untuk memahami keterkaitan atau kebergantunga antara dua variabel atau atribut tersebut, jika ada. Kita lalu membatasi pada dua variabel X_1 X_1 dan X_2 X_2 , dengan D D dinyatakan sebagai matrik dengan ukuran $ n \\times 2$ $$ D = \\left( \\begin{array} { c c } { X _ { 1 } } & { X _ { 2 } } \\ \\hline x _ { 11 } & { x _ { 12 } } \\ { x _ { 21 } } & { x _ { 22 } } \\ { \\vdots } & { \\vdots } \\ { x _ { n 1 } } & { x _ { n 2 } } \\end{array} \\right) $$ Secara geometri, kita dapat memandang D D dalam dua cara. Itu dapat dianggap sebagai n n titik atau vektor dalam 2-ruang dimensi terhadap atribut X_1 X_1 dan X_2 X_2 yaitu x_i =(x_{i1},x_{i2})^T \\in \\mathbb R^2 x_i =(x_{i1},x_{i2})^T \\in \\mathbb R^2 .Selain itu dapat dilihat sebagai 2 titik atau vektor dalam n n -ruang dimensi yang berisi titik, yaitu setiap kolom adalah vektor dalam $ \\mathbb R^n$ sebagai berikut : $$ \\left. \\begin{array} { l } { X _ { 1 } = ( x _ { 11 } , x _ { 21 } , \\ldots , x _ { n 1 } ) ^ { T } } \\ { X _ { 2 } = ( x _ { 12 } , x _ { 22 } , \\ldots , x _ { n 2 } ) ^ { T } } \\end{array} \\right. $$ Dalam sudut pandang probabilistik, vektor kolom X=(X_1,X_2)^T X=(X_1,X_2)^T dianggapa variabel acak bivariate dan titik titik x _ { i } ( 1 \\leq i \\leq n ) x _ { i } ( 1 \\leq i \\leq n ) dinyatakan sebagai sampel acak yang diperoleh dari X X , yaitu x_i x_i dianggap independent and identically distributed (iid) seperti X X . Fungsi Massa Probabilitas Gabungan Empiris Fungsi Massa Probabilitas Gabungan Empiris untuk X X dinyatakan dengan $$ \\hat { f } ( x ) = P ( X = x ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i } = x ) $$ \\hat { f } ( x _ { 1 } , x _ { 2 } ) = P ( X _ { 1 } = x _ { 1 } , X _ { 2 } = x _ { 2 } ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i 1 } = x _ { 1 } , x _ { i 2 } = x _ { 2 } ) \\hat { f } ( x _ { 1 } , x _ { 2 } ) = P ( X _ { 1 } = x _ { 1 } , X _ { 2 } = x _ { 2 } ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i 1 } = x _ { 1 } , x _ { i 2 } = x _ { 2 } ) dimana I I adalah variabel indikator yang bernilai 1 jika argumen argumennya benar $$ I ( x _ { i } = x ) = \\left{ \\begin{array} { l l } { 1 } & { \\text { jika } x _ { i 1 } = x _ { 1 } \\text { dan } x _ { i 2 } = x _ { 2 } } \\ { 0 } & { \\text { untuk yang lainnya } } \\end{array} \\right. $$ Seperti dalam kasus univariate, fungsi probabilitas menempatkan massa probabilitas \\frac {1}{n} \\frac {1}{n} pada setiap objek dalam data sampel.","title":"Analisa Bivariate"},{"location":"Eksplorasi%20data/#mengukur-dispersi","text":"Mean Rata rata bivariate didefinisikan sebagai nilai harapan dari variabel acak vektor X X , didefinisikan sebagai berikut : $$ \\mu = E [ X ] = E \\left[ \\left( \\begin{array} { l } { X _ { 1 } } \\ { X _ { 2 } } \\end{array} \\right) \\right] = \\left( \\begin{array} { l } { E [ X _ { 1 } ] } \\ { E [ X _ { 2 } ] } \\end{array} \\right) = \\left( \\begin{array} { l } { \\mu _ { 1 } } \\ { \\mu _ { 2 } } \\end{array} \\right) $$ Dengan kata lain, rata-rata bivariate adalah nilai harapan dari masing masing atribut. Rata-rata sampel dapat diperoleh dari \\hat f_{x_1} \\hat f_{x_1} dan \\hat f_{x_2} \\hat f_{x_2} , fungsi massa probabilitas empiris dari X_1 X_1 dan X_2 X_2 , menggunakan persamaan (2.5). Dapat juga dihitung dari gabungan fungsi massa probabilitas empiris dalam persamaan (2.17) $$ \\hat { \\mu } = \\sum _ { x } x \\hat { f } ( x ) = \\sum _ { x } x \\left( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i } = x )\\right ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } $$ Variansi Kita dapat menghitung variansi masing masing atribut, yaitu \\sigma_1^2 \\sigma_1^2 untuk X_1 X_1 dan \\sigma_2^2 \\sigma_2^2 untuk X_2 X_2 mengggunkan persamaan (2.8). Variansi secara keseluruhan (1.4) dinyatakan dengan $$ var(D)=\\sigma_1^2 +\\sigma_2^2 $$ Variansi sampel \\hat \\sigma_1^2 + \\hat \\sigma_2^2 \\hat \\sigma_1^2 + \\hat \\sigma_2^2 dapat diestimasi dengan menggunakanpersamaan (2.10) dan jumlah variansi sample adalah \\sigma_1^2 +\\sigma_2^2 \\sigma_1^2 +\\sigma_2^2 2.2.2. Mengukur keterkaitan Covarian Kovarian antara dua atribut X_1 X_1 dan X_2 X_2 mengukur keterkaitan antara kebergantungan linier diantaranya dan didefinisikan dengan $$ \\sigma _ { 12 } = E [ ( X _ { 1 } - \\mu _ { 1 } ) ( X _ { 2 } - \\mu _ { 2 } ) ] $$ Dengan linieraritas dari harapan, kita miliki $$ \\left. \\begin{array}{l}{ \\sigma _ { 12 } = E [ ( X _ { 1 } - \\mu _ { 1 } ) ( X _ { 2 } - \\mu _ { 2 } ) ] }\\{ = E [ X _ { 1 } X _ { 2 } - X _ { 1 } \\mu _ { 2 } - X _ { 2 } \\mu _ { 1 } + \\mu _ { 1 } \\mu _ { 2 } ] }\\{ = E [ X _ { 1 } X _ { 2 } ] - \\mu _ { 2 } E [ X _ { 1 } ] - \\mu _ { 1 } E [ X _ { 2 } ] + \\mu _ { 1 } \\mu _ { 2 } }\\{ = E [ X _ { 1 } X _ { 2 } ] - \\mu _ { 1 } \\mu _ { 2 } }\\{ = E [ X _ { 1 } X _ { 2 } ] - E [ X _ { 1 } ] E [ X _ { 2 } ] }\\end{array} \\right. $$ Persamaan (2.21) dapat dianggap sebagai generalisasi dari variansi univariate persamaan (2.9) pada kasus bivariate. Jika X_1 X_1 dan X_2 X_2 adalah variabel acak saling bebas, maka kita dapat simpulkan bahwa covariannya adalah nol. Ini karena jika X_1 X_1 dan X_2 X_2 adalah saling bebas, maka kita memiliki $$ E [ X _ { 1 } X _ { 2 } ] = E [ X _ { 1 } ] \\cdot E [ X _ { 2 } ] $$ yang pada akhirnya menyiratkan bahwa $$ \\sigma{12}= 0 $$ Namaun sebaliknya tidak benar. Yaitu jika \\sigma_{12}=0 \\sigma_{12}=0 , kita tidak dapat mengklaim bahwa $X_1 $ dan X_2 X_2 adalah saling bebas. Semuanya kita katakan bahwa tidak adalah kebergantung linier antara keduanya. Kovarian sampel antra X1 X1 dan X_2 X_2 dinyatakan dengan $$ \\hat { \\sigma } _ { 12 } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i 1 } - \\hat { \\mu } _ { 1 } ) ( x _ { i 2 } - \\hat { \\mu } _ { 2 } ) $$ Korelasi Korelasi antara variabel X_1 X_1 dan X_2 X_2 adalah standarisasi kovarian, yang didapatkan dengan menormalisasi kovarian dengan standar deviasi masing masing variabel dinyatakan dengan \\rho _ { 12 } = \\frac { \\sigma _ { 12 } } { \\sigma _ { 1 } \\sigma _ { 2 } } = \\frac { \\sigma _ { 12 } } { \\sqrt { \\sigma _ { 1 } ^ { 2 } \\sigma _ { 2 } ^ { 2 } } } $$ Korelasi sample untuk atribut $X_1$ dan $X_2$ dinyatakan dengan $$ \\hat { \\rho } _ { 12 } = \\frac { \\hat { \\sigma } _ { 12 } } { \\hat { \\sigma } _ { 1 } \\hat { \\sigma } _ { 2 } } = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i 1 } - \\hat { \\mu } _ { 1 } ) ( x _ { i 2 } - \\hat { \\mu } _ { 2 } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i 1 } - \\hat { \\mu } _ { 1 } ) ^ { 2 } \\sum _ { i = 1 } ^ { m } ( x _ { i 2 } - \\hat { \\mu } _ { 2 } ) ^ { 2 } } } \\rho _ { 12 } = \\frac { \\sigma _ { 12 } } { \\sigma _ { 1 } \\sigma _ { 2 } } = \\frac { \\sigma _ { 12 } } { \\sqrt { \\sigma _ { 1 } ^ { 2 } \\sigma _ { 2 } ^ { 2 } } } $$ Korelasi sample untuk atribut $X_1$ dan $X_2$ dinyatakan dengan $$ \\hat { \\rho } _ { 12 } = \\frac { \\hat { \\sigma } _ { 12 } } { \\hat { \\sigma } _ { 1 } \\hat { \\sigma } _ { 2 } } = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i 1 } - \\hat { \\mu } _ { 1 } ) ( x _ { i 2 } - \\hat { \\mu } _ { 2 } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i 1 } - \\hat { \\mu } _ { 1 } ) ^ { 2 } \\sum _ { i = 1 } ^ { m } ( x _ { i 2 } - \\hat { \\mu } _ { 2 } ) ^ { 2 } } } Matrik Kovarian Variansi dari untuk dua atribut X_1 X_1 dan X_2 X_2 dapat diringkas dalam matrik covarianse bujursangkar denga ukuran $2 \\times 2 $ dinyatakan dengan $$ \\left. \\begin{array}{l}{ \\Sigma = E [ ( X - \\mu ) ( X - \\mu ) ^ { T } ] }\\{ = E \\left[ \\left( \\begin{array} { c } { X _ { 1 } - \\mu _ { 1 } } \\ { X _ { 2 } - \\mu _ { 2 } } \\end{array} \\right) ( X _ { 1 } - \\mu _ { 1 } \\quad X _ { 2 } - \\mu _ { 2 } ) \\right ] }\\{ = \\left( \\begin{array} { c c } { E [ ( X _ { 1 } - \\mu _ { 1 } ) ( X _ { 1 } - \\mu _ { 1 } ) ] } & { E [ ( X _ { 1 } - \\mu _ { 1 } ) ( X _ { 2 } - \\mu _ { 2 } ) ] } \\ { E [ ( X _ { 2 } - \\mu _ { 2 } ) ( X _ { 1 } - \\mu _ { 1 } ) ] } & { E [ ( X _ { 2 } - \\mu _ { 2 } ) ( X _ { 2 } - \\mu _ { 2 } ) ] } \\end{array} \\right) }\\{ = \\left( \\begin{array} { c c } { \\sigma _ { 1 } ^ { 2 } } & { \\sigma _ { 12 } } \\ { \\sigma _ { 21 } } & { \\sigma _ { 2 } ^ { 2 } } \\end{array} \\right) }\\end{array} \\right. $$ Karena \\sigma_{12}=\\sigma_{21} \\sigma_{12}=\\sigma_{21} , $\\Sigma $ adalah matrik simetris. Matrik vovarian merekam variansi tertentu atribut pada diagonal utamanya, dan informasi covarian pada elemen element bukan diagonal. Total variance dari dua atribut dinyatakan sebagai jumlah elemen elemen diagonal dari $ \\Sigma $ , yang juga disebut trace dari $ \\Sigma $ dinyatakan dengan $$ \\operatorname { var } ( D ) = \\operatorname { tr } ( \\Sigma ) = \\sigma _ { 1 } ^ { 2 } + \\sigma _ { 2 } ^ { 2 } $$ Kita segera memiliki $ tr(\\Sigma)\\geq 0$ Secara umum covarian adalah non-negatif, karena $$ | \\Sigma | = \\operatorname { det } ( \\Sigma ) = \\sigma _ { 1 } ^ { 2 } \\sigma _ { 2 } ^ { 2 } - \\sigma _ { 12 } ^ { 2 } = \\sigma _ { 1 } ^ { 2 } \\sigma _ { 2 } ^ { 2 } - \\rho _ { 12 } ^ { 2 } \\sigma _ { 1 } ^ { 2 } \\sigma _ { 2 } ^ { 2 } = ( 1 - \\rho _ { 12 } ^ { 2 } ) \\sigma _ { 1 } ^ { 2 } \\sigma _ { 2 } ^ { 2 } $$ dimana kitu gunakan persamaan (2.23), yaiut \\rho_{12}\\sigma_1\\sigma_2 \\rho_{12}\\sigma_1\\sigma_2 . dengan |\\Sigma| |\\Sigma| adalah determinan dari matrik kovarian. Perhatikan bahwa |\\rho_{12}|\\leq 1 |\\rho_{12}|\\leq 1 menyebabkan \\rho_{12}^2 \\leq 1 \\rho_{12}^2 \\leq 1 sehingga det (\\Sigma) \\geq 1 (\\Sigma) \\geq 1 furthermore determinannya adalah non-negative. Matrik kovarian sampel dinyatakan dengan $$ \\hat { \\Sigma } = \\left( \\begin{array} { l l } { \\hat { \\sigma } _ { 1 } ^ { 2 } } & { \\hat { \\sigma } _ { 12 } } \\ { \\hat { \\sigma } _ { 12 } } & { \\hat { \\sigma } _ { 2 } ^ { 2 } } \\end{array} \\right) $$ Matrik kovarian sampe $ \\hat \\Sigma$ memilki karakteristik sama seperti \\Sigma \\Sigma , yaitu simetris dan |\\hat \\Sigma| \\geq 0 |\\hat \\Sigma| \\geq 0 dan itu dapat digunakan untum memudahkan mendapatkan total sampel dan variansi secara umum Contoh (Rata rata Sampel dan Covarian) Perhatikan atribut sepal length dan sepal width untuk data iris, seperti yang diplot dalam gambar 2.4. Ada n=150 data dalam d=2 d=2 ruang dimensi. Rata rata sampel adalah $$ \\hat { \\mu } = \\left( \\begin{array} { l } { 5.843 } \\ { 3.054 } \\end{array} \\right) $$ Matrik covarian dinyatakan dengan $$ \\hat { \\Sigma } = \\left( \\begin{array} { r r } { 0.681 } & { - 0.039 } \\ { - 0.039 } & { 0.187 } \\end{array} \\right) $$ Variansi untuk sepal length adalah \\hat \\sigma_1^2=0.681 \\hat \\sigma_1^2=0.681 dan sepal width adalah \\hat \\sigma_2^2=0.187 \\hat \\sigma_2^2=0.187 . Covarian antara dua atribut adalah \\hat \\sigma_{12}=-0.039 \\hat \\sigma_{12}=-0.039 dan korelasi antara dua atribut tersebut adalah $$ \\hat { \\rho } _ { 12 } = \\frac { - 0.039 } { \\sqrt { 0.681 \\cdot 0.187 } } = - 0.109 $$ Lalu, ada korelasi yang sangat lemah antara dua atribut tersebut Total variansi sampel dinyatakan dengan $$ \\operatorname { tr } ( \\hat { \\Sigma } ) = 0.681 + 0.187 = 0.868 $$ dan variansi secara umum dinyatakan dengan $$ \\hat { \\Sigma } | = \\operatorname { det } ( \\hat { \\Sigma } ) = 0.681 \\cdot 0.187 - ( - 0.039 ) ^ { 2 } = 0.126 $$","title":"Mengukur Dispersi"},{"location":"Eksplorasi%20data/#analisa-multivariate","text":"Dalam analisa multivariate, kita melihat atribut numerik dengan d d dimensi X_1,X_2,...X_d X_1,X_2,...X_d . Data dinyatakan degan matrik n\\times d n\\times d seperti berikut $$ D = \\left( \\begin{array} { c c c c } { X _ { 1 } } & { X _ { 2 } } & { \\cdots } & { X _ { d } } \\ \\hline x _ { 11 } & { x _ { 12 } } & { \\cdots } & { x _ { 1 d } } \\ { x _ { 21 } } & { x _ { 22 } } & { \\cdots } & { x _ { 2 d } } \\ { \\vdots } & { \\vdots } & { \\ddots } & { \\vdots } \\ { x _ { n 1 } } & { x _ { n 2 } } & { \\cdots } & { x _ { n d } } \\end{array} \\right) $$ Jika dilihat dari baris data memiliki n n objek atatu vektor dalam d d ruang dimensi atribut $$ x _ { i } = ( x _ { i 1 } , x _ { i 2 } , \\ldots , x _ { i d } ) ^ { T } \\in \\mathbb R ^ { d } $$ Jika dilihat dari sudut pandang kolom, data diangga sebagai d d objek atau vektor dalam n n dimensi ruang dengan titik-titik data $$ X _ { j } = ( x _ { 1 j } , x _ { 2 j } , \\ldots , x _ { n j } ) ^ { T } \\in R ^ { n } $$ Jika dilihat dari sudut pandang probabilitas, d d atribut dimodelkan dengan variabel acak vektor X=(X_1,X_2,...X_d)^T X=(X_1,X_2,...X_d)^T dan titik titik x_i x_i dianggap sebagai sampel acak yang diperoleh dari X X , atribut atribut tersebut independent and identfically distributed dari X X (i.i.d X X ) Mean Generalisasi persamaan (2.18) rata-rata vektor multivariate diperoleh dari masing-masing atribut yang dinyatakan dengan $$ \\mu = E [ X ] = \\left( \\begin{array} { c } { E [ X _ { 1 } ] } \\ { E [ X _ { 2 } ] } \\ { \\vdots } \\ { E [ X _ { d } ] } \\end{array} \\right) = \\left( \\begin{array} { c } { \\mu _ { 1 } } \\ { \\mu _ { 2 } } \\ { \\vdots } \\ { \\mu _ { d } } \\end{array} \\right) $$ Generalisasi persamaan (2.19) rata-rata sampel dinyatakan dengan $$ \\hat { \\mu } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } $$ Matrik Kovarian Generalisasi persamaan (2.26) untuk d d dimensi, kovarian multicovariate di dinyatakan dengan matrik kovarian simetris $ d\\times d $yang menyatakan kovarian untuk setiap pasangan atribut $$ \\Sigma = E [ ( X - \\mu ) ( X - \\mu ) ^ { T } ] = \\left( \\begin{array} { c c c c } { \\sigma _ { 1 } ^ { 2 } } & { \\sigma _ { 12 } } & { \\cdots } & { \\sigma _ { 1 d } } \\ { \\sigma _ { 21 } } & { \\sigma _ { 2 } ^ { 2 } } & { \\cdots } & { \\sigma _ { 2 d } } \\ { \\cdots } & { \\cdots } & { \\cdots } & { \\cdots } \\ { \\sigma _ { d 1 } } & { \\sigma _ { d 2 } } & { \\cdots } & { \\sigma _ { d } ^ { 2 } } \\end{array} \\right) $$ Elemen diagonal $\\sigma_i^2 $ menyatakan variansi atribut X_i X_i , dimana elemen-elemen bukan diagonal \\sigma_{ij} = \\sigma_{ji} \\sigma_{ij} = \\sigma_{ji} menyatakan kovarian antara atribut pasangan X_i X_i dan X_j X_j . Matrik kovarian adalah positif semidefinite Contoh Rata-rata sample dan matrik covarian. Perhatikan semua atribut numerik untuk data iris, namanya sepal length, petal length, dan petal width. Rata rata multivarean dinyatakan dengan \\hat { \\mu } = ( 5.843 \\quad 3.054 \\quad 3.759 \\quad 1.199 ) ^ { T } $$ dan matrik covarian nya adalah $$ \\hat { \\Sigma } = \\left( \\begin{array} { r r r r } { 0.681 } & { - 0.039 } & { 1.265 } & { 0.513 } \\\\ { - 0.039 } & { 0.187 } & { - 0.320 } & { - 0.117 } \\\\ { 1.265 } & { - 0.320 } & { 3.092 } & { 1.288 } \\\\ { 0.513 } & { - 0.117 } & { 1.288 } & { 0.579 } \\end{array} \\right) $$ Jumlah variansi adalah $$ \\operatorname { var } ( D ) = \\operatorname { tr } ( \\hat { \\Sigma } ) = 0.681 + 0.187 + 3.092 + 0.579 = 4.539 \\hat { \\mu } = ( 5.843 \\quad 3.054 \\quad 3.759 \\quad 1.199 ) ^ { T } $$ dan matrik covarian nya adalah $$ \\hat { \\Sigma } = \\left( \\begin{array} { r r r r } { 0.681 } & { - 0.039 } & { 1.265 } & { 0.513 } \\\\ { - 0.039 } & { 0.187 } & { - 0.320 } & { - 0.117 } \\\\ { 1.265 } & { - 0.320 } & { 3.092 } & { 1.288 } \\\\ { 0.513 } & { - 0.117 } & { 1.288 } & { 0.579 } \\end{array} \\right) $$ Jumlah variansi adalah $$ \\operatorname { var } ( D ) = \\operatorname { tr } ( \\hat { \\Sigma } ) = 0.681 + 0.187 + 3.092 + 0.579 = 4.539 Contoh Perkalian dalam dan perkalian luar . Untuk mengdeskripsikan komputasi perkalian dalam dan perkalian luar dari matrik covarian, perhatikan data 2-dimensi $$ D = \\left( \\begin{array} { l l } { A _ { 1 } } & { A _ { 2 } } \\ \\hline 1 & { 0.8 } \\ { 5 } & { 2.4 } \\ { 9 } & { 5.5 } \\end{array} \\right) $$ Rata-rata vektor adalah sebagai berikut $$ \\hat { \\mu } = \\left( \\begin{array} { l } { \\hat { \\mu } _ { 1 } } \\ { \\hat { \\mu } _ { 2 } } \\end{array} \\right) = \\left( \\begin{array} { l } { 15 / 3 } \\ { 8.7 / 3 } \\end{array} \\right) = \\left( \\begin{array} { c } { 5 } \\ { 2.9 } \\end{array} \\right) $$ dan matrik data terpusat dinyatakan $$ Z = D - 1 \\cdot \\mu ^ { T } = \\left( \\begin{array} { l l } { 1 } & { 0.8 } \\ { 5 } & { 2.4 } \\ { 9 } & { 5.5 } \\end{array} \\right) - \\left( \\begin{array} { l } { 1 } \\ { 1 } \\ { 1 } \\end{array} \\right) \\left( \\begin{array} { l l } { 5 } & { 2.9 } \\end{array} \\right) = \\left( \\begin{array} { r r } { - 4 } & { - 2.1 } \\ { 0 } & { - 0.5 } \\ { 4 } & { 2.6 } \\end{array} \\right) $$ Pendekatan perkalian dalam [pers. 2.30] untuk menghitung matrik kovarian adalah $$ \\left. \\begin{array}{l}{ \\hat { \\Sigma } = \\frac { 1 } { n } Z ^ { T } Z = \\frac { 1 } { 3 } \\left( \\begin{array} { c c c } { - 4 } & { 0 } & { 4 } \\ { - 2.1 } & { - 0.5 } & { 2.6 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { - 4 } & { - 2.1 } \\ { 0 } & { - 0.5 } \\ { 4 } & { 2.6 } \\end{array} \\right) }\\{ = \\frac { 1 } { 3 } \\left( \\begin{array} { c c } { 32 } & { 18.8 } \\ { 18.8 } & { 11.42 } \\end{array} \\right) = \\left( \\begin{array} { c c } { 10.67 } & { 6.27 } \\ { 6.27 } & { 3.81 } \\end{array} \\right) }\\end{array} \\right. $$ Pendekatan lain yaitu dengan perkalian luar [pers. 2.31] dibyatakan dengan $$ \\hat { \\Sigma } = \\frac { 1 } { n } \\sum _ { j = 1 } ^ { n } z _ { i } \\cdot z _ { i } ^ { T } $$ = \\frac { 1 } { 3 } \\left [ \\left( \\begin{array} { c } { - 4 } \\\\ { - 2.1 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { - 4 } & { - 2.1 } \\end{array} \\right) + \\left( \\begin{array} { r r } { 0 } \\\\ { - 0.5 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { 0 } & { - 0.5 } \\end{array} \\right) + \\left( \\begin{array} { c } { 4 } \\\\ { 2.6 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { 4 } & { 2.6 } \\end{array} \\right)\\right ] = \\frac { 1 } { 3 } \\left [ \\left( \\begin{array} { c } { - 4 } \\\\ { - 2.1 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { - 4 } & { - 2.1 } \\end{array} \\right) + \\left( \\begin{array} { r r } { 0 } \\\\ { - 0.5 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { 0 } & { - 0.5 } \\end{array} \\right) + \\left( \\begin{array} { c } { 4 } \\\\ { 2.6 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { 4 } & { 2.6 } \\end{array} \\right)\\right ] \\left. \\begin{array} { l } { = \\frac { 1 } { 3 } [ \\left( \\begin{array} { c c } { 16.0 } & { 8.4 } \\\\ { 8.4 } & { 4.41 } \\end{array} \\right) + \\left( \\begin{array} { c c } { 0.0 } & { 0.0 } \\\\ { 0.0 } & { 0.25 } \\end{array} \\right) + \\left( \\begin{array} { c c } { 16.0 } & { 10.4 } \\\\ { 10.4 } & { 6.76 } \\end{array} \\right) ] } \\\\ { = \\frac { 1 } { 3 } \\left( \\begin{array} { c c } { 32.0 } & { 18.8 } \\\\ { 18.8 } & { 11.42 } \\end{array} \\right) = \\left( \\begin{array} { c c } { 10.67 } & { 6.27 } \\\\ { 6.27 } & { 3.81 } \\end{array} \\right) } \\end{array} \\right. \\left. \\begin{array} { l } { = \\frac { 1 } { 3 } [ \\left( \\begin{array} { c c } { 16.0 } & { 8.4 } \\\\ { 8.4 } & { 4.41 } \\end{array} \\right) + \\left( \\begin{array} { c c } { 0.0 } & { 0.0 } \\\\ { 0.0 } & { 0.25 } \\end{array} \\right) + \\left( \\begin{array} { c c } { 16.0 } & { 10.4 } \\\\ { 10.4 } & { 6.76 } \\end{array} \\right) ] } \\\\ { = \\frac { 1 } { 3 } \\left( \\begin{array} { c c } { 32.0 } & { 18.8 } \\\\ { 18.8 } & { 11.42 } \\end{array} \\right) = \\left( \\begin{array} { c c } { 10.67 } & { 6.27 } \\\\ { 6.27 } & { 3.81 } \\end{array} \\right) } \\end{array} \\right. dimana data terpusat z_i z_i adalah baris dari Z Z","title":"Analisa Multivariate"},{"location":"Eksplorasi%20data/#atribut-kategorikal","text":"Kita asumsikan bahwa data terdiri dari satu atribut X X . Domain dari X X terdiri dari m m nilai simbolis dom(X)={a_1,a_2,...a_m} dom(X)={a_1,a_2,...a_m} . Data D D adalah n\\times 1 n\\times 1 matrik data simbolis yang dinyatakan dengan $$ D = \\left( \\begin{array} { c } { X } \\ { x _ { 1 } } \\ { x _ { 2 } } \\ { \\vdots } \\ { x _ { n } } \\end{array} \\right) $$ dimana setiap nilai x_i \\in dom(X) x_i \\in dom(X)","title":"Atribut Kategorikal"},{"location":"Eksplorasi%20data/#variabel-bernouli","text":"Marilah kita lihat kasus ketika atribut kategorikal X X memililik domain $ {a_1,a_2}$ dengan m=2 m=2 . Kita dapat memodelkan X X sebagai variabel acak Bernouli, yang didasarkan pada dua nilai berbeda yaitu 1 dan 0, sesuai dengan pemetaan $$ X ( v ) = \\left{ \\begin{array} { l l } { 1 } & { \\text { if } v = a _ { 1 } } \\ { 0 } & { \\text { if } v = a _ { 2 } } \\end{array} \\right. $$ Fungsi massa probabilitas (PMF) dari X X dinyatakan dengan $$ P ( X = x ) = f ( x ) = \\left{ \\begin{array} { l l } { p _ { 1 } } & { \\text { if } x = 1 } \\ { p _ { 0 } } & { \\text { if } x = 0 } \\end{array} \\right. $$ dimana p_1 p_1 dan p_0 p_0 adalah parameter distribusi, yang harus memenuhi kondisi $$ p_1+p_0=1 $$ Karena hanya ada satu parameter bebas, biasanya menotasikan p_1=p p_1=p maka p_0=1-p p_0=1-p . Fungsi Massa Probabilitas dari variabel acak Bernouli X X dapat kemudian ditulis dengan $$ P ( X = x ) = f ( x ) = p ^ { x } ( 1 - p ) ^ { 1 - x } $$ Kita dapat melihat bahwa P ( X = 1 ) = p ^ { 1 } ( 1 - p ) ^ { 0 } = p \\text { and } P ( X = 0 ) = p ^ { 0 } ( 1 - p ) ^ { 1 } = 1 - p P ( X = 1 ) = p ^ { 1 } ( 1 - p ) ^ { 0 } = p \\text { and } P ( X = 0 ) = p ^ { 0 } ( 1 - p ) ^ { 1 } = 1 - p seperti yand diharapkan Mean dan Variansi Nilai harapan dari X X dinyatakan dengan $$ \\mu = E [ X ] = 1 \\cdot p + 0 \\cdot ( 1 - p ) = p $$ dan variansi dari X X dinyatakan dengan $$ \\left. \\begin{array}{l}{ \\sigma ^ { 2 } = \\operatorname { var } ( X ) = E [ X ^ { 2 } ] - ( E [ X ] ) ^ { 2 } }\\ \\hspace{7mm}= ( 1 ^ { 2 } \\cdot p + 0 ^ { 2 } \\cdot ( 1 - p ) ) - p ^ { 2 } = p - p ^ { 2 } = p ( 1 - p ) \\\\end{array} \\right. $$ Rata-rata sampel dan Variansi Untuk mengestimasi parameter dari variabel Bernouli X X , kita asumsikan bahwa setiap simbol dipetakan ke nilai biner. Sehingga, sekumpulan nilai {x_1,x_2,...x_n} {x_1,x_2,...x_n} diasumsikan menjadi sampel acak yang diperoleh dari X X (yaitu setiap $ x_i$ adalah IID dengan X X . Rata-rata sampel dinyatakan dengan $$ \\hat { \\mu } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } = \\frac { n _ { 1 } } { n } = \\hat { p } $$ dimana n_1 n_1 adalah banyaknya titik dengan x_1=1 x_1=1 dalam sampel acak (sama dengan banyak kejadian dari simbol a_1 a_1 ) Misal n_0=n-n_1 n_0=n-n_1 menyatakan banyak titik dengan x_i=0 x_i=0 dalam sampel acak. Variansi sample dinyatakan dengan $$ \\left. \\begin{array}{l}{ \\hat { \\sigma } ^ { 2 } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\hat { \\mu } ) ^ { 2 } }\\ \\hspace{7mm}{ = \\frac { n _ { 1 } } { n } ( 1 - \\hat { p } ) ^ { 2 } + \\frac { n - n _ { 1 } } { n } ( - \\hat { p } ) ^ { 2 } }\\\\hspace{7mm}{ = \\hat { p } ( 1 - \\hat { p } ) ^ { 2 } + ( 1 - \\hat { p } ) \\hat { p } ^ { 2 } }\\\\hspace{7mm}{ = \\hat { p } ( 1 - \\hat { p } ) ( 1 - \\hat { p } + \\hat { p } ) }\\\\hspace{7mm}{ = \\hat { p } ( 1 - \\hat { p } ) }\\end{array} \\right. $$ Variansi sampel dapat juga diperoleh langsung dari persamaan(3.1) dengan mensubsitusikan \\hat p \\hat p untuk p p . Contoh Perhatikan atribut sepal length ( X X ) untuk dataset iris dalam tabel 1.1. Marilah kita definisikan bunga iris dengan Long jika bunga itu sepal length dalam range [7, \\infty ] [7, \\infty ] , dan short jika sepal length dalam range [-\\infty,7] [-\\infty,7] . Kemudian X_1 X_1 dapat dinyatakan dengan atribut kategorikan dengan domain {Long,Short}. Dari sampel yang diamati ukuran n=150 n=150 , kita menemukan 13 iris long. Rata-rata sampel dari X_1 X_1 adalah $$ \\hat { \\mu } = \\hat { p } = 13 / 150 = 0.087 $$ dan variansinya adalah $$ \\hat { \\sigma } ^ { 2 } = \\hat { p } ( 1 - \\hat { p } ) = 0.087 ( 1 - 0.087 ) = 0.087 \\cdot 0.913 = 0.079 $$ Ditribusi binomial : banyaknya kejadian Diberikan variabel Bernoulli X X , misal \\{x_1,x_2,...x_n\\} \\{x_1,x_2,...x_n\\} menyatakan sampel acak dari ukuran n n yang diperoleh dari X X . Misal N N adalah variabel acak yang menyatakan numlah kejadi dari simbol a_1 a_1 (nilai X=1 X=1 ) dalam sampe. N adalah distribusi binomial yang dinyatakan dengan $$ f ( N = n _ { 1 } | n , p ) = \\left( \\begin{array} { l } { n } \\ { n _ { 1 } } \\end{array} \\right) p ^ { n _ { 1 } } ( 1 - p ) ^ { n - n _ { 1 } } $$ Dalam kenyataannya, N N adalah jumlah dari n n variabel acak Bernoulli x_i x_i yang saling bebas dan (IID) dengan X X yaitu N=\\sum_{i=1}^n x_i N=\\sum_{i=1}^n x_i . Dengan liniearitas dari ekpektasi, mean atau jumlah harapan dari kejadian simbol a_i a_i dinyatakan dengan $$ \\mu _ { N } = E [ N ] = E \\left[ \\sum _ { i = 1 } ^ { n } x _ { i } \\right] = \\sum _ { i = 1 } ^ { n } E [ x _ { i } ] = \\sum _ { i = 1 } ^ { n } p = n p $$ Karena x_i x_i adalah semuanya saling bebas, variansi dari N N dinyatakan dengan $$ \\sigma _ { N } ^ { 2 } = \\operatorname { var } ( N ) = \\sum _ { i = 1 } ^ { n } \\operatorname { var } ( x _ { i } ) = \\sum _ { i = 1 } ^ { n } p ( 1 - p ) = n p ( 1 - p ) $$ Contoh 3.2. Dengan meneruskan contoh 3.1, kita dapat menggunakan parameter yang telah diestimasi \\hat p=0.087 \\hat p=0.087 untuk menghitung banyaknya kejadian yang diharapkan N long dari sepal length. distribusi binomial Iris $$ E [ N ] = n \\hat { p } = 150 \\cdot 0.087 = 13 $$ Dalam kasus ini, karena p p dihitung dari sample melalui \\hat p \\hat p , tidak mengherankan bahwa jumlah kejadian diharapkan dari Long Iris sama dengan kejadian yang sebenarnya. Akan tetapi yang lebih menarik adalah kita dapat menghitung variansi jumlah kejadian $$ \\operatorname { var } ( N ) = n \\hat { p } ( 1 - \\hat { p } ) = 150 \\cdot 0.079 = 11.9 $$ Meningkatnya ukuran sample, distribusi binomial seperti yang diberikan dapalam persamaan 3.3 cenderung ke distribusi normal dengan \\mu=13 \\mu=13 dan \\sigma=\\sqrt{11.9}=3.45 \\sigma=\\sqrt{11.9}=3.45 . Sehingga dengan kepercaan lebih besar dari 95%, kita dapat mengklam bahwa jumlah kejadian dari a_i a_i akan terletak dalam rentang \\mu \\pm 2 \\sigma = [ 9.55,16.45 ] \\mu \\pm 2 \\sigma = [ 9.55,16.45 ] yang mengikuti dari fakta bahwa untuk distribusi normal 95,45% dari massa probabilitas terletak dalam dua standar deviasi dari rata-rata.","title":"Variabel Bernouli"},{"location":"Eksplorasi%20data/#variable-multivariate-bernoulli","text":"Sekarang kita memandang kasus umum ketika X X adalah atribut kategorical dengan domain \\{a_1,a_2,...a_m\\} \\{a_1,a_2,...a_m\\} . Kita dapat memodelkan X X sebagai variabel acak Bernoulli m m -dimensi X = ( A _ { 1 } , A _ { 2 } , \\ldots , A _ { m } ) ^ { T } X = ( A _ { 1 } , A _ { 2 } , \\ldots , A _ { m } ) ^ { T } dimana setiap A_i A_i adalah variabel Bernoulli dengan parameter p_i p_i yang menotasikan probabilitas dari pengamatan simbol a_i a_i . Akan tetapi karena X X dapat mengasumsikan hanya satu dari nilai simbolik pada suatu waktum jika X=a_i X=a_i maka A_i=1 A_i=1 dan A_j=0 A_j=0 untuk semua j \\neq i j \\neq i . Variabel acak X \\in {0,1}^m X \\in {0,1}^m , dan jika X=a_i X=a_i , maka X=e_i X=e_i , dimana e_i e_i adalah standar vektor basis ke i, e_i\\in\\mathbb R^m e_i\\in\\mathbb R^m yang dinyatakan dengan $$ e _ { i } = ( \\overbrace { 0 , \\ldots , 0 } ^ { i - 1 } , 1 , \\overbrace { 0 , \\ldots , 0 } ^ { m - i } ) ^ { T } $$ Pada e_i e_i hanya elemen ke i adalah 1 ( e_{ii}=1 e_{ii}=1 ) , sedangkan semua elemen yang lain adalah nol, ( e_{ij}=0, \\forall j \\neq i e_{ij}=0, \\forall j \\neq i ). Disini, definis yang lebih tepat dari variabel Bernoulli multivariate , yaitu generalisasi dari variabel Bernoullii dari dua hasil ke m m hasil. Kita kemudian memodelkan atribut kategorical X X sebagai variabel Bernoulli multivariate X X didefinisikan dengan $$ X ( v ) = e _ { i } \\text { if } v = a _ { i } $$ Rentang dari X X terdiri dari m m nilai vektor berbeda \\{e_1,e_2,...e_m\\} \\{e_1,e_2,...e_m\\} dengan fungsi massa probabilitas dari X X dinyatakan dengan $$ P ( X = e _ { i } ) = f ( e _ { i } ) = p _ { i } $$ dimana p_i p_i adalah probabilitas dari nilai pengamatan a_i a_i . Parameter ini harus memenuhi kondisi $$ \\sum _ { i = 1 } ^ { m } p _ { i } = 1 $$ Fungsi massa prababilitas dapat ditulis secara utuh sebagai berikut $$ P ( X = e _ { i } ) = f ( e _ { i } ) = \\prod _ { j = 1 } ^ { m } p _ { j } ^ { e _ { i j } }Ka $$ Kareana e_ii=1 e_ii=1 dan e_ij=0 e_ij=0 funtuk $ j\\neq i$, kita dapat melihat bahwa, seperti yang diharapkan, kita miliki $$ f ( e _ { i } ) = \\prod _ { j=1 } ^ { m } p _ { j } ^ { e _ { i j } } = p _ { 1 } ^ { e _ { i 0 } } \\times \\cdots p _ { i } ^ { e _ { i i } } \\cdots \\times p _ { m } ^ { e _ { i m } } = p _ { 1 } ^ { 0 } \\times \\cdots p _ { i } ^ { 1 } \\cdots \\times p _ { m } ^ { 0 } = p _ { i } $$ \\left. \\begin{array} { | l | l | l | } \\hline \\text { Bins } & { { \\text { Domain } } } & { { \\text { Counts } } } \\\\ \\hline [ 4.3,5.2 ] & { \\text { Very Short } ( a _ { 1 } ) } & { n _ { 1 } = 45 } \\\\ { ( 5.2,6.1 ] } & { \\text { Short } ( a _ { 2 } ) } & { n _ { 2 } = 50 } \\\\ { ( 6.1,7.0 ] } & { \\text { Long } ( a _ { 3 } ) } & { n _ { 3 } = 43 } \\\\ { ( 7.0,7.9 ] } & { \\text { Very Long } ( a _ { 4 } ) } & { n _ { 4 } = 12 } \\\\ \\hline \\end{array} \\right. \\left. \\begin{array} { | l | l | l | } \\hline \\text { Bins } & { { \\text { Domain } } } & { { \\text { Counts } } } \\\\ \\hline [ 4.3,5.2 ] & { \\text { Very Short } ( a _ { 1 } ) } & { n _ { 1 } = 45 } \\\\ { ( 5.2,6.1 ] } & { \\text { Short } ( a _ { 2 } ) } & { n _ { 2 } = 50 } \\\\ { ( 6.1,7.0 ] } & { \\text { Long } ( a _ { 3 } ) } & { n _ { 3 } = 43 } \\\\ { ( 7.0,7.9 ] } & { \\text { Very Long } ( a _ { 4 } ) } & { n _ { 4 } = 12 } \\\\ \\hline \\end{array} \\right. Contoh : Marilah kita lihat atribut sepal length ( X_1 X_1 ) untuk data Iris seperti yang ditunjukkan dalam tabel 1.2. Kita membagi sepal length kedalam empat interval yang sama, dan memberikan nama untuk setiap interval seperti yang diunjukkan dalam tabel 3.1. Kita lihat X_1 X_1 sebagai atribut kategorical dengan domain $$ {a _ { 2 } = \\text { VeryShort, } a _ { 2 } = \\text { Short, } a _ { 3 } = \\operatorname { Long } , a _ { 4 } = \\operatorname{Very Long}} $$ Kita memodelkan atribut kategorical X_1 X_1 sebagai variabel X X Bernoulli multivariate, didefinisikan dengan $$ X ( v ) = \\left{ \\begin{array} { l l } { e _ { 1 } = ( 1,0,0,0 ) } & { \\text { jika } v = a _ { 1 } } \\ { e _ { 2 } = ( 0,1,0,0 ) } & { \\text { jika } v = a _ { 2 } } \\ { e _ { 3 } = ( 0,0,1,0 ) } & { \\text { jika } v = a _ { 3 } } \\ { e _ { 4 } = ( 0,0,0,1 ) } & { \\text { jika } v = a _ { 4 } } \\end{array} \\right. $$ Misalkan, simbol x_1=Short=a_2 x_1=Short=a_2 dinyatakan dengan (0,1,0,0)^T=e_2 (0,1,0,0)^T=e_2 Mean Mean atau nilai harapan dari X X dapat diperoleh dengan $$ \\mu = E [ X ] = \\sum _ { i = 1 } ^ { m } e _ { i } f ( e _ { i } ) = \\sum _ { i = 1 } ^ { m } e _ { i } p _ { i } = \\left( \\begin{array} { l } { 1 } \\ { 0 } \\ { \\vdots } \\ { 0 } \\end{array} \\right) p _ { 1 } + \\cdots + \\left( \\begin{array} { l } { 0 } \\ { 0 } \\ { \\vdots } \\ { 1 } \\end{array} \\right) p _ { m } = \\left( \\begin{array} { c } { p _ { 1 } } \\ { p _ { 2 } } \\ { \\vdots } \\ { p _ { m } } \\end{array} \\right) = p $$","title":"Variable multivariate Bernoulli"},{"location":"Eksplorasidata-2/","text":"Install hugo https://www.mikedane.com/static-site-generators/hugo/installing-hugo-on-windows/ Sampel Mean Disumsikan bahwa setiap titk simbol x_i \\in D x_i \\in D dipetakan ke variabel x_i=X(x_i) x_i=X(x_i) . Data yang telah dipetakan x_1,x_2,....x_n x_1,x_2,....x_n adalah kemudian diasumsikan sampel acak IID dengan X X . Kita dapat menghitung sampel mean dengan menempatkan massa proabilitas dari $ \\frac {1}{n}$ pada setiap titik $$ \\hat { \\mu } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } = \\sum _ { i = 1 } ^ { m } \\frac { n _ { i } } { n } e _ { i } = \\left( \\begin{array} { c } { n _ { 1 } / n } \\ { n _ { 2 } / n } \\ { \\vdots } \\ { n _ { m } / n } \\end{array} \\right) = \\left( \\begin{array} { c } { \\hat { p } _ { 1 } } \\ { \\hat { p } _ { 2 } } \\ { \\vdots } \\ { \\hat { p } _ { m } } \\end{array} \\right) = \\hat { p } $$ dimana n_i n_i adalah banyaknya kejadian dari nilai vektor e_i e_i dalam sampel, yang ekivalen dengan banyaknya kejadian dari simbol a_i a_i . Selanjutnya, kita memiliki \\sum_{i=1}^m n_i=n \\sum_{i=1}^m n_i=n , yang mengikuti dari fakta bahwa X X hanya dapat diperoleh pada m m yang berbeda e_i e_i dan perhitungan setiap nilai haru ditambahkan hingga ke ukuran sampel n n Contoh3.4. Sampel Mean . Perthatikan jumlah yang diamati untuk setiap nilai a_i a_i (e_i) (e_i) dari diskritisasi atribut sepal length dalam tabel 3.1. Karena jumlah sampel adalah n=150 n=150 , dari sini kita dapat estimasi \\hat p_i \\hat p_i sebagai berikut $$ \\left. \\begin{array} { l } { \\hat { p } _ { 1 } = 45 / 150 = 0.3 } \\ { \\hat { p } _ { 2 } = 50 / 150 = 0.333 } \\ { \\hat { p } _ { 3 } = 43 / 150 = 0.287 } \\ { \\hat { p } _ { 4 } = 12 / 150 = 0.08 } \\end{array} \\right. $$ Fungsi Massa Probabilias diplot dalam gambar 3.1 dan sample mean untuk X dinyatakan dengan $$ \\hat { \\mu } = \\hat { p } = \\left( \\begin{array} { c } { 0.3 } \\ { 0.333 } \\ { 0.287 } \\ { 0.08 } \\end{array} \\right) $$ Matrik Covarian Perhatikan lagi bahwa m m -dimensi variabel multivariate Bernouli adalah sederhananya vektor dari m m variabel Bernoulli. Misalkan X=(A_1,A_2,...A_m)^T X=(A_1,A_2,...A_m)^T dimana A_i A_i adalah variabel Bernoulli yang terkait dengan simbol a_i a_i . Informasi variansi covarian antara unsur-unsur variabel Bernoully yang menghasilkan matrik untuk X X Marilah kita pertama kita perhatikan variansi dari setiap variabel Bernoulli A_i A_i . Dengan persamaan (3.1),kita segera memiliki $$ \\sigma _ { i } ^ { 2 } = \\operatorname { var } ( A _ { i } ) = p _ { i } ( 1 - p _ { i } ) $$ Berikutnya perhatikan covariasi antara A_1 A_1 dan A_j A_j . Dengan memanfaatkan identitas (2.21) kita miliki $$ \\sigma _ { i j } = E [ A _ { i } A _ { j } ] - E [ A _ { i } ] \\cdot E [ A _ { j } ] = 0 - p _ { i } p _ { j } = - p _ { i } p _ { j } $$ yang mengikuti dari fakta bahwa E[A_iA_j]=0 E[A_iA_j]=0 sehingga A_1 A_1 dan A_2 A_2 keduanya tidak sama dengan 1 dan kemudian perkalian A_iA_j=0 A_iA_j=0 . Fakta yang sama ini terkait dengan relasi negatif antara A_i A_i dan A_j A_j . Yang menarik adalah bahwa derajat keterkaitan negatif adalah proporsional pada perkalian dari nilai mean A_i A_i dan A_j A_j . Dari eskperesi sebelumnya untuk varian dan covarian, m\\times m m\\times m matrik covarian untuk X X dinyatakan dengan $$ \\Sigma = \\left( \\begin{array} { c c c c } { \\sigma _ { 1 } ^ { 2 } } & { \\sigma _ { 12 } } & { \\dots } & { \\sigma _ { 1 m } } \\ { \\sigma _ { 12 } } & { \\sigma _ { 2 } ^ { 2 } } & { \\dots } & { \\sigma _ { 2 m } } \\ { \\vdots } & { \\vdots } & { \\ddots } & { \\vdots } \\ { \\sigma _ { 1 m } } & { \\sigma _ { 2 m } } & { \\dots } & { \\sigma _ { m } ^ { 2 } } \\end{array} \\right) = \\left( \\begin{array} { c c c c } { p _ { 1 } ( 1 - p _ { 1 } ) } & { - p _ { 1 } p _ { 2 } } & { \\dots } & { - p _ { 1 } p _ { m } } \\ { - p _ { 1 } p _ { 2 } } & { p _ { 2 } ( 1 - p _ { 2 } ) } & { \\dots } & { - p _ { 2 } p _ { m } } \\ { \\vdots } & { \\vdots } & { \\ddots } & { \\vdots } \\ { - p _ { 1 } p _ { m } } & { - p _ { 2 } p _ { m } } & { \\cdots } & { p _ { m } ( 1 - p _ { m } ) } \\end{array} \\right) $$ Perhatikan bagaimana setiap baris dalam \\Sigma \\Sigma adalah nol. Misalkan, untuk baris i i kita punya $$ - p _ { i } p _ { 1 } - p _ { i } p _ { 2 } - \\cdots + p _ { i } ( 1 - p _ { i } ) - \\cdots - p _ { i } p _ { m } = p _ { i } - p _ { i } \\sum _ { l = 1 } ^ { m } p _ { j } = p _ { i } - p _ { i } = 0 $$ Karna \\Sigma \\Sigma adalah simetris, maka memungkinkan setiap kolom jumlahnya adalah nol. Definisi P sebagai m\\times m m\\times m matrik diagonal: $$ P = \\operatorname { diag } ( p ) = \\operatorname { diag } ( p _ { 1 } , p _ { 2 } , \\ldots , p _ { m } ) = \\left( \\begin{array} { c c c c } { p _ { 1 } } & { 0 } & { \\cdots } & { 0 } \\ { 0 } & { p _ { 2 } } & { \\cdots } & { 0 } \\ { \\vdots } & { \\vdots } & { \\ddots } & { \\vdots } \\ { 0 } & { 0 } & { \\cdots } & { p _ { m } } \\end{array} \\right) $$ Kita dapat menulis matrik kovarian X X dengan $$ \\Sigma = P - p \\cdot p ^ { T } $$ Matrik Kovarian Sampel Matrik kovarian sample dapat diperoleh dari (3.8) dengan jelas yaitu $$ \\hat { \\Sigma } = \\hat { P } - \\hat { p } \\cdot \\hat { p } ^ { T }dimana \\hat P=diag(\\hat p) \\hat P=diag(\\hat p) dan $\\hat p=\\hat \\mu =(hat) $$ dimana \\hat P=diag(\\hat p) \\hat P=diag(\\hat p) dan \\hat p=\\hat \\mu =(\\hat p_1,\\hat p_2,...\\hat p_m) \\hat p=\\hat \\mu =(\\hat p_1,\\hat p_2,...\\hat p_m) menyatakan fungsi massa k probabilitas empiris untuk X X . Contoh. Dari hasil diskritisasi atribut sepal length dalam contoh 3.4 kita telah memiliki $ \\hat { \\mu } = \\hat { p } = ( 0.3,0.333,0.287,0.08 ) ^ { T }$ Mtrik kovarian sample dinyatakan dengan $$ \\left. \\begin{array}{l}{ \\hat { \\Sigma } = \\hat { P } - \\hat { p } \\cdot \\hat { p } ^ { T } }\\\\hspace{5mm}{ = \\left( \\begin{array} { c c c c } { 0.3 } & { 0 } & { 0 } & { 0 } \\ { 0 } & { 0.333 } & { 0 } & { 0 } \\ { 0 } & { 0 } & { 0.287 } & { 0 } \\ { 0 } & { 0 } & { 0 } & { 0.08 } \\end{array} \\right) - \\left( \\begin{array} { c } { 0.3 } \\ { 0.333 } \\ { 0.287 } \\ { 0.08 } \\end{array} \\right) \\left( \\begin{array} { l l l l } { 0.3 } & { 0.333 } & { 0.287 } & { 0.08 } \\end{array} \\right) }\\end{array} \\right.\\ \\left. \\begin{array} { l } { = \\left( \\begin{array} { c c c c } { 0.3 } & { 0 } & { 0 } & { 0 } \\ { 0 } & { 0.333 } & { 0 } & { 0 } \\ { 0 } & { 0 } & { 0.287 } & { 0 } \\ { 0 } & { 0 } & { 0 } & { 0.08 } \\end{array} \\right) - \\left( \\begin{array} { c c c c } { 0.09 } & { 0.1 } & { 0.086 } & { 0.024 } \\ { 0.1 } & { 0.111 } & { 0.096 } & { 0.027 } \\ { 0.086 } & { 0.096 } & { 0.082 } & { 0.023 } \\ { 0.024 } & { 0.027 } & { 0.023 } & { 0.006 } \\end{array} \\right) } \\ { = \\left( \\begin{array} { r r r r } { 0.21 } & { - 0.1 } & { - 0.086 } & { - 0.024 } \\ { - 0.1 } & { 0.222 } & { - 0.096 } & { - 0.027 } \\ { - 0.086 } & { - 0.096 } & { 0.204 } & { - 0.023 } \\ { - 0.024 } & { - 0.027 } & { - 0.023 } & { 0.074 } \\end{array} \\right) } \\end{array} \\right. $$ Mean sample persamaan (3.6 ) adalah $$ \\hat { \\mu } = \\hat { p } = ( 2 / 5,3 / 5 ) ^ { T } = ( 0.4,0.6 ) ^ { T } $$ dan matrik kovarian sample (3.9) adalah $$ \\hat { \\Sigma } = \\hat { P } - \\hat { p } \\hat { p } ^ { T } = \\left( \\begin{array} { c c } { 0.4 } & { 0 } \\ { 0 } & { 0.6 } \\end{array} \\right) - \\left( \\begin{array} { l } { 0.4 } \\ { 0.6 } \\end{array} \\right) \\left( \\begin{array} { l l } { 0.4 } & { 0.6 } \\end{array} \\right)\\ \\hspace{50mm}= \\left( \\begin{array} { c c } { 0.4 } & { 0 } \\ { 0 } & { 0.6 } \\end{array} \\right) - \\left( \\begin{array} { c c } { 0.16 } & { 0.24 } \\ { 0.24 } & { 0.36 } \\end{array} \\right) = \\left( \\begin{array} { r r } { 0.24 } & { - 0.24 } \\ { - 0.24 } & { 0.24 } \\end{array} \\right) $$ Tabel 3.2 (a) Dataset Kategorical (b) dataset yang telah dipetakan ke biner \u00a9 dataset yang telah dicentering Untuk menunjukkan bahwa hasilnya sema yang telah diperoleh dengan analisa standar numerik, kita memetakan atribut kategorical X X menjadi dua atribut Bernoulli A_1 A_1 dan A_2 A_2 masing masing dengan simbol Long dan Short masing masing. Dataset yang dipetakan ditunjukkan dalam tabel 3.2b. Mean sampel sederhana diperolah dengan $$ \\hat { \\mu } = \\frac { 1 } { 5 } \\sum _ { i = 1 } ^ { 5 } x _ { i } = \\frac { 1 } { 5 } ( 2,3 ) ^ { T } = ( 0.4,0.6 ) ^ { T } $$ Selanjutnyaang te, kita centerkan dataset dengan mengurangkan mean dari masing masing atribut. Setelah dicentering, dataset dipetakan seperti yang ditunjukkan dalam tabel 3.2 dengan atribut Z_i Z_i seperti atrbut yang telah dicenter A_i A_i . KIta dapat menghitung matrik covarian dengan menggunakan inner product [persamaan2.30] pada kolom yang telah dipusatkan. Kita miliki $$ \\left. \\begin{array}{l}{ \\sigma _ { 1 } ^ { 2 } = \\frac { 1 } { 5 } Z _ { 1 } ^ { T } Z _ { 1 } = 1.2 / 5 = 0.24 }\\{ \\sigma _ { 2 } ^ { 2 } = \\frac { 1 } { 5 } Z _ { 2 } ^ { T } Z _ { 2 } = 1.2 / 5 = 0.24 }\\{ \\sigma _ { 12 } = \\frac { 1 } { 5 } Z _ { 1 } ^ { T } Z _ { 2 } = - 1.2 / 5 = - 0.24 }\\end{array} \\right. $$ Kemudian matrik kovarian sample dinyatakan dengn $$ \\hat { \\Sigma } = \\left( \\begin{array} { r r } { 0.24 } & { - 0.24 } \\ { - 0.24 } & { 0.24 } \\end{array} \\right) $$ yang sesuai dengan hasil yang diperoleh dengan menggunakan pendekatan model Bernoulli multivariate. Analisa Bivariate \u00b6 Asumsikan data terdiri dari dua atribut kategorikal X_1 X_1 dan X_2 X_2 dengan $$ \\left. \\begin{array} { l } { \\operatorname { dom } ( X _ { 1 } ) = { a _ { 11 } , a _ { 12 } , \\ldots , a _ { 1 m _ { 1 } } } } \\ { \\operatorname { dom } ( X _ { 2 } ) = { a _ { 21 } , a _ { 22 } , \\ldots , a _ { 2 m _ { 2 } } } } \\end{array} \\right. $$ Kita telah memberikan n n titik kategorical dari bentuk $x_i=(x_{i1},x_{i2})^T $ dengan x_{i1} \\in dom(X_1) x_{i1} \\in dom(X_1) dan x_{i2} \\in dom(X_2) x_{i2} \\in dom(X_2) . Dataset adalah matrik simbolik n\\times 2 n\\times 2 yaitu $$ D = \\left( \\begin{array} { c c } { X _ { 1 } } & { X _ { 2 } } \\ \\hline x _ { 11 } & { x _ { 12 } } \\ { x _ { 21 } } & { x _ { 22 } } \\ { \\vdots } & { \\vdots } \\ { x _ { n 1 } } & { x _ { n 2 } } \\end{array} \\right) $$ Kita dapat memodelkan X_1 X_1 dan X_2 X_2 sebagai variabel Bernoulli multivariate X1 X1 dan $ X_2$ dengan dimensi m_1 m_1 dan m_2 m_2 . Fungsi massa probabilitas untuk X_1 X_1 dan X_2 X_2 dinyatakan dengan sesuai persamaan (3.4) $$ \\left. \\begin{array} { l } { P ( X _ { 1 } = e _ { 1 i } ) = f _ { 1 } ( e _ { 1 i } ) = p _ { i } ^ { 1 } = \\prod _ { k = 1 } ^ { m _ { 1 } } ( p _ { i } ^ { 1 } ) ^ { c _ { i k } ^ { 1 } } } \\ { P ( X _ { 2 } = e _ { 2 j } ) = f _ { 2 } ( e _ { 2 j } ) = p _ { j } ^ { 2 } = \\prod _ { k = 1 } ^ { m _ { 2 } } ( p _ { j } ^ { 2 } ) ^ { e _ { j k } ^ { 2 } } } \\end{array} \\right. $$","title":"Eksplorasidata 2"},{"location":"Eksplorasidata-2/#analisa-bivariate","text":"Asumsikan data terdiri dari dua atribut kategorikal X_1 X_1 dan X_2 X_2 dengan $$ \\left. \\begin{array} { l } { \\operatorname { dom } ( X _ { 1 } ) = { a _ { 11 } , a _ { 12 } , \\ldots , a _ { 1 m _ { 1 } } } } \\ { \\operatorname { dom } ( X _ { 2 } ) = { a _ { 21 } , a _ { 22 } , \\ldots , a _ { 2 m _ { 2 } } } } \\end{array} \\right. $$ Kita telah memberikan n n titik kategorical dari bentuk $x_i=(x_{i1},x_{i2})^T $ dengan x_{i1} \\in dom(X_1) x_{i1} \\in dom(X_1) dan x_{i2} \\in dom(X_2) x_{i2} \\in dom(X_2) . Dataset adalah matrik simbolik n\\times 2 n\\times 2 yaitu $$ D = \\left( \\begin{array} { c c } { X _ { 1 } } & { X _ { 2 } } \\ \\hline x _ { 11 } & { x _ { 12 } } \\ { x _ { 21 } } & { x _ { 22 } } \\ { \\vdots } & { \\vdots } \\ { x _ { n 1 } } & { x _ { n 2 } } \\end{array} \\right) $$ Kita dapat memodelkan X_1 X_1 dan X_2 X_2 sebagai variabel Bernoulli multivariate X1 X1 dan $ X_2$ dengan dimensi m_1 m_1 dan m_2 m_2 . Fungsi massa probabilitas untuk X_1 X_1 dan X_2 X_2 dinyatakan dengan sesuai persamaan (3.4) $$ \\left. \\begin{array} { l } { P ( X _ { 1 } = e _ { 1 i } ) = f _ { 1 } ( e _ { 1 i } ) = p _ { i } ^ { 1 } = \\prod _ { k = 1 } ^ { m _ { 1 } } ( p _ { i } ^ { 1 } ) ^ { c _ { i k } ^ { 1 } } } \\ { P ( X _ { 2 } = e _ { 2 j } ) = f _ { 2 } ( e _ { 2 j } ) = p _ { j } ^ { 2 } = \\prod _ { k = 1 } ^ { m _ { 2 } } ( p _ { j } ^ { 2 } ) ^ { e _ { j k } ^ { 2 } } } \\end{array} \\right. $$","title":"Analisa Bivariate"},{"location":"Evaluasi%20Model/","text":"Referensi : Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning, Sebastian Raschka https://ui.agiletoolkit.org/demos/form/form3.php Evaluasi Model, Memilih Model dan Memilih Algoritma dalam Pembelajaran Mesin \u00b6 Mengunakan secara benar dalam evaluasi model, memilih model dan memilih algoritma adalah penting dalam penelitian akademik pembelajaran mesin. Artikel ini membahas berbagai teknik yang digunakan untuk masing masing dari tiga tugas diatas dan membahas kelebihan dan kekurangan dari setiap teknik denga mengacu kepada studi teoritis dan empiris. Selanjutnya, rekomendasi diberikan untuk mendapatkan yang terbaik dalam penelitian dan aplikasi pembelajaran mesin. Metode yang paling umum seperti holdout methods untuk evaluasi model dan memilih model akan dibahas yang tidak direkomendasikan ketika bekerja dengan dataset yang sedikit. Kemudian berikutnya juga membahas teknik cross-validation seperit leave-one-out cross-validation dan teknik k-fold cross-validation, serta jalan tengah bias-variance untuk memilih k k juga akan dibahas dan tip praktis untuk memilih secara optimal k k didasarkan pada kejadian empiris. Berbagai tes statistik untuk membandingkan algoritma dibahas dan strategi yang berkaitan dengan berbagai perbandingan seperti test omnibus dan multiple-comparison corrections juga aakan dibahas. Dan terakhir adalah metode lain untuk memilih algoritma seperti kombias F-test 5x2 cross-validation dan nested cross-validation direkomendasikan untuk membandingkan algoritma ketika dataset sedikit Pengantar. Prinsip dasar Evaluasi Model dan istilah-istilahnya \u00b6 Pembelajaran Mesin telah menjadi bagian penting dalam kehidupan, khususnya peneliti dan praktisi industri. Bilamana kita menggunakan teknik pemodela untuk penelitian kita atau masalah bisnis yang kita hadapi, saya yakitn kita memiliki satu hal umum, kita ingin membuat \"good \" prediksi. Membuat suatu model yang sesuai dengan data latih kita adalah satu hal yang digunakan tetapi bagaiman kita tahu bahwa model yang telah kita buat baik untuk mengeneralisasi data yang belum kita ketahui. Artinya bahwa model yang baik yang didasarkan pada proses pembelajaran dari data latih akan baik untuk data yang akan diprediksi . Dan bagaimana kita memilih model yang baik. Mungkin algoritma pembelajaran yang berbeda dapat lebih baik untuk masalah yang kita tangani?. Mengevaluasi model tentunya bukan akhir dari suatu mesin pembelajaran. Seblum menangani data, kita ingin berencana kedepan dan menggunakan teknik yang sesuai dengan tujuan kita. Dalam paper ini, kita ingin memilih teknik ini dan kita akan melihat bagaimana dan melihat bagaiman menyesuaikan dengan gambaran garis besar dalam pembelajaran mesin. 1.1. Mengukur Performansi: Performansi secara umum dan Memilih Model Mari kita perhatikan pertanyaan sebelumnya, bagaiman kita menghitung performansi dari model pembelajaran mesin. Jawaban umum untuk pertanyaan ini mungkin sebagai berikut: Pertaa, kita masukkan data latih ke algoritma pembelajran mesin untuk belajar model. Kedua, kita prediksi label dari data tes kita.Ketiga kita hitung jumlah prediksi yang yang salah pada data tes untuk menghitung akurasi model prediksinya. Sayangnya, menghitung performansi suatu model adalah bukan hal sepele. Kita akan menyelesaikan pertanyaan sebelumnya itu dari sudut pandang yang berbeda. Mengapa kita memperhatikan segalanya? Idealnya, performansi yang dihitung dari model terhadap data yang tidak nampak-/membuat prediksi pada data baru yang menjadi masalah utama yang akan kita pecahkan dalam pembelajaran mesian atau pengembangan algoritma baru. Biasanya, pembelajaran mesin melibatkan banyak eksperimen, meskipun - misalnya, penyetelan bagian bagian internal algoritma pembelajaran, yang disebut hyperparameters. Menjalankan algoritma pembelajaran melalui dataset pelatihan dengan pengaturan hyperparameter yang berbeda akan menghasilkan model yang berbeda dan selanjutnya kita memperhatikan model tertentu dengan menentukan peringkat mereka terhadap satu sama lain. Beberapa hal yang perlu diperhatikan dalam membandingkan suatu algoritma dengan algoritma lain yaitu Kita akan menghitung performansi secara umum, performansi prediksi dari model pada data baru (data yang belum diketahui) Kita ingin meningkatkan performansi prediksi dengan mengutak-atik algoritma pembelajaran dan memilih model dengan kinerja terbaik dari ruang hipotesis yang diberikan Kami ingin mengidentifikasi algoritma pembelajaran mesin yang paling cocok untuk masalah yang dihadapi n; jadi, kami ingin membandingkan berbagai algoritma, memilih yang berkinerja terbaik serta model berkinerja terbaik dari ruang hipotesis algoritma Walaupun ketiga hal diatas secara umum sama yaitu ingin menghitung performansi suatu model, ketiganya masing masing masing memiliki pendekatan yang berbeda. Kita akan membahas berbagai medoe untuk menyelesaiakn masing masing dalam artikel ini. Jelas, kita ingin menghitung performansi kedepan dari model seakurat mungkin. Akan tetapi kita akan memperhatikan bahwa performansi bias dari model sebenarnya dalam memilih model dan memilih algoritma kola efek bias semua model sama. Jika kita urutkan berbagai model terhadap yang lain, selanjutnya kiat akan memilih model yang memiliki performansi terbaik, kita hanya perlu mengetetahui performansi relatif mereka. 1.2 Asumsi dan istilah Evaluasi model tentunya topik yang sangat kompleks. Untuk memastikan bahwa kita tidak menyebar dari intinya, kita menentukan asumsi dan selanjutnya akan kita gunakan dalam artikel ini. i.i.d . Kita asumsikan bahwa masing masing objekpada data latih adalah i.id (independent and identically distributed) yang artinya bahwa semua objek diperoleh dari distribusi probabilitas yang sama dan secara statistik saling bebas dengan yang lain. Skenario dimana data latih adalah tidak saling bebas akan bekerja pada data temporal atau data deret berkala Pembelajaran terawasi dan Klasifikasi . Artikel ini fokus pada pembelajarn terawasi, bagian dari kategori pembelajaran mesin dimana nilai target diketahui pada dataset yang tersedia. Walaupun banyak konsep juga digunakan pada analisa regresi, kita akan fokus pada klasifikasi, menyatakan label target kategorikal pada data latih dan data tes 0-1 loss dan akurasi prediksi Dalam artikel berikut, kita akan fokus pada akurasi prediksi yang didefinisikan sebagai banyaknya prediksi yang benar dibagi dengan banyaknya data dalam dataset. Kita menghitung akurasi prediksi dengan banyak prediksi yang bendar dibagi dengan banyaknya data. Atau dalam istilah formalnya, kita definiikasn akurasi prediksi ACC dengan $$ ACC=1-ERR $$ dimana eror prediksi ERR dihitung dengan nilai harapan dari 0-1 loss terhadap n data dalam dalam dataset S $$ ERR _ { S } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } L ( \\hat { y } _ { i } , y _ { i } ) $$ 0-1 loss $L(.) $ didefinisikan dengan $$ L ( \\hat { y } _ { i } , y _ { i } ) = \\left{ \\begin{array} { l l } { 0 } & { \\text { jika } \\hat { y } _ { i } = y _ { i } } \\ { 1 } & { \\text { jika } \\hat { y } _ { i } \\neq y _ { i } } \\end{array} \\right. $$ dimana y_i y_i adalah label class benar ke- i i dan \\hat y_i \\hat y_i adalah label kelas prediksi ke i i . Tujuan kita adalah untuk belajar model h h yang memiliki performansi baik secara umum. Sehingga model memaksimalkan akurasi prediksi atau sebalikan meminimalkan probabilitas, C(h) C(h) dari mebuat kesalahan prediksi: $$ C ( h ) = \\operatorname { Pr } _ { ( x , y ) \\sim D } [ h ( x ) \\neq y ] $$ Disini $ D$ adalah distribusi yang dihasilkan dari datasetn, dengan x x adalah vektor fitur data latih dengan label kelas y y Terakhir, karena artikle ini lebih mengacu kepada akurasi prediksi ( bukan error) kita definisikan fungsi Kronecker\u2019s Delta $$ \\delta ( L ( \\hat { y } _ { i } , y _ { i } ) ) = 1 - L ( \\hat { y } _ { i } , y _ { i } ) $$ sehingga $$ \\delta ( L ( \\hat { y } _ { i } , y _ { i } ) ) = 1 \\text { if } \\hat { y } _ { i } = y _ { i } $$ dan $$ \\delta ( L ( \\hat { y } _ { i } , y _ { i } ) ) = 0 \\text { if } \\hat { y } _ { i } \\neq y _ { i } $$ Bias . Pada artikel ini, istilah bias mengacu pada statistical bias ( berbeda bias dalam sistem pembelajaran mesin). Istilah secara umum bias dari estimator \\hat \\beta \\hat \\beta adalah selisih diantara nilai harapan E[\\hat \\beta] E[\\hat \\beta] dan nilai sesungguhnya dari parameter \\beta \\beta yang diestimasi $$ Bias = E [ \\hat { \\beta } ] - \\beta $$ Selanjutnya, jika $ Bias = E [ \\hat { \\beta } ] - \\beta=0$, maka $\\hat \\beta $ adalah bukan bias dari estimator \\beta \\beta . Lebih tepatnya, kita menghitung bias prediksi sebagai selisih antara akurasi prediksi yang diharapkan dari model dan akurasi prediksi yang sesungguhnya. Variance. Varian adalah seperti varian statistik dari estimator \\hat \\beta \\hat \\beta dan nilai harapan $E[\\hat \\beta] misalkan beda kuadrat dari $$ Variance =E\\left[(\\hat\\beta-E[\\hat\\beta])^2\\right] $$ Varians adalah ukuran variabilitas prediksi model jika kita mengulangi proses pembelajaran beberapa kali dengan fluktuasi kecil di set pelatihan. Semakin sensitif pembuatan model proses menuju fluktuasi ini, semakin tinggi variansnya Akhirnya, mari kita jelaskan model istilah, hipotesis, klasifikasi, algoritma pembelajaran, dan parameter: Fungsi Target Dalam modeling, kita biasanya yang kita perhatikan dalam pemodelan proses tertentu dalah kita ingin belajar atau pendektan spesifikdari fungsi yang tidak diketahui. Fungsi target f(x)=y f(x)=y adalah fungsi sesungguhnya $f(.) yang kita ingin modelkan Hypotesa Hipotesa adalah fungsi tertentu yang akan kita percara (atau harapan) adalah sama dengan fungsi sesungguhnya,fungsi target yang ingin kita modelkan. Model Dalam bidang pembelajaran mesin, istilah hypotesa dan model adalah sering digunakan secara bergantian. Dalam ilmu lain, istilah ini dapat memiliki makna berbeda beda. Hipotesa dapat menjadi educated guess. bagi ilmuwan. dan model akan menjadi manifestasi dari panduan ini untuk menguji hipotesa ini. Algoritma pembelajaran . Sekali lagi, tujuan kita adalah untuk menemukan pendekatan fungsi target dan algoritma pembelajaran adalah serangkaian instruksi yang mencoba untuk memodelkan fungsi target menggunakan data latih. Hyperparameter . Hyperparameter adalah memilih paramter dari algoritma pembelajaran. Misalkan nilai untuk menentukan kedalaman maksimum dari pohon keputusan Membandingkan Algoritma \u00b6 4.1 Pendahuluan Bab akhir dari artikel ini adalah menjelaskan beberapa pendekatan pengujiian hipotesa statistik dengan aplikasinya pada model pembelajaran mesin dan perbandingan algoritma. 4.2 Pengujian Beda Proporsi Ada beberapa macam kerangka uji hipotesa statistik yang digunakan untuk membandingkan performansi model klasifikasi, yaitu metode konvensional seperti perbedaan dari dua proporsi (disini proporsi adalah akurasi umum yang telah dihitung dari data uji)","title":"Evaluasi Model"},{"location":"Evaluasi%20Model/#evaluasi-model-memilih-model-dan-memilih-algoritma-dalam-pembelajaran-mesin","text":"Mengunakan secara benar dalam evaluasi model, memilih model dan memilih algoritma adalah penting dalam penelitian akademik pembelajaran mesin. Artikel ini membahas berbagai teknik yang digunakan untuk masing masing dari tiga tugas diatas dan membahas kelebihan dan kekurangan dari setiap teknik denga mengacu kepada studi teoritis dan empiris. Selanjutnya, rekomendasi diberikan untuk mendapatkan yang terbaik dalam penelitian dan aplikasi pembelajaran mesin. Metode yang paling umum seperti holdout methods untuk evaluasi model dan memilih model akan dibahas yang tidak direkomendasikan ketika bekerja dengan dataset yang sedikit. Kemudian berikutnya juga membahas teknik cross-validation seperit leave-one-out cross-validation dan teknik k-fold cross-validation, serta jalan tengah bias-variance untuk memilih k k juga akan dibahas dan tip praktis untuk memilih secara optimal k k didasarkan pada kejadian empiris. Berbagai tes statistik untuk membandingkan algoritma dibahas dan strategi yang berkaitan dengan berbagai perbandingan seperti test omnibus dan multiple-comparison corrections juga aakan dibahas. Dan terakhir adalah metode lain untuk memilih algoritma seperti kombias F-test 5x2 cross-validation dan nested cross-validation direkomendasikan untuk membandingkan algoritma ketika dataset sedikit","title":"Evaluasi Model, Memilih Model dan Memilih Algoritma dalam Pembelajaran Mesin"},{"location":"Evaluasi%20Model/#pengantar-prinsip-dasar-evaluasi-model-dan-istilah-istilahnya","text":"Pembelajaran Mesin telah menjadi bagian penting dalam kehidupan, khususnya peneliti dan praktisi industri. Bilamana kita menggunakan teknik pemodela untuk penelitian kita atau masalah bisnis yang kita hadapi, saya yakitn kita memiliki satu hal umum, kita ingin membuat \"good \" prediksi. Membuat suatu model yang sesuai dengan data latih kita adalah satu hal yang digunakan tetapi bagaiman kita tahu bahwa model yang telah kita buat baik untuk mengeneralisasi data yang belum kita ketahui. Artinya bahwa model yang baik yang didasarkan pada proses pembelajaran dari data latih akan baik untuk data yang akan diprediksi . Dan bagaimana kita memilih model yang baik. Mungkin algoritma pembelajaran yang berbeda dapat lebih baik untuk masalah yang kita tangani?. Mengevaluasi model tentunya bukan akhir dari suatu mesin pembelajaran. Seblum menangani data, kita ingin berencana kedepan dan menggunakan teknik yang sesuai dengan tujuan kita. Dalam paper ini, kita ingin memilih teknik ini dan kita akan melihat bagaimana dan melihat bagaiman menyesuaikan dengan gambaran garis besar dalam pembelajaran mesin. 1.1. Mengukur Performansi: Performansi secara umum dan Memilih Model Mari kita perhatikan pertanyaan sebelumnya, bagaiman kita menghitung performansi dari model pembelajaran mesin. Jawaban umum untuk pertanyaan ini mungkin sebagai berikut: Pertaa, kita masukkan data latih ke algoritma pembelajran mesin untuk belajar model. Kedua, kita prediksi label dari data tes kita.Ketiga kita hitung jumlah prediksi yang yang salah pada data tes untuk menghitung akurasi model prediksinya. Sayangnya, menghitung performansi suatu model adalah bukan hal sepele. Kita akan menyelesaikan pertanyaan sebelumnya itu dari sudut pandang yang berbeda. Mengapa kita memperhatikan segalanya? Idealnya, performansi yang dihitung dari model terhadap data yang tidak nampak-/membuat prediksi pada data baru yang menjadi masalah utama yang akan kita pecahkan dalam pembelajaran mesian atau pengembangan algoritma baru. Biasanya, pembelajaran mesin melibatkan banyak eksperimen, meskipun - misalnya, penyetelan bagian bagian internal algoritma pembelajaran, yang disebut hyperparameters. Menjalankan algoritma pembelajaran melalui dataset pelatihan dengan pengaturan hyperparameter yang berbeda akan menghasilkan model yang berbeda dan selanjutnya kita memperhatikan model tertentu dengan menentukan peringkat mereka terhadap satu sama lain. Beberapa hal yang perlu diperhatikan dalam membandingkan suatu algoritma dengan algoritma lain yaitu Kita akan menghitung performansi secara umum, performansi prediksi dari model pada data baru (data yang belum diketahui) Kita ingin meningkatkan performansi prediksi dengan mengutak-atik algoritma pembelajaran dan memilih model dengan kinerja terbaik dari ruang hipotesis yang diberikan Kami ingin mengidentifikasi algoritma pembelajaran mesin yang paling cocok untuk masalah yang dihadapi n; jadi, kami ingin membandingkan berbagai algoritma, memilih yang berkinerja terbaik serta model berkinerja terbaik dari ruang hipotesis algoritma Walaupun ketiga hal diatas secara umum sama yaitu ingin menghitung performansi suatu model, ketiganya masing masing masing memiliki pendekatan yang berbeda. Kita akan membahas berbagai medoe untuk menyelesaiakn masing masing dalam artikel ini. Jelas, kita ingin menghitung performansi kedepan dari model seakurat mungkin. Akan tetapi kita akan memperhatikan bahwa performansi bias dari model sebenarnya dalam memilih model dan memilih algoritma kola efek bias semua model sama. Jika kita urutkan berbagai model terhadap yang lain, selanjutnya kiat akan memilih model yang memiliki performansi terbaik, kita hanya perlu mengetetahui performansi relatif mereka. 1.2 Asumsi dan istilah Evaluasi model tentunya topik yang sangat kompleks. Untuk memastikan bahwa kita tidak menyebar dari intinya, kita menentukan asumsi dan selanjutnya akan kita gunakan dalam artikel ini. i.i.d . Kita asumsikan bahwa masing masing objekpada data latih adalah i.id (independent and identically distributed) yang artinya bahwa semua objek diperoleh dari distribusi probabilitas yang sama dan secara statistik saling bebas dengan yang lain. Skenario dimana data latih adalah tidak saling bebas akan bekerja pada data temporal atau data deret berkala Pembelajaran terawasi dan Klasifikasi . Artikel ini fokus pada pembelajarn terawasi, bagian dari kategori pembelajaran mesin dimana nilai target diketahui pada dataset yang tersedia. Walaupun banyak konsep juga digunakan pada analisa regresi, kita akan fokus pada klasifikasi, menyatakan label target kategorikal pada data latih dan data tes 0-1 loss dan akurasi prediksi Dalam artikel berikut, kita akan fokus pada akurasi prediksi yang didefinisikan sebagai banyaknya prediksi yang benar dibagi dengan banyaknya data dalam dataset. Kita menghitung akurasi prediksi dengan banyak prediksi yang bendar dibagi dengan banyaknya data. Atau dalam istilah formalnya, kita definiikasn akurasi prediksi ACC dengan $$ ACC=1-ERR $$ dimana eror prediksi ERR dihitung dengan nilai harapan dari 0-1 loss terhadap n data dalam dalam dataset S $$ ERR _ { S } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } L ( \\hat { y } _ { i } , y _ { i } ) $$ 0-1 loss $L(.) $ didefinisikan dengan $$ L ( \\hat { y } _ { i } , y _ { i } ) = \\left{ \\begin{array} { l l } { 0 } & { \\text { jika } \\hat { y } _ { i } = y _ { i } } \\ { 1 } & { \\text { jika } \\hat { y } _ { i } \\neq y _ { i } } \\end{array} \\right. $$ dimana y_i y_i adalah label class benar ke- i i dan \\hat y_i \\hat y_i adalah label kelas prediksi ke i i . Tujuan kita adalah untuk belajar model h h yang memiliki performansi baik secara umum. Sehingga model memaksimalkan akurasi prediksi atau sebalikan meminimalkan probabilitas, C(h) C(h) dari mebuat kesalahan prediksi: $$ C ( h ) = \\operatorname { Pr } _ { ( x , y ) \\sim D } [ h ( x ) \\neq y ] $$ Disini $ D$ adalah distribusi yang dihasilkan dari datasetn, dengan x x adalah vektor fitur data latih dengan label kelas y y Terakhir, karena artikle ini lebih mengacu kepada akurasi prediksi ( bukan error) kita definisikan fungsi Kronecker\u2019s Delta $$ \\delta ( L ( \\hat { y } _ { i } , y _ { i } ) ) = 1 - L ( \\hat { y } _ { i } , y _ { i } ) $$ sehingga $$ \\delta ( L ( \\hat { y } _ { i } , y _ { i } ) ) = 1 \\text { if } \\hat { y } _ { i } = y _ { i } $$ dan $$ \\delta ( L ( \\hat { y } _ { i } , y _ { i } ) ) = 0 \\text { if } \\hat { y } _ { i } \\neq y _ { i } $$ Bias . Pada artikel ini, istilah bias mengacu pada statistical bias ( berbeda bias dalam sistem pembelajaran mesin). Istilah secara umum bias dari estimator \\hat \\beta \\hat \\beta adalah selisih diantara nilai harapan E[\\hat \\beta] E[\\hat \\beta] dan nilai sesungguhnya dari parameter \\beta \\beta yang diestimasi $$ Bias = E [ \\hat { \\beta } ] - \\beta $$ Selanjutnya, jika $ Bias = E [ \\hat { \\beta } ] - \\beta=0$, maka $\\hat \\beta $ adalah bukan bias dari estimator \\beta \\beta . Lebih tepatnya, kita menghitung bias prediksi sebagai selisih antara akurasi prediksi yang diharapkan dari model dan akurasi prediksi yang sesungguhnya. Variance. Varian adalah seperti varian statistik dari estimator \\hat \\beta \\hat \\beta dan nilai harapan $E[\\hat \\beta] misalkan beda kuadrat dari $$ Variance =E\\left[(\\hat\\beta-E[\\hat\\beta])^2\\right] $$ Varians adalah ukuran variabilitas prediksi model jika kita mengulangi proses pembelajaran beberapa kali dengan fluktuasi kecil di set pelatihan. Semakin sensitif pembuatan model proses menuju fluktuasi ini, semakin tinggi variansnya Akhirnya, mari kita jelaskan model istilah, hipotesis, klasifikasi, algoritma pembelajaran, dan parameter: Fungsi Target Dalam modeling, kita biasanya yang kita perhatikan dalam pemodelan proses tertentu dalah kita ingin belajar atau pendektan spesifikdari fungsi yang tidak diketahui. Fungsi target f(x)=y f(x)=y adalah fungsi sesungguhnya $f(.) yang kita ingin modelkan Hypotesa Hipotesa adalah fungsi tertentu yang akan kita percara (atau harapan) adalah sama dengan fungsi sesungguhnya,fungsi target yang ingin kita modelkan. Model Dalam bidang pembelajaran mesin, istilah hypotesa dan model adalah sering digunakan secara bergantian. Dalam ilmu lain, istilah ini dapat memiliki makna berbeda beda. Hipotesa dapat menjadi educated guess. bagi ilmuwan. dan model akan menjadi manifestasi dari panduan ini untuk menguji hipotesa ini. Algoritma pembelajaran . Sekali lagi, tujuan kita adalah untuk menemukan pendekatan fungsi target dan algoritma pembelajaran adalah serangkaian instruksi yang mencoba untuk memodelkan fungsi target menggunakan data latih. Hyperparameter . Hyperparameter adalah memilih paramter dari algoritma pembelajaran. Misalkan nilai untuk menentukan kedalaman maksimum dari pohon keputusan","title":"Pengantar. Prinsip dasar Evaluasi Model  dan istilah-istilahnya"},{"location":"Evaluasi%20Model/#membandingkan-algoritma","text":"4.1 Pendahuluan Bab akhir dari artikel ini adalah menjelaskan beberapa pendekatan pengujiian hipotesa statistik dengan aplikasinya pada model pembelajaran mesin dan perbandingan algoritma. 4.2 Pengujian Beda Proporsi Ada beberapa macam kerangka uji hipotesa statistik yang digunakan untuk membandingkan performansi model klasifikasi, yaitu metode konvensional seperti perbedaan dari dua proporsi (disini proporsi adalah akurasi umum yang telah dihitung dari data uji)","title":"Membandingkan Algoritma"},{"location":"Menghitung%20akuras/","text":"Sumber : COMBINING PATTERN CLASSIFIERS Membandingkan dua classifier \u00b6 Tidak ada satu classifier terbaik. Classifier digunakan pada berbagai masalah dan dilatih dengan performansi yang berbeda beda. Studi banding biasanya didasarkan pada ekperimen menggunakan beberapa simulasi dan sekumpulan data real. Ketika berbicara tentagn rancangan ekperimen, kita dapat","title":"Menghitung akuras"},{"location":"Menghitung%20akuras/#membandingkan-dua-classifier","text":"Tidak ada satu classifier terbaik. Classifier digunakan pada berbagai masalah dan dilatih dengan performansi yang berbeda beda. Studi banding biasanya didasarkan pada ekperimen menggunakan beberapa simulasi dan sekumpulan data real. Ketika berbicara tentagn rancangan ekperimen, kita dapat","title":"Membandingkan dua classifier"},{"location":"Pengantar%20webscrape/","text":"Dalam materi ini, kita akan membahan topik \u00b6 Pengantar bidang web scraping Menjelaskan tantangan legalitas web scraping Apa itu web scraping \u00b6 Web scraping adalah proses mengektrak data dari website. Selanjutnya data yang tersedia di website dinyatakan dalam bentuk format yang dapat dengan mudah dioleh oleh mesin untuk dianalisa lebih lanjut. Misalkan data dinyatakan dalam bentuk format CSV, disimpan dalam database tertentu dan sebagainya. Data yang ada di website itu bentuknya tidak terstrukur, artinya tidak siap digunakan untuk analisis. Ada beberapa cara scrape data dari website untuk diektrak informasinya untuk digunakan. Bentuk yang paling sederhana, adalah dengan menyalin dan mempast bagian bagian tersebut dari website. TEntunya ini tidak praktis dilakukan jika banyak data yang yang akan diektrak, atau tersebar dibeberapa website. Sehingga diperlukan tool khusus dan teknik khusus yang digunakan untuk melakukan secara otomatis proses ini, dengan menetapkan webiste yang akan dijelajahi informasi apa yang akan dicari dan apakah ektraksi data berhenti diakhir halama yang ditemukan ataukah mengikuti hyperlink dan mengulangi proses secara rekursif. Proses automatis dari web scraping juga memungkinkan untuk dilakukan apakah proses akan dijalankan pada rentang waktu tertentu dan menangkap perubahan yang terjadi dari data. Teknik Webscraping membutuhkan pemahaman teknologi yang digunakan untuk menampilkan inforamasi pada web. Oleh karena itu diperlukan pemahaman tentang HTML dan Document Object Model (DOM) termasuk pemahana sintak XPath untuk memilih elemen pada website untuk Apa web scraping diperlukan \u00b6 Untuk web indexing yang digunakan oleh mesin pencari misal Google untuk menganalisa secara masal web untuk membangun indeknya. Memonitoring perubahan data e-commerse secara online untuk tujuan pemasaran. Seandainya kita telah memiliki toko penjualan sepatu dan ingin untuk melacak terus harga pesaing kita. Kita dapat mengunjungi website pesaing kita setiap hari untuk membandingkan dengan masing masing harga sepatu yang kita miliki. Akan tetapi ini akan butuh waktu banyak dan tidak layak dilakukan jika kita menjual ribuan sepatu atau dibutuhkan untuk mengecek perubahan harga lebih sering. Ini adalah pekerjaan yang tidak efisien dan efektif . Oleh karena itu kita perlu mesin atomatis dengan teknik web scraping untuk menggantikan proses manual tersebut Web scraping juga banyak digunakan oleh is also increasingly being used by scholars to create data sets for text mining projects; these might be collections of journal articles or digitised texts. The practice of data journalism , in particular, relies on the ability of investigative journalists to harvest data that is not always presented or published in a form that allows analysis. Apakah web scraping legal? \u00b6 Jika data yang ditarik (scrap) keperluan sendiri, dalam prakteknya tidak masalah. Akan tetapi jika data akan dipublikasikan lagi maka ada beberapa aspek hukum yang harus dipertimbangkan. Secara singkat bahwa legalitas tidaknya kita melakukan web scraping atau abu abunya tergantung pada tiga hal ini: Bentuk data yang akan discrap Bagaiman anda merencanakan untuk menggunakan data yang di-scrap Bagaiman anda mengektrak data dari website Nomor 1 dan 2 sangat jelas sehingga kita akan memulainya daru sini sebelum membahas nomor 3. Apa saja dari bentuk data yang ilegal untuk di scrap \u00b6 Data seperi e-commerce, personal atau data article , adalah bentuk data yang sangat terkait dengan legalitas. Dua bentuk data yang kita perlu hati-hati atau dikwatirkan legalitasnya adalah Personal Data Copyrighted Data JIka data yang anda tarik adalah bukan bentuk data dari diatas, secara umum adalah legal Bentuk data 1: Personal Data \u00b6 Personal data adalah data yang digunakan secara langsung atau tidak langsung untuk mengidentifksi individu tertentu. Bentuk dari data personal yaitu : Name Email Phone Number Address User Name IP Address Date of Birth Employment Info Bank or Credit Card Info Medical Data Biometric Data Bentuk data #2 : Copyrighted Data \u00b6 Bentuk data yang perlu hati hati untuk ditarik adalah copyrighted data. Copyrighted data data yang dimiliki oleh pribadi atau bisnis untuk mengendalikan terkait reproduksi dan kepemilikan. Seperti gambar dan lagu walaupun data dipublikasi di internet bukan berarti itu legal untuk ditarik tanpa pemberitahuan pemiliknya. Secara umum data tersebut adalah: Articles Videos Pictures Stories Music Databases Mengekstrak data berhak cipta bukanlah tindakan ilegal, jadi tindakan legal dan ilegal benar-benar tergantung pada bagaimana Anda berencana menggunakan datahak cipta tersebut setelah Anda menyalin /men-scrap data tersebut. Juga aspek database data, artinya jika menarik seluruh database dari web dan kemudian merepoduksi ulang untuk tujuan sendiri. Amerika dan EU memiliki regulasi yang berebda terkait aspek ini. Risiko melanggar hack terkait dengan masalah databasea dapat dikurangi dengan mengubah cara pengumpulan dan penggunaan data: Hanya menscrap beberapa data yang tersedia; Jangan mereplikasi struktur susunan dari database asli; Selanjutnya kita akan menjawan masalah legalitas daengan bagai anda mengektrak data dari suatu website. Is Web Scraping Itself Illegal? \u00b6 It\u2019s pretty straightforward to determine if scraping personal or copyrighted data will make your web scraping illegal because there are clear laws that set out what is legal and what is illegal. It gets a lot more tricky when it comes to the act of web scraping itself because no government has passed any law explicitly legalising or delegalising web scraping. Instead, we have to go off the verdicts of lawsuits between web scrapers and website owners. Which there are many: Craigslist vs 3Taps Ryanair vs PR Aviation Facebook vs Power Ventures HiQ vs LinkedIn To name a few. The main issue of all these cases is the question of whether the Terms of Service listed on many websites that forbid web scraping (or automatic access) are legally enforceable. Although cases have gone both ways, as of 2020, the courts are clarifying the situation of for web scrapers. The most recent of which HiQ vs LinkedIn, found that scraping data from a website doesn\u2019t violate anti-hacking laws as long as the data is public and the scraper hasn\u2019t explicitly agreed to the website\u2019s terms and conditions in advance. What this means is that so long as the data is publicly available on a website, and doesn\u2019t require the web scraper to login and explicitly accept the terms of conditions of the website, the web scraper is within their right to scrape the publically available data. So how does this affect web scrapers? If you are scraping a website then you need to ask these questions to determine if its legal or not: Is the data publicly available? If the data isn\u2019t hidden behind a login, then the website\u2019s terms and conditions aren\u2019t enforceable so you can legally scrape the public data. Do you need to create an account and login to access the data? If this is the case then you need to examine the terms and conditions you agreed to when you created the account, because by agreeing to them you made them legally enforceable. Your Own Legal Sanity Check \u00b6 So there you go, we\u2019ve discussed all the main issues that determine the legality of your web scraping. In the majority of cases we see, what companies want to scrape is perfectly legal. However, we always advise them to double-check their plans with these three simple checks: Am I scraping personal data? Am I scraping copyrighted data? Am I scraping data from behind a login? If your answers to all three of these questions is \u201cNo\u201d, then your web scraping is legal. However, if you answer \u201cYes\u201d to any of them, then you should take a step back and do a full legal review of your web scraping to ensure your not scraping the web illegally. https://www.scraperapi.com/blog/is-web-scraping-legal/ Apa perbedaan antara web scraping dan web crawling \u00b6 Web scraping adalah prose ektraksi data dari website-website. Web scraping lebih tepatnya menjelajahi suatu website untuk menyalin dan mem-paste isi dari website, sedangakan web crawling adalah terkait denga link-link yang diikuti, anda menjelajahi website dan menyimpan link link yang ada didalam website yang dijelajahi","title":"Pengantar webscrape"},{"location":"Pengantar%20webscrape/#dalam-materi-ini-kita-akan-membahan-topik","text":"Pengantar bidang web scraping Menjelaskan tantangan legalitas web scraping","title":"Dalam materi ini, kita akan membahan topik"},{"location":"Pengantar%20webscrape/#apa-itu-web-scraping","text":"Web scraping adalah proses mengektrak data dari website. Selanjutnya data yang tersedia di website dinyatakan dalam bentuk format yang dapat dengan mudah dioleh oleh mesin untuk dianalisa lebih lanjut. Misalkan data dinyatakan dalam bentuk format CSV, disimpan dalam database tertentu dan sebagainya. Data yang ada di website itu bentuknya tidak terstrukur, artinya tidak siap digunakan untuk analisis. Ada beberapa cara scrape data dari website untuk diektrak informasinya untuk digunakan. Bentuk yang paling sederhana, adalah dengan menyalin dan mempast bagian bagian tersebut dari website. TEntunya ini tidak praktis dilakukan jika banyak data yang yang akan diektrak, atau tersebar dibeberapa website. Sehingga diperlukan tool khusus dan teknik khusus yang digunakan untuk melakukan secara otomatis proses ini, dengan menetapkan webiste yang akan dijelajahi informasi apa yang akan dicari dan apakah ektraksi data berhenti diakhir halama yang ditemukan ataukah mengikuti hyperlink dan mengulangi proses secara rekursif. Proses automatis dari web scraping juga memungkinkan untuk dilakukan apakah proses akan dijalankan pada rentang waktu tertentu dan menangkap perubahan yang terjadi dari data. Teknik Webscraping membutuhkan pemahaman teknologi yang digunakan untuk menampilkan inforamasi pada web. Oleh karena itu diperlukan pemahaman tentang HTML dan Document Object Model (DOM) termasuk pemahana sintak XPath untuk memilih elemen pada website","title":"Apa itu web scraping"},{"location":"Pengantar%20webscrape/#untuk-apa-web-scraping-diperlukan","text":"Untuk web indexing yang digunakan oleh mesin pencari misal Google untuk menganalisa secara masal web untuk membangun indeknya. Memonitoring perubahan data e-commerse secara online untuk tujuan pemasaran. Seandainya kita telah memiliki toko penjualan sepatu dan ingin untuk melacak terus harga pesaing kita. Kita dapat mengunjungi website pesaing kita setiap hari untuk membandingkan dengan masing masing harga sepatu yang kita miliki. Akan tetapi ini akan butuh waktu banyak dan tidak layak dilakukan jika kita menjual ribuan sepatu atau dibutuhkan untuk mengecek perubahan harga lebih sering. Ini adalah pekerjaan yang tidak efisien dan efektif . Oleh karena itu kita perlu mesin atomatis dengan teknik web scraping untuk menggantikan proses manual tersebut Web scraping juga banyak digunakan oleh is also increasingly being used by scholars to create data sets for text mining projects; these might be collections of journal articles or digitised texts. The practice of data journalism , in particular, relies on the ability of investigative journalists to harvest data that is not always presented or published in a form that allows analysis.","title":"untuk Apa web scraping diperlukan"},{"location":"Pengantar%20webscrape/#apakah-web-scraping-legal","text":"Jika data yang ditarik (scrap) keperluan sendiri, dalam prakteknya tidak masalah. Akan tetapi jika data akan dipublikasikan lagi maka ada beberapa aspek hukum yang harus dipertimbangkan. Secara singkat bahwa legalitas tidaknya kita melakukan web scraping atau abu abunya tergantung pada tiga hal ini: Bentuk data yang akan discrap Bagaiman anda merencanakan untuk menggunakan data yang di-scrap Bagaiman anda mengektrak data dari website Nomor 1 dan 2 sangat jelas sehingga kita akan memulainya daru sini sebelum membahas nomor 3.","title":"Apakah web scraping legal?"},{"location":"Pengantar%20webscrape/#apa-saja-dari-bentuk-data-yang-ilegal-untuk-di-scrap","text":"Data seperi e-commerce, personal atau data article , adalah bentuk data yang sangat terkait dengan legalitas. Dua bentuk data yang kita perlu hati-hati atau dikwatirkan legalitasnya adalah Personal Data Copyrighted Data JIka data yang anda tarik adalah bukan bentuk data dari diatas, secara umum adalah legal","title":"Apa saja dari bentuk data yang ilegal untuk di scrap"},{"location":"Pengantar%20webscrape/#bentuk-data-1-personal-data","text":"Personal data adalah data yang digunakan secara langsung atau tidak langsung untuk mengidentifksi individu tertentu. Bentuk dari data personal yaitu : Name Email Phone Number Address User Name IP Address Date of Birth Employment Info Bank or Credit Card Info Medical Data Biometric Data","title":"Bentuk data 1: Personal Data"},{"location":"Pengantar%20webscrape/#bentuk-data-2-copyrighted-data","text":"Bentuk data yang perlu hati hati untuk ditarik adalah copyrighted data. Copyrighted data data yang dimiliki oleh pribadi atau bisnis untuk mengendalikan terkait reproduksi dan kepemilikan. Seperti gambar dan lagu walaupun data dipublikasi di internet bukan berarti itu legal untuk ditarik tanpa pemberitahuan pemiliknya. Secara umum data tersebut adalah: Articles Videos Pictures Stories Music Databases Mengekstrak data berhak cipta bukanlah tindakan ilegal, jadi tindakan legal dan ilegal benar-benar tergantung pada bagaimana Anda berencana menggunakan datahak cipta tersebut setelah Anda menyalin /men-scrap data tersebut. Juga aspek database data, artinya jika menarik seluruh database dari web dan kemudian merepoduksi ulang untuk tujuan sendiri. Amerika dan EU memiliki regulasi yang berebda terkait aspek ini. Risiko melanggar hack terkait dengan masalah databasea dapat dikurangi dengan mengubah cara pengumpulan dan penggunaan data: Hanya menscrap beberapa data yang tersedia; Jangan mereplikasi struktur susunan dari database asli; Selanjutnya kita akan menjawan masalah legalitas daengan bagai anda mengektrak data dari suatu website.","title":"Bentuk data  #2: Copyrighted Data"},{"location":"Pengantar%20webscrape/#is-web-scraping-itself-illegal","text":"It\u2019s pretty straightforward to determine if scraping personal or copyrighted data will make your web scraping illegal because there are clear laws that set out what is legal and what is illegal. It gets a lot more tricky when it comes to the act of web scraping itself because no government has passed any law explicitly legalising or delegalising web scraping. Instead, we have to go off the verdicts of lawsuits between web scrapers and website owners. Which there are many: Craigslist vs 3Taps Ryanair vs PR Aviation Facebook vs Power Ventures HiQ vs LinkedIn To name a few. The main issue of all these cases is the question of whether the Terms of Service listed on many websites that forbid web scraping (or automatic access) are legally enforceable. Although cases have gone both ways, as of 2020, the courts are clarifying the situation of for web scrapers. The most recent of which HiQ vs LinkedIn, found that scraping data from a website doesn\u2019t violate anti-hacking laws as long as the data is public and the scraper hasn\u2019t explicitly agreed to the website\u2019s terms and conditions in advance. What this means is that so long as the data is publicly available on a website, and doesn\u2019t require the web scraper to login and explicitly accept the terms of conditions of the website, the web scraper is within their right to scrape the publically available data. So how does this affect web scrapers? If you are scraping a website then you need to ask these questions to determine if its legal or not: Is the data publicly available? If the data isn\u2019t hidden behind a login, then the website\u2019s terms and conditions aren\u2019t enforceable so you can legally scrape the public data. Do you need to create an account and login to access the data? If this is the case then you need to examine the terms and conditions you agreed to when you created the account, because by agreeing to them you made them legally enforceable.","title":"Is Web Scraping Itself Illegal?"},{"location":"Pengantar%20webscrape/#your-own-legal-sanity-check","text":"So there you go, we\u2019ve discussed all the main issues that determine the legality of your web scraping. In the majority of cases we see, what companies want to scrape is perfectly legal. However, we always advise them to double-check their plans with these three simple checks: Am I scraping personal data? Am I scraping copyrighted data? Am I scraping data from behind a login? If your answers to all three of these questions is \u201cNo\u201d, then your web scraping is legal. However, if you answer \u201cYes\u201d to any of them, then you should take a step back and do a full legal review of your web scraping to ensure your not scraping the web illegally. https://www.scraperapi.com/blog/is-web-scraping-legal/","title":"Your Own Legal Sanity Check"},{"location":"Pengantar%20webscrape/#apa-perbedaan-antara-web-scraping-dan-web-crawling","text":"Web scraping adalah prose ektraksi data dari website-website. Web scraping lebih tepatnya menjelajahi suatu website untuk menyalin dan mem-paste isi dari website, sedangakan web crawling adalah terkait denga link-link yang diikuti, anda menjelajahi website dan menyimpan link link yang ada didalam website yang dijelajahi","title":"Apa perbedaan antara web scraping dan web crawling"},{"location":"Pengantar/","text":"Apa itu Web Mining \u00b6 Web mining digunakan untuk menemukan informasi yang berguna atau pengetahuan dari web hyperlink structure, page content dan usage data. Walaupun web mining menggunakan teknik data mining seperti yang telah disebutkan tetapi tidak murni seluruhnya karena heterogen dan semi terstruktur dan tidak terstrukturnya data web. Berdasarkan tugas pokok yang dilakukan dalam proses menambang, web mining dapat kelompokkan menjadi tiga bentuk, yaitu Web Structure Mining, Web Content Mining, dan Web Usage Mining Web struktur Mining : Web structure mining adalah tugas untuk menemukan pengetahuan yang bermanfaat dari hyperlink (atau link singkatnya), yang menyatakan struktur dari dari web. Misalkan dari link-link kita dapat menemukan web page penting, yang menjadi teknologi utama yang digunakan dalam mesin pencarian. Kita dapat juga menemukan komunitas dalam media sosial. Datamining biasa tidak melakukan tugas tersebut, karena tidak ada struktur link dalam tabel tradisional. Web Content Mining. Web content mining adalah mengektrak atau menambang informasi yang berguna atau pengetahuan dari isi halam web. Misalkan kita dapat secara otomatis mengklasifikasikan dan mengelompokkan halam wen sesuai deng topiknya. Tugas ini sama dengan data mining biasa. Akan tetapi kita juga dapat menemukan pla dalam web site untuk mengektrak data yang berguna seperti deskripsi produk, postingan forum, dan lain sebagainya untuk banyak tujuan. Selanjutnya kita dapa menambang ulasan dan postingan dari forum untuk menemukan opini konsumen. Disin tidak ada pada data mining tradisional. Contoh sederhana dari web content mining adalah : **Analisa Sentimen terhadap review produk ** Analisa Sentimen dapat dilakukan terhadap ulasan ulasan dari produk produk Amazon yang di-scrap. Suatu penelitian membantu mengidentifikasi emosi pengguna terhadap produk tertentu. INi dapat membantu penjual dan calon pembeli lain yang tertarik membeli dalam memahami penilaian secara umu terhadap suatu produk tertentu. Mengoptimalkan penjualan dropshipping Dropshipping adalah bentuk bisnis yang memungkinkan perusahaan tertentu untuk bekerja tanpa inventory atau depository untuk menyimpan produknya . Anda dapat menggunakan web web scraping untuk mendapatkan harga produk opini user dan pemahaman terhadap kebutuhan konsumen, dan mengikuti arah perkembangan produk. Monitoring reputasi e-commerce Dengan adanya web mining dapat membantu mengektraksi review data sehingga dapat menjadi masukan untuk mengukur sentiman pengguna/nasabah terhadap suatu organisasi Web Usage Mining. Web Usage Mining adalah menemukan pola akses user dari log web yang disimpan, yang merekam setiap klik yang dilakukan oleh setiap user. Web Usage Mining menggunakan banyak algoritma algoritma datamiing. Salah satu masalah utama dalam Web Usage Mining adalah preprocessing dari data clikstream dalam data usage log agar menghasilkan data yang benar untuk ditambang. Dalam kuliah ini kita lebih menekannkan pada dua tugas yaitu web content mining dan web struktur mining. Apa itu web scraping \u00b6 Web scraping adalah proses mengektrak data dari website. Selanjutnya data yang tersedia di website dinyatakan dalam bentuk format yang dapat dengan mudah dioleh oleh mesin untuk dianalisa lebih lanjut. Misalkan data dinyatakan dalam bentuk format CSV, disimpan dalam database tertentu dan sebagainya. Data yang ada di website itu bentuknya tidak terstrukur, artinya tidak siap digunakan untuk analisis. Ada beberapa cara scrape data dari website untuk diektrak informasinya untuk digunakan. Bentuk yang paling sederhana, adalah dengan menyalin dan mempast bagian bagian tersebut dari website. TEntunya ini tidak praktis dilakukan jika banyak data yang yang akan diektrak, atau tersebar dibeberapa website. Sehingga diperlukan tool khusus dan teknik khusus yang digunakan untuk melakukan secara otomatis proses ini, dengan menetapkan webiste yang akan dijelajahi informasi apa yang akan dicari dan apakah ektraksi data berhenti diakhir halama yang ditemukan ataukah mengikuti hyperlink dan mengulangi proses secara rekursif. Proses automatis dari web scraping juga memungkinkan untuk dilakukan apakah proses akan dijalankan pada rentang waktu tertentu dan menangkap perubahan yang terjadi dari data. Teknik Webscraping membutuhkan pemahaman teknologi yang digunakan untuk menampilkan inforamasi pada web. Oleh karena itu diperlukan pemahaman tentang HTML dan Document Object Model (DOM) termasuk pemahana sintak XPath untuk memilih elemen pada website untuk Apa web scraping diperlukan \u00b6 Untuk web indexing yang digunakan oleh mesin pencari misal Google untuk menganalisa secara masal web untuk membangun indeknya. Memonitoring perubahan data e-commerse secara online untuk tujuan pemasaran. Seandainya kita telah memiliki toko penjualan sepatu dan ingin untuk melacak terus harga pesaing kita. Kita dapat mengunjungi website pesaing kita setiap hari untuk membandingkan dengan masing masing harga sepatu yang kita miliki. Akan tetapi ini akan butuh waktu banyak dan tidak layak dilakukan jika kita menjual ribuan sepatu atau dibutuhkan untuk mengecek perubahan harga lebih sering. Ini adalah pekerjaan yang tidak efisien dan efektif . Oleh karena itu kita perlu mesin atomatis dengan teknik web scraping untuk menggantikan proses manual tersebut Web scraping juga banyak digunakan oleh is also increasingly being used by scholars to create data sets for text mining projects; these might be collections of journal articles or digitised texts. The practice of data journalism , in particular, relies on the ability of investigative journalists to harvest data that is not always presented or published in a form that allows analysis. Apakah web scraping legal? \u00b6 Jika data yang ditarik (scrap) keperluan sendiri, dalam prakteknya tidak masalah. Akan tetapi jika data akan dipublikasikan lagi maka ada beberapa aspek hukum yang harus dipertimbangkan. Secara singkat bahwa legalitas tidaknya kita melakukan web scraping atau abu abunya tergantung pada tiga hal ini: Bentuk data yang akan discrap Bagaiman anda merencanakan untuk menggunakan data yang di-scrap Bagaiman anda mengektrak data dari website Nomor 1 dan 2 sangat jelas sehingga kita akan memulainya daru sini sebelum membahas nomor 3. Apa saja dari bentuk data yang ilegal untuk di scrap \u00b6 Data seperi e-commerce, personal atau data article , adalah bentuk data yang sangat terkait dengan legalitas. Dua bentuk data yang kita perlu hati-hati atau dikwatirkan legalitasnya adalah Personal Data Copyrighted Data JIka data yang anda tarik adalah bukan bentuk data dari diatas, secara umum adalah legal Bentuk data 1: Personal Data \u00b6 Personal data adalah data yang digunakan secara langsung atau tidak langsung untuk mengidentifksi individu tertentu. Bentuk dari data personal yaitu : Name Email Phone Number Address User Name IP Address Date of Birth Employment Info Bank or Credit Card Info Medical Data Biometric Data Bentuk data #2 : Copyrighted Data \u00b6 Bentuk data yang perlu hati hati untuk ditarik adalah copyrighted data. Copyrighted data data yang dimiliki oleh pribadi atau bisnis untuk mengendalikan terkait reproduksi dan kepemilikan. Seperti gambar dan lagu walaupun data dipublikasi di internet bukan berarti itu legal untuk ditarik tanpa pemberitahuan pemiliknya. Secara umum data tersebut adalah: Articles Videos Pictures Stories Music Databases Mengekstrak data berhak cipta bukanlah tindakan ilegal, jadi tindakan legal dan ilegal benar-benar tergantung pada bagaimana Anda berencana menggunakan data hak cipta tersebut setelah Anda menyalin /men-scrap data tersebut. Juga aspek database data, artinya jika menarik seluruh database dari web dan kemudian merepoduksi ulang untuk tujuan sendiri. Amerika dan EU memiliki regulasi yang berbeda terkait aspek ini. Membangun Scraping Website . \u00b6 Untuk melakukan tahapan penarikan data dari suatu halaman halaman web maka yang perlu dipehatikan adalah **Menganalisa ** strutktur HTML dari laman web Scraping adalah tentang menemukan pola dalam laman- laman web dan mengektraksi isi dari laman tersebut. Sebelum mulai untuk menulis tool scrapyng (scraper), kita perlu untuk memahami struktur HTML dari web page yang akan ditarik datanya dan mengidentifikasi pola didalamnya. Pola dapat terkait dengan penggunaan classes, id, and elemen elemen lain HTML. **Membuat Scrapy parser dengan python ** Setelah menganlisa struktur dari halaman web yang akan ditarik datanya, kita lakukan implementasi dengan menulis code untuk menghasilkan scrapy parser . Scrapy parser bertanggung jawab untuk menjelajahi web yang akan ditarik datanya dan mengektrak informasi sesuai dengan aturan yang dibuat Mengumpulkan dan menyimpan informasi Parser dapat menyimpan hasilnya sesuai dengan format yang anda inginkan misalkan csv atau json. Ini adalah hasil akhir dari data yang telah dikumpulkan 1.Analisa Struktur HTML Halaman Website \u00b6 XPath (kepanjangan dari XML Path Language) adalah expression language yang digunakan untuk menetapkanbagian bagian bagian bagian dari dokumen XML . XPath digunakan dalam perangkat lunak dan bahasa yang digunakan untuk manipulasi dokumen XML, seperti XSLT, XQuery atau alat alat web scraping XPath dapat juga digunakan dalam struktur yang sama seperti XML, misalkan html Markup Languages \u00b6 XML dan HTML adalah markup languages . Artinya keduanya menggunakan sekumpulan tags atau rule untuk menyusun dan menyedian informasi yang ada didalamnya. Struktur ini membantu untuk secara otomatis untuk pemrosesan , pengeditan, pemformatan dan penampilan dan pencetakan informasi tersebut. Dokumen XML menyimpan data dalam format plain teks. Ini menyediakan perangkat lunan dan perangkat kerans dengan cara bebas untuk menyimpan, mentransformasi, dan menshare data. Format XML adalah format terbuka. Anda dapat membuka dokumen XML dalam editor teks dan data yang ada didalamnya dapat disajikan. Ini memungkinkan pertukran sistem yang tidak kompatibel dan mudah untuk mengkonversi suatu data. Dokumen XML memilki aturan-aturan dasar sebagai berikut: Dokumen XML disusun menggunakan nodes , yaitu terdiri dari node node elemen, node node atribute dan node node teks Node node elemen XML harus harus dibuka dan ditutup tag, misal. tag pembuka <catfood> dan tag penutup </catfood> Tag XML adalah sensitif e, misal. <catfood> tidak sama dengan <catFood> Elemen XML harus bertingkat dengan benar: <catfood> <manufacturer> Purina </manufacturer> <address> 12 Cat Way, Boise, Idaho, 21341 </address> <date> 2019-10-01 </date> </catfood> Node node teks (data) isinya didalam tag pembuka dan tag penutup Node note atribut XML berisi nilai-nilai yang harus di quoted, misal. <catfood type=\"basic\"></catfood> XPath selalu mengasumsikan data terstruktur ( structured data). \u00b6 Now let\u2019s start using XPath. Menelusuri pohon node HTML menggunakan XPath \u00b6 Cara yang paling umum untuk merepresentasikan struktur dari dokumen XML atau HTML adalah dengan pohon node (node tree): Dalam sebuah dokumen HTML , segala sesuatunya adalah node: Seluruh dokumen adalah node dokumen Setiap elemen elemen HTML adalah node elemen Teks didalam elemen HTML adalah node-node teks Node-node dalam suatu pohon memiliki hubungan hirarki dengan yang lain. Kita menggunakan istilah parent , child dan sibling untuk menjelaskan hubungan ini: Dalam pohon node, node paling atas disebut dengan root (atau root node ) Setiap node memiliki tetap satu parent , kecuali root (yang tidak memiliki parent) Sutau node dapat memilik satu, beberapa atau tidak sama sekali children *Sibling adalah node-node dengan parent sama Serangkaian hubungan antra node ke node disebut dengan path Path-path dalam XPath didefinisikan dengan menggunakan slash ( / ) untuk memisahkan langkah/tahapan dalam rangkaian keterkaitan node, seperti URL-URLS atau direktori Unix Dalam XPath, semua ekspresi didasarkan pada context node . Context node adalah node path dari mana dimulai. Default context adalah node root , yang dinyatakan dengan slash tunggal (/), seperti dalam contoh diatas. Yang paling banyak digunakan untuk ekpresi patha adalah sebagai berikut: Expression Description nodename Memilih semua node dengan nama \u201cnodename\u201d / Garis miring tunggal awal menunjukkan pemilihan dari simpul akar, garis miring berikutnya menunjukkan pemilihan simpul anak dari simpul saat ini // Pilih node anak langsung dan tidak langsung dalam dokumen dari node saat ini - ini memberi kita kemampuan untuk \"skip levels\" . Piiih context node saat ini .. Pilih parent dari context node @ Pilih atribut dari context node [@attribute = 'value'] Pilihan node dengan nilai atribut tertentu text() Pilih isi teks dari node | Rantai ekspresi dan membawa kembali hasil dari ekspresi mana pun Sintaks XPath lanjutan \u00b6 Operators \u00b6 Operator digunakan untuk membandingkan node-node. Terdaoat operator matematika,operator boolean. Operators dapat memberikan nilai boolean (benar /salah) sebagai hasil. Disini beberap yang banyak digunakan : Operator Penjelasan = Membandingkan kesamaan, dapat digunakan untuk nilai numerik maupun teks != Dignuanak untuk membandikanketidak samaan >, >= Lebih besar, lebih besar dari atau sama dengan <, <= Lebih dari, lebih dari atau sama dengan or Boolean atau (or) and Boolean dan (and) not Boolean bukan (not) Contoh \u00b6 Ekspresi Path Hasil Ekspresi html/body/div/h3/ @id =\u2019exercises-2\u2019 Apakah exercise 2 ada? html/body/div/h3/ @id !=\u2019exercises-4\u2019 Apakah exercise 4 tidak ada? //h1/ @id =\u2019references\u2019 or @id =\u2019introduction\u2019 Apakah terdapat terdapat references h1 atau introduction? Predikat \u00b6 Predikat digunakan untuk menemukan node tertentu atau node yang berisi nilai tertentu Predikat selalu diletakkan dalam kurung siku, dimaksudkan untuk memberikan informasi pemfilteran tambahan untuk mengembalikan node. Anda dapat memfilter pada sebuah node dengan menggunakan operator atau fungsi Examples \u00b6 Operator Explanation [1] Pilih node pertama [last()] Pilih node yang terakhir [last()-1] Pilih node terakhir kedua [position()<3] Pilih dua node pertama, perhatikan posisi pertama dimulai dari 1, bukan = [@lang] Pilih node yang memiliki atribut \u2018lang\u2019 [@lang='en'] Pilih semua node yang memiliki atribute dengan nilai atribute \u201cen\u201d [price>15.00] Pilih semua node yang memiliki node price yang lebih besar dari 15.00 Pencarian dalam teks \u00b6 XPath dapat melakukan pencarian dalam teks mengunakan fungsi. Perhatikan: pencarian dalatem teks adalah case-sensitive! Path Expression Result //author[contains(.,\"Matt\")] Cocokkan pada semua node author, dalam node sekarang yang berisi Matt (case-sensitive) //author[starts-with(.,\"G\")] Cocokkan pada semua node author, dalam node sekarang yang dimulai dengan G (case-sensitive) //author[ends-with(.,\"w\")] Cocokkan pada semua dalam node sekarang yang yang berakhiran dengan w (case-sensitive) Menampilkan source dari suatu page \u00b6 Untuk menemukan xpath element halaman website maka kita dapat membuka source page halama tersebut. Yaitu dengan cara klik kanan pada halaman Memetakan suatu webpage denganXPath menggunakan console suatu browser \u00b6 KIta akan menggunakan kode HTML yang ada pada website portal tugas akhir mahasiswa sebagai contoh. Biasanya, setiap browser web dapat menampilkan sumber kode HTML dengan fungsinya tersendiri dari browser tersebut. Contoh diatas adalah menggunakan browser firefox dengan melakukan klik kanan pada pointer akan menampilakn submenu salah satunya View Page Source. Kemudian klik menu tersebut akand dapatkan source sumbernya sebagai berikut. <!DOCTYPE html> < html > < head > < meta charset = \"utf-8\" /> < title > Portal Tugas Akhir Univ. Trunojoyo </ title > < link rel = \"stylesheet\" href = \"https://pta.trunojoyo.ac.id/css/reset.css\" type = \"text/css\" /> < link rel = \"stylesheet\" href = \"https://pta.trunojoyo.ac.id/css/style.css\" type = \"text/css\" /> < link rel = \"stylesheet\" href = \"https://pta.trunojoyo.ac.id/css/960_12_col.css\" type = \"text/css\" /> < link rel = \"stylesheet\" media = \"screen\" href = \"https://pta.trunojoyo.ac.id/css/smoothness/jquery-ui-1.8.18.custom.css\" /> <!--[if lte IE 8]> <link rel=\"stylesheet\" type=\"text/css\" href=\"css/ie.css\" /> <![endif]--> <!-- favicon --> < link rel = \"shortcut icon\" type = \"image/x-icon\" href = \"https://pta.trunojoyo.ac.id/images/favicon.ico\" /> (...) </ body > </ html > Kita dapat melihat dari source code bahwa judul dari halama ini dalam elemen title yang ada dalam elemen head , yang ada didalam elemen html yang berisi seluruh isi dari page Sehingga jika kita ingin memberi tahu web scraper untuk mencari judul dari halaman ini, kita menggunakan informasi ini untuk menentukan path yang diperlukan untuk menjelajahi isi HTML dari page untuk menemukan element title . XPath memungkinkan untuk melakukan hal tersebut. Kita dapat menjalankan query XPath langsung pada browser dengan JavaScript console yang ada didalamnya. Menampilkan console dalam browser \u00b6 Pad Firefox, gunakan menut Tools > Web Developer > Web Console . PadaChrome, gunakan menut View > Developer > JavaScript Console Disini bagaimana console tampak dalam browser Firefox : Untuk saat ini, jangan khawatir terlalu banyak pesan error manakalan anda meliha dalam console ketika anda membukanya. Console akan dengan muncul prompt dengan karakter > (atau >> dalam Firefox) menunggu anda untuk mengetik perintah . Sintaks untuk menjalankan query XPath dalam JavaScript console adalah $x(\"XPATH_QUERY\") , misalkan: $x(\"/html/head/title/text()\") Ini akan menghasilkan sesuatu seperti ini <- Array [ #text \"Portal Tugas Akhir Univ. Trunojoyo\" ] Menggunkan extension firefox xPath Finder \u00b6 Untuk menentukan cpath dari element dari halam website dengan mudah dilakukan menggunakan xPath Finder. Jika belum ada ektension firefox xPath Finder ,silahkan install dulu extension tersebut. Penggunaannya adalah dengan cara klik pada icon extension curson akan berubah menjadi crosshair, kemudian pilih elemen yang diinginkan dengan kursor diarahkan pada elemen tersebut ( elemen akan berubah warna ). Klik elemen tersebut sehingga akan tampil xPath dari elemen tersebut di bawah kiri dari halaman In [ 3 ] : fetch ( 'https://pta.trunojoyo.ac.id/' ) 2020 -09-25 04 :32:42 [ scrapy.core.engine ] DEBUG: Crawled ( 200 ) <GET https://pta.t runojoyo.ac.id/> ( referer: None ) In [ 4 ] : response.xpath ( \"/html/body/div[2]/div[1]/div[2]/ul/li[1]/div[1]/a\" ) Out [ 4 ] : [ <Selector xpath = '/html/body/div[2]/div[1]/div[2]/ul/li[1]/div[1]/a' dat a = '<a class=\"title\" href=\"#\">Analisis Wa...' > ] Dari hasil scrap diatas ddapat diperbaiki ouputnya dengan perintah sebagai berikut In [ 5 ] : response.xpath ( \"//item/title/text()\" ) .extract_first () In [ 6 ] : response.xpath ( \"/html/body/div[2]/div[1]/div[2]/ul/li[1]/div[1]/a/text( ...: )\" ) .extract_first () Out [ 6 ] : 'Analisis Wacana Media Online Detik.com dalam Memberitakan Peristiwa Ker usuhan Mahasiswa Papua di Surabaya' Mari kita bahas query XPath yang digunakan pada contoh diatas dengan XPath /html/head/title/text() . Yang pertama / menyatakan root dari suatu dokumen. Dengan query tersebut , kita memberit tahu browser untuk / Start at the root of the document\u2026 html/ \u2026 mengarahkan ke node html \u2026 head/ \u2026 kemudaian ke node head yang ada didalamnya\u2026 title/ \u2026 kemudaian ke node title yang ada didalamnya\u2026 text() dan pilih node text yang terdapat dalam elemen itu Dengan menggunakan sintaks ini , XPath kemudian memungkinkan kita untuk menentukan path yang tepat pada suatu node. Memilih judul tugas akhir \u00b6 $x ( \"/html/body/div[2]/div[1]/div[2]/ul/li[1]/div[1]/a\" ) Menggunakan Scraper Chrome extension untuk menavigasi XPath \u00b6 Ini adalah alat yang memudahkan untuk mengimplementasikan XPath. Alat ini adalah tambahan dari browser Chrome untuk melakukan scrap data dari suatu website. Jika kita ingin menggunakan tool ini maka perlu ditambahkan pada browser Chrome kita seperti biasa kita menambahkan extension Chrome secara umum. Setelah kita install extension ini misalkan kita sedang menjelajahi pta.trunojoyo.ac.id, pada saat klik kanan pada website tersebut maka akan muncul menu Dan bila kita mengklik menu scraper similar maka akan memunculkan XPath : //div[2]/div[1]/div[2]/ul/li dengan data yang ditarik dari halam website tersebut. Pada view di samping Xpath judul, penulis, dosen pembimbing dan abstrak ditampilkan dalam satu kolom dengan nama kolom text. Untuk memperbaiki hasil tampilan ini, supaya menjadi field field yang sesuai, maka diperlukan perubahan dengan menambahkan nama kolom (field) dan memecah XPath yang sesuai. Berikut perubahan yang telah dilakukan 2.**Membuat Scrapy parser dengan Python ** (TUGAS) \u00b6 Persiapan \u00b6 instalasi alat alat yaitu Scrapy library pip install scrapy Desain secara umum scraping website \u00b6 Source Pembuatan Proyek \u00b6 Proyek 1. Menarik data harga dari website a. Membuat proyek bernama mytugaswebmining scrapy startproject mytugaswebmining Akan terbentuk struktur folder shopee seperti berikut \\---mytugaswebmining | scrapy.cfg | \\---mytugaswebmining | items.py | middlewares.py | pipelines.py | settings.py | __init__.py | \\---spiders __init__.py Scrapy merupakan framewerok aplikasi yang memungkinkan suatu suatu proyek sesuai dengan gaya pemrograman Object Oriented untuk mendefinisikan item-item dan spider untuk keseluruhan aplikasi. Struktur proyek dari scrapy yang telah dibuat diatas dengan masing masing sebagai berikut scrapy.cfg : ini adalah file konfigurasi proyek yang berisi modul pengaturan untuk proyek dan juga dengan informasi pengimplementasiannya. test_project : Ini adalah direktori aplikasi dengan banyak macam file yang benar bena bertanggung jawab untuk menjalankan dan menarik data dari urls-url web. items.py : item-item yang akan ditarik oleh scraper, ini juga bekerja seperti Python dicts . Karena kita dapat menggunakan plain Python dicts dengan Scrapy, Items menyediakan proteksi tambahan terhadap field-field yang tidak dideklarasikan. Hal itu dideklarasikan dengan pembuatan scrapy.Item class dan mendefinisika atribut atributnya sebagai scrapy.Field . pipelines.py : Setelah item di scrap oleh spider, item itu dikirim ke Item Pipeline yang memprosesnya dengan menggunakan beberapa komponen yang dijalankan secara berurutan. Masing masing item pipeline component adalah kelas Python yang harus mengimplementasikan metode yang disebut dengan process_item untuk memproses item yang di- scrap . Proses dilakukan terhadap item dengan memutuskan item harus diteruskan atau didrop dan tidak dinajutkan proses berikutnya. settings.py : Ini memungkinkan kita untuk mengkustom semua karakteristi komponen-komponen Scrapy termasuk core, extensions, pipelines dan spiders itu sediri. spiders : Spiders adalah direktori yang berisi semua spiders/crawlers seperti kelas-kelas Python . Ketika kita menjalankan atau meng-crawler spider maka scrapy mencari didalam di direktori ini dan mencoba untuk menemukan spider dengan namanya yang telah ditetapkan oleh pengguna. Spider mendefinisikan bagaimana site tertentu atau sekumpulan site, termasuk bagaimana melakukan crawl dan bagaimana mengektrak data dari page-pagenya. Artinya, Spiders adalah tempat dimana kita mendefinisikan tiga atribut utam yaitu start_urls yang meberi tahu URLs mana yang akan ditarik, , allowed_domains yaitu mendefinisikan domain mana saja yang boleh parse adalah suatu metode yang dipanggil ketika ada respon yang berasal dari lodged requests . Atribut ini adalah penting karena ini inti dari definisi Spider Referensi \u00b6 Zaki, Mohammed J., and Wagner Meira. Data mining and analysis: fundamental concepts and algorithms. Cambridge University Press, 2014 \u21a9 https://blog.datahut.co/scraping-amazon-reviews-python-scrapy/ \u21a9","title":"Materi I"},{"location":"Pengantar/#apa-itu-web-mining","text":"Web mining digunakan untuk menemukan informasi yang berguna atau pengetahuan dari web hyperlink structure, page content dan usage data. Walaupun web mining menggunakan teknik data mining seperti yang telah disebutkan tetapi tidak murni seluruhnya karena heterogen dan semi terstruktur dan tidak terstrukturnya data web. Berdasarkan tugas pokok yang dilakukan dalam proses menambang, web mining dapat kelompokkan menjadi tiga bentuk, yaitu Web Structure Mining, Web Content Mining, dan Web Usage Mining Web struktur Mining : Web structure mining adalah tugas untuk menemukan pengetahuan yang bermanfaat dari hyperlink (atau link singkatnya), yang menyatakan struktur dari dari web. Misalkan dari link-link kita dapat menemukan web page penting, yang menjadi teknologi utama yang digunakan dalam mesin pencarian. Kita dapat juga menemukan komunitas dalam media sosial. Datamining biasa tidak melakukan tugas tersebut, karena tidak ada struktur link dalam tabel tradisional. Web Content Mining. Web content mining adalah mengektrak atau menambang informasi yang berguna atau pengetahuan dari isi halam web. Misalkan kita dapat secara otomatis mengklasifikasikan dan mengelompokkan halam wen sesuai deng topiknya. Tugas ini sama dengan data mining biasa. Akan tetapi kita juga dapat menemukan pla dalam web site untuk mengektrak data yang berguna seperti deskripsi produk, postingan forum, dan lain sebagainya untuk banyak tujuan. Selanjutnya kita dapa menambang ulasan dan postingan dari forum untuk menemukan opini konsumen. Disin tidak ada pada data mining tradisional. Contoh sederhana dari web content mining adalah : **Analisa Sentimen terhadap review produk ** Analisa Sentimen dapat dilakukan terhadap ulasan ulasan dari produk produk Amazon yang di-scrap. Suatu penelitian membantu mengidentifikasi emosi pengguna terhadap produk tertentu. INi dapat membantu penjual dan calon pembeli lain yang tertarik membeli dalam memahami penilaian secara umu terhadap suatu produk tertentu. Mengoptimalkan penjualan dropshipping Dropshipping adalah bentuk bisnis yang memungkinkan perusahaan tertentu untuk bekerja tanpa inventory atau depository untuk menyimpan produknya . Anda dapat menggunakan web web scraping untuk mendapatkan harga produk opini user dan pemahaman terhadap kebutuhan konsumen, dan mengikuti arah perkembangan produk. Monitoring reputasi e-commerce Dengan adanya web mining dapat membantu mengektraksi review data sehingga dapat menjadi masukan untuk mengukur sentiman pengguna/nasabah terhadap suatu organisasi Web Usage Mining. Web Usage Mining adalah menemukan pola akses user dari log web yang disimpan, yang merekam setiap klik yang dilakukan oleh setiap user. Web Usage Mining menggunakan banyak algoritma algoritma datamiing. Salah satu masalah utama dalam Web Usage Mining adalah preprocessing dari data clikstream dalam data usage log agar menghasilkan data yang benar untuk ditambang. Dalam kuliah ini kita lebih menekannkan pada dua tugas yaitu web content mining dan web struktur mining.","title":"Apa itu Web Mining"},{"location":"Pengantar/#apa-itu-web-scraping","text":"Web scraping adalah proses mengektrak data dari website. Selanjutnya data yang tersedia di website dinyatakan dalam bentuk format yang dapat dengan mudah dioleh oleh mesin untuk dianalisa lebih lanjut. Misalkan data dinyatakan dalam bentuk format CSV, disimpan dalam database tertentu dan sebagainya. Data yang ada di website itu bentuknya tidak terstrukur, artinya tidak siap digunakan untuk analisis. Ada beberapa cara scrape data dari website untuk diektrak informasinya untuk digunakan. Bentuk yang paling sederhana, adalah dengan menyalin dan mempast bagian bagian tersebut dari website. TEntunya ini tidak praktis dilakukan jika banyak data yang yang akan diektrak, atau tersebar dibeberapa website. Sehingga diperlukan tool khusus dan teknik khusus yang digunakan untuk melakukan secara otomatis proses ini, dengan menetapkan webiste yang akan dijelajahi informasi apa yang akan dicari dan apakah ektraksi data berhenti diakhir halama yang ditemukan ataukah mengikuti hyperlink dan mengulangi proses secara rekursif. Proses automatis dari web scraping juga memungkinkan untuk dilakukan apakah proses akan dijalankan pada rentang waktu tertentu dan menangkap perubahan yang terjadi dari data. Teknik Webscraping membutuhkan pemahaman teknologi yang digunakan untuk menampilkan inforamasi pada web. Oleh karena itu diperlukan pemahaman tentang HTML dan Document Object Model (DOM) termasuk pemahana sintak XPath untuk memilih elemen pada website","title":"Apa itu web scraping"},{"location":"Pengantar/#untuk-apa-web-scraping-diperlukan","text":"Untuk web indexing yang digunakan oleh mesin pencari misal Google untuk menganalisa secara masal web untuk membangun indeknya. Memonitoring perubahan data e-commerse secara online untuk tujuan pemasaran. Seandainya kita telah memiliki toko penjualan sepatu dan ingin untuk melacak terus harga pesaing kita. Kita dapat mengunjungi website pesaing kita setiap hari untuk membandingkan dengan masing masing harga sepatu yang kita miliki. Akan tetapi ini akan butuh waktu banyak dan tidak layak dilakukan jika kita menjual ribuan sepatu atau dibutuhkan untuk mengecek perubahan harga lebih sering. Ini adalah pekerjaan yang tidak efisien dan efektif . Oleh karena itu kita perlu mesin atomatis dengan teknik web scraping untuk menggantikan proses manual tersebut Web scraping juga banyak digunakan oleh is also increasingly being used by scholars to create data sets for text mining projects; these might be collections of journal articles or digitised texts. The practice of data journalism , in particular, relies on the ability of investigative journalists to harvest data that is not always presented or published in a form that allows analysis.","title":"untuk Apa web scraping diperlukan"},{"location":"Pengantar/#apakah-web-scraping-legal","text":"Jika data yang ditarik (scrap) keperluan sendiri, dalam prakteknya tidak masalah. Akan tetapi jika data akan dipublikasikan lagi maka ada beberapa aspek hukum yang harus dipertimbangkan. Secara singkat bahwa legalitas tidaknya kita melakukan web scraping atau abu abunya tergantung pada tiga hal ini: Bentuk data yang akan discrap Bagaiman anda merencanakan untuk menggunakan data yang di-scrap Bagaiman anda mengektrak data dari website Nomor 1 dan 2 sangat jelas sehingga kita akan memulainya daru sini sebelum membahas nomor 3.","title":"Apakah web scraping legal?"},{"location":"Pengantar/#apa-saja-dari-bentuk-data-yang-ilegal-untuk-di-scrap","text":"Data seperi e-commerce, personal atau data article , adalah bentuk data yang sangat terkait dengan legalitas. Dua bentuk data yang kita perlu hati-hati atau dikwatirkan legalitasnya adalah Personal Data Copyrighted Data JIka data yang anda tarik adalah bukan bentuk data dari diatas, secara umum adalah legal","title":"Apa saja dari bentuk data yang ilegal untuk di scrap"},{"location":"Pengantar/#bentuk-data-1-personal-data","text":"Personal data adalah data yang digunakan secara langsung atau tidak langsung untuk mengidentifksi individu tertentu. Bentuk dari data personal yaitu : Name Email Phone Number Address User Name IP Address Date of Birth Employment Info Bank or Credit Card Info Medical Data Biometric Data","title":"Bentuk data 1: Personal Data"},{"location":"Pengantar/#bentuk-data-2-copyrighted-data","text":"Bentuk data yang perlu hati hati untuk ditarik adalah copyrighted data. Copyrighted data data yang dimiliki oleh pribadi atau bisnis untuk mengendalikan terkait reproduksi dan kepemilikan. Seperti gambar dan lagu walaupun data dipublikasi di internet bukan berarti itu legal untuk ditarik tanpa pemberitahuan pemiliknya. Secara umum data tersebut adalah: Articles Videos Pictures Stories Music Databases Mengekstrak data berhak cipta bukanlah tindakan ilegal, jadi tindakan legal dan ilegal benar-benar tergantung pada bagaimana Anda berencana menggunakan data hak cipta tersebut setelah Anda menyalin /men-scrap data tersebut. Juga aspek database data, artinya jika menarik seluruh database dari web dan kemudian merepoduksi ulang untuk tujuan sendiri. Amerika dan EU memiliki regulasi yang berbeda terkait aspek ini.","title":"Bentuk data  #2: Copyrighted Data"},{"location":"Pengantar/#membangun-scraping-website","text":"Untuk melakukan tahapan penarikan data dari suatu halaman halaman web maka yang perlu dipehatikan adalah **Menganalisa ** strutktur HTML dari laman web Scraping adalah tentang menemukan pola dalam laman- laman web dan mengektraksi isi dari laman tersebut. Sebelum mulai untuk menulis tool scrapyng (scraper), kita perlu untuk memahami struktur HTML dari web page yang akan ditarik datanya dan mengidentifikasi pola didalamnya. Pola dapat terkait dengan penggunaan classes, id, and elemen elemen lain HTML. **Membuat Scrapy parser dengan python ** Setelah menganlisa struktur dari halaman web yang akan ditarik datanya, kita lakukan implementasi dengan menulis code untuk menghasilkan scrapy parser . Scrapy parser bertanggung jawab untuk menjelajahi web yang akan ditarik datanya dan mengektrak informasi sesuai dengan aturan yang dibuat Mengumpulkan dan menyimpan informasi Parser dapat menyimpan hasilnya sesuai dengan format yang anda inginkan misalkan csv atau json. Ini adalah hasil akhir dari data yang telah dikumpulkan","title":"Membangun Scraping Website ."},{"location":"Pengantar/#1analisa-struktur-html-halaman-website","text":"XPath (kepanjangan dari XML Path Language) adalah expression language yang digunakan untuk menetapkanbagian bagian bagian bagian dari dokumen XML . XPath digunakan dalam perangkat lunak dan bahasa yang digunakan untuk manipulasi dokumen XML, seperti XSLT, XQuery atau alat alat web scraping XPath dapat juga digunakan dalam struktur yang sama seperti XML, misalkan html","title":"1.Analisa Struktur HTML Halaman Website"},{"location":"Pengantar/#markup-languages","text":"XML dan HTML adalah markup languages . Artinya keduanya menggunakan sekumpulan tags atau rule untuk menyusun dan menyedian informasi yang ada didalamnya. Struktur ini membantu untuk secara otomatis untuk pemrosesan , pengeditan, pemformatan dan penampilan dan pencetakan informasi tersebut. Dokumen XML menyimpan data dalam format plain teks. Ini menyediakan perangkat lunan dan perangkat kerans dengan cara bebas untuk menyimpan, mentransformasi, dan menshare data. Format XML adalah format terbuka. Anda dapat membuka dokumen XML dalam editor teks dan data yang ada didalamnya dapat disajikan. Ini memungkinkan pertukran sistem yang tidak kompatibel dan mudah untuk mengkonversi suatu data. Dokumen XML memilki aturan-aturan dasar sebagai berikut: Dokumen XML disusun menggunakan nodes , yaitu terdiri dari node node elemen, node node atribute dan node node teks Node node elemen XML harus harus dibuka dan ditutup tag, misal. tag pembuka <catfood> dan tag penutup </catfood> Tag XML adalah sensitif e, misal. <catfood> tidak sama dengan <catFood> Elemen XML harus bertingkat dengan benar: <catfood> <manufacturer> Purina </manufacturer> <address> 12 Cat Way, Boise, Idaho, 21341 </address> <date> 2019-10-01 </date> </catfood> Node node teks (data) isinya didalam tag pembuka dan tag penutup Node note atribut XML berisi nilai-nilai yang harus di quoted, misal. <catfood type=\"basic\"></catfood>","title":"Markup Languages"},{"location":"Pengantar/#xpath-selalu-mengasumsikan-data-terstruktur-structured-data","text":"Now let\u2019s start using XPath.","title":"XPath selalu mengasumsikan data terstruktur (structured data)."},{"location":"Pengantar/#menelusuri-pohon-node-html-menggunakan-xpath","text":"Cara yang paling umum untuk merepresentasikan struktur dari dokumen XML atau HTML adalah dengan pohon node (node tree): Dalam sebuah dokumen HTML , segala sesuatunya adalah node: Seluruh dokumen adalah node dokumen Setiap elemen elemen HTML adalah node elemen Teks didalam elemen HTML adalah node-node teks Node-node dalam suatu pohon memiliki hubungan hirarki dengan yang lain. Kita menggunakan istilah parent , child dan sibling untuk menjelaskan hubungan ini: Dalam pohon node, node paling atas disebut dengan root (atau root node ) Setiap node memiliki tetap satu parent , kecuali root (yang tidak memiliki parent) Sutau node dapat memilik satu, beberapa atau tidak sama sekali children *Sibling adalah node-node dengan parent sama Serangkaian hubungan antra node ke node disebut dengan path Path-path dalam XPath didefinisikan dengan menggunakan slash ( / ) untuk memisahkan langkah/tahapan dalam rangkaian keterkaitan node, seperti URL-URLS atau direktori Unix Dalam XPath, semua ekspresi didasarkan pada context node . Context node adalah node path dari mana dimulai. Default context adalah node root , yang dinyatakan dengan slash tunggal (/), seperti dalam contoh diatas. Yang paling banyak digunakan untuk ekpresi patha adalah sebagai berikut: Expression Description nodename Memilih semua node dengan nama \u201cnodename\u201d / Garis miring tunggal awal menunjukkan pemilihan dari simpul akar, garis miring berikutnya menunjukkan pemilihan simpul anak dari simpul saat ini // Pilih node anak langsung dan tidak langsung dalam dokumen dari node saat ini - ini memberi kita kemampuan untuk \"skip levels\" . Piiih context node saat ini .. Pilih parent dari context node @ Pilih atribut dari context node [@attribute = 'value'] Pilihan node dengan nilai atribut tertentu text() Pilih isi teks dari node | Rantai ekspresi dan membawa kembali hasil dari ekspresi mana pun","title":"Menelusuri pohon node HTML menggunakan XPath"},{"location":"Pengantar/#sintaks-xpath-lanjutan","text":"","title":"Sintaks XPath lanjutan"},{"location":"Pengantar/#operators","text":"Operator digunakan untuk membandingkan node-node. Terdaoat operator matematika,operator boolean. Operators dapat memberikan nilai boolean (benar /salah) sebagai hasil. Disini beberap yang banyak digunakan : Operator Penjelasan = Membandingkan kesamaan, dapat digunakan untuk nilai numerik maupun teks != Dignuanak untuk membandikanketidak samaan >, >= Lebih besar, lebih besar dari atau sama dengan <, <= Lebih dari, lebih dari atau sama dengan or Boolean atau (or) and Boolean dan (and) not Boolean bukan (not)","title":"Operators"},{"location":"Pengantar/#contoh","text":"Ekspresi Path Hasil Ekspresi html/body/div/h3/ @id =\u2019exercises-2\u2019 Apakah exercise 2 ada? html/body/div/h3/ @id !=\u2019exercises-4\u2019 Apakah exercise 4 tidak ada? //h1/ @id =\u2019references\u2019 or @id =\u2019introduction\u2019 Apakah terdapat terdapat references h1 atau introduction?","title":"Contoh"},{"location":"Pengantar/#predikat","text":"Predikat digunakan untuk menemukan node tertentu atau node yang berisi nilai tertentu Predikat selalu diletakkan dalam kurung siku, dimaksudkan untuk memberikan informasi pemfilteran tambahan untuk mengembalikan node. Anda dapat memfilter pada sebuah node dengan menggunakan operator atau fungsi","title":"Predikat"},{"location":"Pengantar/#examples","text":"Operator Explanation [1] Pilih node pertama [last()] Pilih node yang terakhir [last()-1] Pilih node terakhir kedua [position()<3] Pilih dua node pertama, perhatikan posisi pertama dimulai dari 1, bukan = [@lang] Pilih node yang memiliki atribut \u2018lang\u2019 [@lang='en'] Pilih semua node yang memiliki atribute dengan nilai atribute \u201cen\u201d [price>15.00] Pilih semua node yang memiliki node price yang lebih besar dari 15.00","title":"Examples"},{"location":"Pengantar/#pencarian-dalam-teks","text":"XPath dapat melakukan pencarian dalam teks mengunakan fungsi. Perhatikan: pencarian dalatem teks adalah case-sensitive! Path Expression Result //author[contains(.,\"Matt\")] Cocokkan pada semua node author, dalam node sekarang yang berisi Matt (case-sensitive) //author[starts-with(.,\"G\")] Cocokkan pada semua node author, dalam node sekarang yang dimulai dengan G (case-sensitive) //author[ends-with(.,\"w\")] Cocokkan pada semua dalam node sekarang yang yang berakhiran dengan w (case-sensitive)","title":"Pencarian dalam teks"},{"location":"Pengantar/#menampilkan-source-dari-suatu-page","text":"Untuk menemukan xpath element halaman website maka kita dapat membuka source page halama tersebut. Yaitu dengan cara klik kanan pada halaman","title":"Menampilkan source dari suatu page"},{"location":"Pengantar/#memetakan-suatu-webpage-denganxpath-menggunakan-console-suatu-browser","text":"KIta akan menggunakan kode HTML yang ada pada website portal tugas akhir mahasiswa sebagai contoh. Biasanya, setiap browser web dapat menampilkan sumber kode HTML dengan fungsinya tersendiri dari browser tersebut. Contoh diatas adalah menggunakan browser firefox dengan melakukan klik kanan pada pointer akan menampilakn submenu salah satunya View Page Source. Kemudian klik menu tersebut akand dapatkan source sumbernya sebagai berikut. <!DOCTYPE html> < html > < head > < meta charset = \"utf-8\" /> < title > Portal Tugas Akhir Univ. Trunojoyo </ title > < link rel = \"stylesheet\" href = \"https://pta.trunojoyo.ac.id/css/reset.css\" type = \"text/css\" /> < link rel = \"stylesheet\" href = \"https://pta.trunojoyo.ac.id/css/style.css\" type = \"text/css\" /> < link rel = \"stylesheet\" href = \"https://pta.trunojoyo.ac.id/css/960_12_col.css\" type = \"text/css\" /> < link rel = \"stylesheet\" media = \"screen\" href = \"https://pta.trunojoyo.ac.id/css/smoothness/jquery-ui-1.8.18.custom.css\" /> <!--[if lte IE 8]> <link rel=\"stylesheet\" type=\"text/css\" href=\"css/ie.css\" /> <![endif]--> <!-- favicon --> < link rel = \"shortcut icon\" type = \"image/x-icon\" href = \"https://pta.trunojoyo.ac.id/images/favicon.ico\" /> (...) </ body > </ html > Kita dapat melihat dari source code bahwa judul dari halama ini dalam elemen title yang ada dalam elemen head , yang ada didalam elemen html yang berisi seluruh isi dari page Sehingga jika kita ingin memberi tahu web scraper untuk mencari judul dari halaman ini, kita menggunakan informasi ini untuk menentukan path yang diperlukan untuk menjelajahi isi HTML dari page untuk menemukan element title . XPath memungkinkan untuk melakukan hal tersebut. Kita dapat menjalankan query XPath langsung pada browser dengan JavaScript console yang ada didalamnya.","title":"Memetakan suatu webpage denganXPath menggunakan console suatu  browser"},{"location":"Pengantar/#menampilkan-console-dalam-browser","text":"Pad Firefox, gunakan menut Tools > Web Developer > Web Console . PadaChrome, gunakan menut View > Developer > JavaScript Console Disini bagaimana console tampak dalam browser Firefox : Untuk saat ini, jangan khawatir terlalu banyak pesan error manakalan anda meliha dalam console ketika anda membukanya. Console akan dengan muncul prompt dengan karakter > (atau >> dalam Firefox) menunggu anda untuk mengetik perintah . Sintaks untuk menjalankan query XPath dalam JavaScript console adalah $x(\"XPATH_QUERY\") , misalkan: $x(\"/html/head/title/text()\") Ini akan menghasilkan sesuatu seperti ini <- Array [ #text \"Portal Tugas Akhir Univ. Trunojoyo\" ]","title":"Menampilkan console dalam  browser"},{"location":"Pengantar/#menggunkan-extension-firefox-xpath-finder","text":"Untuk menentukan cpath dari element dari halam website dengan mudah dilakukan menggunakan xPath Finder. Jika belum ada ektension firefox xPath Finder ,silahkan install dulu extension tersebut. Penggunaannya adalah dengan cara klik pada icon extension curson akan berubah menjadi crosshair, kemudian pilih elemen yang diinginkan dengan kursor diarahkan pada elemen tersebut ( elemen akan berubah warna ). Klik elemen tersebut sehingga akan tampil xPath dari elemen tersebut di bawah kiri dari halaman In [ 3 ] : fetch ( 'https://pta.trunojoyo.ac.id/' ) 2020 -09-25 04 :32:42 [ scrapy.core.engine ] DEBUG: Crawled ( 200 ) <GET https://pta.t runojoyo.ac.id/> ( referer: None ) In [ 4 ] : response.xpath ( \"/html/body/div[2]/div[1]/div[2]/ul/li[1]/div[1]/a\" ) Out [ 4 ] : [ <Selector xpath = '/html/body/div[2]/div[1]/div[2]/ul/li[1]/div[1]/a' dat a = '<a class=\"title\" href=\"#\">Analisis Wa...' > ] Dari hasil scrap diatas ddapat diperbaiki ouputnya dengan perintah sebagai berikut In [ 5 ] : response.xpath ( \"//item/title/text()\" ) .extract_first () In [ 6 ] : response.xpath ( \"/html/body/div[2]/div[1]/div[2]/ul/li[1]/div[1]/a/text( ...: )\" ) .extract_first () Out [ 6 ] : 'Analisis Wacana Media Online Detik.com dalam Memberitakan Peristiwa Ker usuhan Mahasiswa Papua di Surabaya' Mari kita bahas query XPath yang digunakan pada contoh diatas dengan XPath /html/head/title/text() . Yang pertama / menyatakan root dari suatu dokumen. Dengan query tersebut , kita memberit tahu browser untuk / Start at the root of the document\u2026 html/ \u2026 mengarahkan ke node html \u2026 head/ \u2026 kemudaian ke node head yang ada didalamnya\u2026 title/ \u2026 kemudaian ke node title yang ada didalamnya\u2026 text() dan pilih node text yang terdapat dalam elemen itu Dengan menggunakan sintaks ini , XPath kemudian memungkinkan kita untuk menentukan path yang tepat pada suatu node.","title":"Menggunkan extension firefox xPath Finder"},{"location":"Pengantar/#memilih-judul-tugas-akhir","text":"$x ( \"/html/body/div[2]/div[1]/div[2]/ul/li[1]/div[1]/a\" )","title":"Memilih judul tugas akhir"},{"location":"Pengantar/#menggunakan-scraper-chrome-extension-untuk-menavigasi-xpath","text":"Ini adalah alat yang memudahkan untuk mengimplementasikan XPath. Alat ini adalah tambahan dari browser Chrome untuk melakukan scrap data dari suatu website. Jika kita ingin menggunakan tool ini maka perlu ditambahkan pada browser Chrome kita seperti biasa kita menambahkan extension Chrome secara umum. Setelah kita install extension ini misalkan kita sedang menjelajahi pta.trunojoyo.ac.id, pada saat klik kanan pada website tersebut maka akan muncul menu Dan bila kita mengklik menu scraper similar maka akan memunculkan XPath : //div[2]/div[1]/div[2]/ul/li dengan data yang ditarik dari halam website tersebut. Pada view di samping Xpath judul, penulis, dosen pembimbing dan abstrak ditampilkan dalam satu kolom dengan nama kolom text. Untuk memperbaiki hasil tampilan ini, supaya menjadi field field yang sesuai, maka diperlukan perubahan dengan menambahkan nama kolom (field) dan memecah XPath yang sesuai. Berikut perubahan yang telah dilakukan","title":"Menggunakan  Scraper Chrome extension untuk menavigasi XPath"},{"location":"Pengantar/#2membuat-scrapy-parser-dengan-python-tugas","text":"","title":"2.**Membuat Scrapy parser dengan Python ** (TUGAS)"},{"location":"Pengantar/#persiapan","text":"instalasi alat alat yaitu Scrapy library pip install scrapy","title":"Persiapan"},{"location":"Pengantar/#desain-secara-umum-scraping-website","text":"Source","title":"Desain secara umum scraping website"},{"location":"Pengantar/#pembuatan-proyek","text":"Proyek 1. Menarik data harga dari website a. Membuat proyek bernama mytugaswebmining scrapy startproject mytugaswebmining Akan terbentuk struktur folder shopee seperti berikut \\---mytugaswebmining | scrapy.cfg | \\---mytugaswebmining | items.py | middlewares.py | pipelines.py | settings.py | __init__.py | \\---spiders __init__.py Scrapy merupakan framewerok aplikasi yang memungkinkan suatu suatu proyek sesuai dengan gaya pemrograman Object Oriented untuk mendefinisikan item-item dan spider untuk keseluruhan aplikasi. Struktur proyek dari scrapy yang telah dibuat diatas dengan masing masing sebagai berikut scrapy.cfg : ini adalah file konfigurasi proyek yang berisi modul pengaturan untuk proyek dan juga dengan informasi pengimplementasiannya. test_project : Ini adalah direktori aplikasi dengan banyak macam file yang benar bena bertanggung jawab untuk menjalankan dan menarik data dari urls-url web. items.py : item-item yang akan ditarik oleh scraper, ini juga bekerja seperti Python dicts . Karena kita dapat menggunakan plain Python dicts dengan Scrapy, Items menyediakan proteksi tambahan terhadap field-field yang tidak dideklarasikan. Hal itu dideklarasikan dengan pembuatan scrapy.Item class dan mendefinisika atribut atributnya sebagai scrapy.Field . pipelines.py : Setelah item di scrap oleh spider, item itu dikirim ke Item Pipeline yang memprosesnya dengan menggunakan beberapa komponen yang dijalankan secara berurutan. Masing masing item pipeline component adalah kelas Python yang harus mengimplementasikan metode yang disebut dengan process_item untuk memproses item yang di- scrap . Proses dilakukan terhadap item dengan memutuskan item harus diteruskan atau didrop dan tidak dinajutkan proses berikutnya. settings.py : Ini memungkinkan kita untuk mengkustom semua karakteristi komponen-komponen Scrapy termasuk core, extensions, pipelines dan spiders itu sediri. spiders : Spiders adalah direktori yang berisi semua spiders/crawlers seperti kelas-kelas Python . Ketika kita menjalankan atau meng-crawler spider maka scrapy mencari didalam di direktori ini dan mencoba untuk menemukan spider dengan namanya yang telah ditetapkan oleh pengguna. Spider mendefinisikan bagaimana site tertentu atau sekumpulan site, termasuk bagaimana melakukan crawl dan bagaimana mengektrak data dari page-pagenya. Artinya, Spiders adalah tempat dimana kita mendefinisikan tiga atribut utam yaitu start_urls yang meberi tahu URLs mana yang akan ditarik, , allowed_domains yaitu mendefinisikan domain mana saja yang boleh parse adalah suatu metode yang dipanggil ketika ada respon yang berasal dari lodged requests . Atribut ini adalah penting karena ini inti dari definisi Spider","title":"Pembuatan Proyek"},{"location":"Pengantar/#referensi","text":"Zaki, Mohammed J., and Wagner Meira. Data mining and analysis: fundamental concepts and algorithms. Cambridge University Press, 2014 \u21a9 https://blog.datahut.co/scraping-amazon-reviews-python-scrapy/ \u21a9","title":"Referensi"},{"location":"Scrapingdef/","text":"Dalam materi ini, kita akan membahan topik \u00b6 Pengantar bidang web scraping Menjelaskan tantangan legalitas web scraping Apa itu web scraping \u00b6 Web scraping adalah proses mengektrak data dari website. Selanjutnya data yang tersedia di website dinyatakan dalam bentuk format yang dapat dengan mudah dioleh oleh mesin untuk dianalisa lebih lanjut. Misalkan data dinyatakan dalam bentuk format CSV, disimpan dalam database tertentu dan sebagainya. Data yang ada di website itu bentuknya tidak terstrukur, artinya tidak siap digunakan untuk analisis. Ada beberapa cara scrape data dari website untuk diektrak informasinya untuk digunakan. Bentuk yang paling sederhana, adalah dengan menyalin dan mempast bagian bagian tersebut dari website. TEntunya ini tidak praktis dilakukan jika banyak data yang yang akan diektrak, atau tersebar dibeberapa website. Sehingga diperlukan tool khusus dan teknik khusus yang digunakan untuk melakukan secara otomatis proses ini, dengan menetapkan webiste yang akan dijelajahi informasi apa yang akan dicari dan apakah ektraksi data berhenti diakhir halama yang ditemukan ataukah mengikuti hyperlink dan mengulangi proses secara rekursif. Proses automatis dari web scraping juga memungkinkan untuk dilakukan apakah proses akan dijalankan pada rentang waktu tertentu dan menangkap perubahan yang terjadi dari data. Teknik Webscraping membutuhkan pemahaman teknologi yang digunakan untuk menampilkan inforamasi pada web. Oleh karena itu diperlukan pemahaman tentang HTML dan Document Object Model (DOM) termasuk pemahana sintak XPath untuk memilih elemen pada website untuk Apa web scraping diperlukan \u00b6 Untuk web indexing yang digunakan oleh mesin pencari misal Google untuk menganalisa secara masal web untuk membangun indeknya. Memonitoring perubahan data e-commerse secara online untuk tujuan pemasaran. Seandainya kita telah memiliki toko penjualan sepatu dan ingin untuk melacak terus harga pesaing kita. Kita dapat mengunjungi website pesaing kita setiap hari untuk membandingkan dengan masing masing harga sepatu yang kita miliki. Akan tetapi ini akan butuh waktu banyak dan tidak layak dilakukan jika kita menjual ribuan sepatu atau dibutuhkan untuk mengecek perubahan harga lebih sering. Ini adalah pekerjaan yang tidak efisien dan efektif . Oleh karena itu kita perlu mesin atomatis dengan teknik web scraping untuk menggantikan proses manual tersebut Web scraping juga banyak digunakan oleh is also increasingly being used by scholars to create data sets for text mining projects; these might be collections of journal articles or digitised texts. The practice of data journalism , in particular, relies on the ability of investigative journalists to harvest data that is not always presented or published in a form that allows analysis. Apkah web scraping legal? \u00b6 Jika data yang ditarik (scrap) keperluan sendiri, dalam prakteknya tidak masalah. Akan tetapi jika data akan dipublikasikan lagi maka ada beberapa aspek hukum yang harus dipertimbangkan. Secara singkat bahwa legalitas tidaknya kita melakukan web scraping atau abu abunya tergantung pada tiga hal ini: Bentuk data yang akan discrap Bagaiman anda merencanakan untuk menggunakan data yang di-scrap Bagaiman anda mengektrak data dari website Nomor 1 dan 2 sangat jelas sehingga kita akan memulainya daru sini sebelum membahas nomor 3. Apa saja dari bentuk data yang ilegal untuk di scrap \u00b6 Data seperi e-commerce, personal atau data article , adalah bentuk data yang sangat terkait dengan legalitas. Dua bentuk data yang kita perlu hati-hati atau dikwatirkan legalitasnya adalah Personal Data Copyrighted Data JIka data yang anda tarik adalah bukan bentuk data dari diatas, secara umum adalah legal Bentuk data 1: Personal Data \u00b6 Personal data adalah data yang digunakan secara langsung atau tidak langsung untuk mengidentifksi individu tertentu. Bentuk dari data personal yaitu : Name Email Phone Number Address User Name IP Address Date of Birth Employment Info Bank or Credit Card Info Medical Data Biometric Data Bentuk data #2 : Copyrighted Data \u00b6 Bentuk data yang perlu hati hati untuk ditarik adalah copyrighted data. Copyrighted data data yang dimiliki oleh pribadi atau bisnis untuk mengendalikan terkait reproduksi dan kepemilikan. Seperti gambar dan lagu walaupun data dipublikasi di internet bukan berarti itu legal untuk ditarik tanpa pemberitahuan pemiliknya. Secara umum data tersebut adalah: Articles Videos Pictures Stories Music Databases Mengekstrak data berhak cipta bukanlah tindakan ilegal, jadi tindakan legal dan ilegal benar-benar tergantung pada bagaimana Anda berencana menggunakan datahak cipta tersebut setelah Anda menyalin /men-scrap data tersebut. Juga aspek database data, artinya jika menarik seluruh database dari web dan kemudian merepoduksi ulang untuk tujuan sendiri. Amerika dan EU memiliki regulasi yang berebda terkait aspek ini. Risiko melanggar hack terkait dengan masalah databasea dapat dikurangi dengan mengubah cara pengumpulan dan penggunaan data: Hanya menscrap beberapa data yang tersedia; Jangan mereplikasi struktur susunan dari database asli; Selanjutnya kita akan menjawan masalah legalitas daengan bagai anda mengektrak data dari suatu website. Is Web Scraping Itself Illegal? \u00b6 It\u2019s pretty straightforward to determine if scraping personal or copyrighted data will make your web scraping illegal because there are clear laws that set out what is legal and what is illegal. It gets a lot more tricky when it comes to the act of web scraping itself because no government has passed any law explicitly legalising or delegalising web scraping. Instead, we have to go off the verdicts of lawsuits between web scrapers and website owners. Which there are many: Craigslist vs 3Taps Ryanair vs PR Aviation Facebook vs Power Ventures HiQ vs LinkedIn To name a few. The main issue of all these cases is the question of whether the Terms of Service listed on many websites that forbid web scraping (or automatic access) are legally enforceable. Although cases have gone both ways, as of 2020, the courts are clarifying the situation of for web scrapers. The most recent of which HiQ vs LinkedIn, found that scraping data from a website doesn\u2019t violate anti-hacking laws as long as the data is public and the scraper hasn\u2019t explicitly agreed to the website\u2019s terms and conditions in advance. What this means is that so long as the data is publicly available on a website, and doesn\u2019t require the web scraper to login and explicitly accept the terms of conditions of the website, the web scraper is within their right to scrape the publically available data. So how does this affect web scrapers? If you are scraping a website then you need to ask these questions to determine if its legal or not: Is the data publicly available? If the data isn\u2019t hidden behind a login, then the website\u2019s terms and conditions aren\u2019t enforceable so you can legally scrape the public data. Do you need to create an account and login to access the data? If this is the case then you need to examine the terms and conditions you agreed to when you created the account, because by agreeing to them you made them legally enforceable. Your Own Legal Sanity Check \u00b6 So there you go, we\u2019ve discussed all the main issues that determine the legality of your web scraping. In the majority of cases we see, what companies want to scrape is perfectly legal. However, we always advise them to double-check their plans with these three simple checks: Am I scraping personal data? Am I scraping copyrighted data? Am I scraping data from behind a login? If your answers to all three of these questions is \u201cNo\u201d, then your web scraping is legal. However, if you answer \u201cYes\u201d to any of them, then you should take a step back and do a full legal review of your web scraping to ensure your not scraping the web illegally. https://www.scraperapi.com/blog/is-web-scraping-legal/ Apa perbedaan antara web scraping dan web crawling \u00b6 Web scraping adalah prose ektraksi data dari website-website. Web scraping lebih tepatnya menjelajahi suatu website untuk menyalin dan mem-paste isi dari website, sedangakan web crawling adalah terkait denga link-link yang diikuti, anda menjelajahi website dan menyimpan link link yang ada didalam website yang dijelajahi","title":"Scrapingdef"},{"location":"Scrapingdef/#dalam-materi-ini-kita-akan-membahan-topik","text":"Pengantar bidang web scraping Menjelaskan tantangan legalitas web scraping","title":"Dalam materi ini, kita akan membahan topik"},{"location":"Scrapingdef/#apa-itu-web-scraping","text":"Web scraping adalah proses mengektrak data dari website. Selanjutnya data yang tersedia di website dinyatakan dalam bentuk format yang dapat dengan mudah dioleh oleh mesin untuk dianalisa lebih lanjut. Misalkan data dinyatakan dalam bentuk format CSV, disimpan dalam database tertentu dan sebagainya. Data yang ada di website itu bentuknya tidak terstrukur, artinya tidak siap digunakan untuk analisis. Ada beberapa cara scrape data dari website untuk diektrak informasinya untuk digunakan. Bentuk yang paling sederhana, adalah dengan menyalin dan mempast bagian bagian tersebut dari website. TEntunya ini tidak praktis dilakukan jika banyak data yang yang akan diektrak, atau tersebar dibeberapa website. Sehingga diperlukan tool khusus dan teknik khusus yang digunakan untuk melakukan secara otomatis proses ini, dengan menetapkan webiste yang akan dijelajahi informasi apa yang akan dicari dan apakah ektraksi data berhenti diakhir halama yang ditemukan ataukah mengikuti hyperlink dan mengulangi proses secara rekursif. Proses automatis dari web scraping juga memungkinkan untuk dilakukan apakah proses akan dijalankan pada rentang waktu tertentu dan menangkap perubahan yang terjadi dari data. Teknik Webscraping membutuhkan pemahaman teknologi yang digunakan untuk menampilkan inforamasi pada web. Oleh karena itu diperlukan pemahaman tentang HTML dan Document Object Model (DOM) termasuk pemahana sintak XPath untuk memilih elemen pada website","title":"Apa itu web scraping"},{"location":"Scrapingdef/#untuk-apa-web-scraping-diperlukan","text":"Untuk web indexing yang digunakan oleh mesin pencari misal Google untuk menganalisa secara masal web untuk membangun indeknya. Memonitoring perubahan data e-commerse secara online untuk tujuan pemasaran. Seandainya kita telah memiliki toko penjualan sepatu dan ingin untuk melacak terus harga pesaing kita. Kita dapat mengunjungi website pesaing kita setiap hari untuk membandingkan dengan masing masing harga sepatu yang kita miliki. Akan tetapi ini akan butuh waktu banyak dan tidak layak dilakukan jika kita menjual ribuan sepatu atau dibutuhkan untuk mengecek perubahan harga lebih sering. Ini adalah pekerjaan yang tidak efisien dan efektif . Oleh karena itu kita perlu mesin atomatis dengan teknik web scraping untuk menggantikan proses manual tersebut Web scraping juga banyak digunakan oleh is also increasingly being used by scholars to create data sets for text mining projects; these might be collections of journal articles or digitised texts. The practice of data journalism , in particular, relies on the ability of investigative journalists to harvest data that is not always presented or published in a form that allows analysis.","title":"untuk Apa web scraping diperlukan"},{"location":"Scrapingdef/#apkah-web-scraping-legal","text":"Jika data yang ditarik (scrap) keperluan sendiri, dalam prakteknya tidak masalah. Akan tetapi jika data akan dipublikasikan lagi maka ada beberapa aspek hukum yang harus dipertimbangkan. Secara singkat bahwa legalitas tidaknya kita melakukan web scraping atau abu abunya tergantung pada tiga hal ini: Bentuk data yang akan discrap Bagaiman anda merencanakan untuk menggunakan data yang di-scrap Bagaiman anda mengektrak data dari website Nomor 1 dan 2 sangat jelas sehingga kita akan memulainya daru sini sebelum membahas nomor 3.","title":"Apkah web scraping legal?"},{"location":"Scrapingdef/#apa-saja-dari-bentuk-data-yang-ilegal-untuk-di-scrap","text":"Data seperi e-commerce, personal atau data article , adalah bentuk data yang sangat terkait dengan legalitas. Dua bentuk data yang kita perlu hati-hati atau dikwatirkan legalitasnya adalah Personal Data Copyrighted Data JIka data yang anda tarik adalah bukan bentuk data dari diatas, secara umum adalah legal","title":"Apa saja dari bentuk data yang ilegal untuk di scrap"},{"location":"Scrapingdef/#bentuk-data-1-personal-data","text":"Personal data adalah data yang digunakan secara langsung atau tidak langsung untuk mengidentifksi individu tertentu. Bentuk dari data personal yaitu : Name Email Phone Number Address User Name IP Address Date of Birth Employment Info Bank or Credit Card Info Medical Data Biometric Data","title":"Bentuk data 1: Personal Data"},{"location":"Scrapingdef/#bentuk-data-2-copyrighted-data","text":"Bentuk data yang perlu hati hati untuk ditarik adalah copyrighted data. Copyrighted data data yang dimiliki oleh pribadi atau bisnis untuk mengendalikan terkait reproduksi dan kepemilikan. Seperti gambar dan lagu walaupun data dipublikasi di internet bukan berarti itu legal untuk ditarik tanpa pemberitahuan pemiliknya. Secara umum data tersebut adalah: Articles Videos Pictures Stories Music Databases Mengekstrak data berhak cipta bukanlah tindakan ilegal, jadi tindakan legal dan ilegal benar-benar tergantung pada bagaimana Anda berencana menggunakan datahak cipta tersebut setelah Anda menyalin /men-scrap data tersebut. Juga aspek database data, artinya jika menarik seluruh database dari web dan kemudian merepoduksi ulang untuk tujuan sendiri. Amerika dan EU memiliki regulasi yang berebda terkait aspek ini. Risiko melanggar hack terkait dengan masalah databasea dapat dikurangi dengan mengubah cara pengumpulan dan penggunaan data: Hanya menscrap beberapa data yang tersedia; Jangan mereplikasi struktur susunan dari database asli; Selanjutnya kita akan menjawan masalah legalitas daengan bagai anda mengektrak data dari suatu website.","title":"Bentuk data  #2: Copyrighted Data"},{"location":"Scrapingdef/#is-web-scraping-itself-illegal","text":"It\u2019s pretty straightforward to determine if scraping personal or copyrighted data will make your web scraping illegal because there are clear laws that set out what is legal and what is illegal. It gets a lot more tricky when it comes to the act of web scraping itself because no government has passed any law explicitly legalising or delegalising web scraping. Instead, we have to go off the verdicts of lawsuits between web scrapers and website owners. Which there are many: Craigslist vs 3Taps Ryanair vs PR Aviation Facebook vs Power Ventures HiQ vs LinkedIn To name a few. The main issue of all these cases is the question of whether the Terms of Service listed on many websites that forbid web scraping (or automatic access) are legally enforceable. Although cases have gone both ways, as of 2020, the courts are clarifying the situation of for web scrapers. The most recent of which HiQ vs LinkedIn, found that scraping data from a website doesn\u2019t violate anti-hacking laws as long as the data is public and the scraper hasn\u2019t explicitly agreed to the website\u2019s terms and conditions in advance. What this means is that so long as the data is publicly available on a website, and doesn\u2019t require the web scraper to login and explicitly accept the terms of conditions of the website, the web scraper is within their right to scrape the publically available data. So how does this affect web scrapers? If you are scraping a website then you need to ask these questions to determine if its legal or not: Is the data publicly available? If the data isn\u2019t hidden behind a login, then the website\u2019s terms and conditions aren\u2019t enforceable so you can legally scrape the public data. Do you need to create an account and login to access the data? If this is the case then you need to examine the terms and conditions you agreed to when you created the account, because by agreeing to them you made them legally enforceable.","title":"Is Web Scraping Itself Illegal?"},{"location":"Scrapingdef/#your-own-legal-sanity-check","text":"So there you go, we\u2019ve discussed all the main issues that determine the legality of your web scraping. In the majority of cases we see, what companies want to scrape is perfectly legal. However, we always advise them to double-check their plans with these three simple checks: Am I scraping personal data? Am I scraping copyrighted data? Am I scraping data from behind a login? If your answers to all three of these questions is \u201cNo\u201d, then your web scraping is legal. However, if you answer \u201cYes\u201d to any of them, then you should take a step back and do a full legal review of your web scraping to ensure your not scraping the web illegally. https://www.scraperapi.com/blog/is-web-scraping-legal/","title":"Your Own Legal Sanity Check"},{"location":"Scrapingdef/#apa-perbedaan-antara-web-scraping-dan-web-crawling","text":"Web scraping adalah prose ektraksi data dari website-website. Web scraping lebih tepatnya menjelajahi suatu website untuk menyalin dan mem-paste isi dari website, sedangakan web crawling adalah terkait denga link-link yang diikuti, anda menjelajahi website dan menyimpan link link yang ada didalam website yang dijelajahi","title":"Apa perbedaan antara web scraping dan web crawling"},{"location":"Tutorial%20Mathjax/","text":"http://saxarona.github.io/mathjax-viewer/","title":"Tutorial Mathjax"},{"location":"authors-notes/","text":"Author's notes \u00b6 Hi, I'm Martin ( @squidfunk ) \u00b6 I'm a freelance polyglot software engineer and entrepreneur from Cologne, Germany with more than 12 years of experience in full-stack web development and system programming. Why another theme? \u00b6 Some time ago I wanted to release a project to the open, but it was in need of user documentation. I checked out the available tools and stuck with MkDocs, because it was so simple and easy to use. However, none of the available themes convinced me. I wanted to build something that was usable on all screen sizes from the ground up, something beautiful and practical at the same time. Google's Material Design appeared to be the perfect fit and this something became Material, a Material Design theme for MkDocs.","title":"Author's notes"},{"location":"authors-notes/#authors-notes","text":"","title":"Author's notes"},{"location":"authors-notes/#hi-im-martin-squidfunk","text":"I'm a freelance polyglot software engineer and entrepreneur from Cologne, Germany with more than 12 years of experience in full-stack web development and system programming.","title":"Hi, I'm Martin (@squidfunk)"},{"location":"authors-notes/#why-another-theme","text":"Some time ago I wanted to release a project to the open, but it was in need of user documentation. I checked out the available tools and stuck with MkDocs, because it was so simple and easy to use. However, none of the available themes convinced me. I wanted to build something that was usable on all screen sizes from the ground up, something beautiful and practical at the same time. Google's Material Design appeared to be the perfect fit and this something became Material, a Material Design theme for MkDocs.","title":"Why another theme?"},{"location":"compliance/","text":"Compliance with GDPR \u00b6 Material does not process any personal data \u00b6 Material is a theme for MkDocs, a static site generator. In itself, Material does not perform any tracking or processing of personal data. However, some of the third-party services that Material integrates with may actually be in breach with the General Data Protection Regulation (GDPR) and need to be evaluated carefully. Third-party services \u00b6 Google Fonts \u00b6 Material makes fonts easily configurable by relying on Google Fonts CDN. However, embedding fonts from Google is currently within a gray area as there's no official statement or ruling regarding GDPR compliance and the topic is still actively discussed . For this reason, if you need to ensure GDPR compliance, you should disable the usage of the Google Font CDN with: theme : font : false When Google Fonts are disabled, Material will default to Helvetica Neue and Monaco with their corresponding fall backs, relying on system fonts. You could however include your own, self-hosted webfont by overriding the fonts block. The icon fonts (Material and FontAwesome) are bundled with the theme, and thus self-hosted so there's no third-party involved. Google Analytics and Disqus \u00b6 Material comes with Google Analytics and Disqus integrations that need to be enabled explicitly . Disable both integrations in order to be in compliance with the GDPR.","title":"Compliance with GDPR"},{"location":"compliance/#compliance-with-gdpr","text":"","title":"Compliance with GDPR"},{"location":"compliance/#material-does-not-process-any-personal-data","text":"Material is a theme for MkDocs, a static site generator. In itself, Material does not perform any tracking or processing of personal data. However, some of the third-party services that Material integrates with may actually be in breach with the General Data Protection Regulation (GDPR) and need to be evaluated carefully.","title":"Material does not process any personal data"},{"location":"compliance/#third-party-services","text":"","title":"Third-party services"},{"location":"compliance/#google-fonts","text":"Material makes fonts easily configurable by relying on Google Fonts CDN. However, embedding fonts from Google is currently within a gray area as there's no official statement or ruling regarding GDPR compliance and the topic is still actively discussed . For this reason, if you need to ensure GDPR compliance, you should disable the usage of the Google Font CDN with: theme : font : false When Google Fonts are disabled, Material will default to Helvetica Neue and Monaco with their corresponding fall backs, relying on system fonts. You could however include your own, self-hosted webfont by overriding the fonts block. The icon fonts (Material and FontAwesome) are bundled with the theme, and thus self-hosted so there's no third-party involved.","title":"Google Fonts"},{"location":"compliance/#google-analytics-and-disqus","text":"Material comes with Google Analytics and Disqus integrations that need to be enabled explicitly . Disable both integrations in order to be in compliance with the GDPR.","title":"Google Analytics and Disqus"},{"location":"contributing/","text":"../CONTRIBUTING.md","title":"Contributing"},{"location":"customization/","text":"Silahkan ditunggu next content... \u00b6","title":"Silahkan ditunggu  next content..."},{"location":"customization/#silahkan-ditunggu-next-content","text":"","title":"Silahkan ditunggu  next content..."},{"location":"license/","text":"License \u00b6 MIT License Copyright \u00a9 2016 - 2019 Martin Donath Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"license/#license","text":"MIT License Copyright \u00a9 2016 - 2019 Martin Donath Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"memahami-data/","text":"Memahami Data \u00b6 Macam macam Data \u00b6 Dalam data data mining dan maha datar, Anda akan menemukan banyak jenis data yang berbeda, dan masing-masing cenderung membutuhkan alat dan teknik yang berbeda. Macam macam data dikelompokkan sebagai berikut: Data terstruktur (structured) Data tidak terstruktur(unstructured Data bahasa alami(Natural Language) Data yang dibangkit oleh Mesin (Machined-Generated) Data Audio, Video,Citra Data Streamming Data berbasis Graph(Graph-based) Data Terstruktur \u00b6 Data terstruktur adalah data yang bergantung pada model data dan yang dinyatakan dalam bentuk tabel dengan atribut} (kolom) dan baris. Data terstruktur mudah disimpan dalam database dalam bentuk tabel atau file excel (Ms Office), SQl (structure Query Language)sehingga mudah dilakukan query terhadap data tersebut. Tetapi realitanya banyak data yang ada dalam dalam bentuk data tidak terstruktur karena data dihasilkan oleh manusia dan mesin Gambar 2.1 Contoh data terstruktur Macam- macam atribut \u00b6 Atribut adalah data yang mewakili karakteristik atau fitur dari objek data. Atribut bisa disebut juga dengan dimensi, fitur, dan variabel yang istilah itu sering digunakan literatur. Dimensi istilah yang biasanya digunakan dalam data warehouse. Dalam literatur pembelajaran mesin cenderung menggunakan istilah fitur, sementara dalam bidang statistik lebih memilih menggunakan istilah variabel. Dalam penambangan data atau data miniing dan database biasa menggunakan istilah atribut atau fitur , dan dalam buku ini juga menggunakan istilah atribut atau fitur. Contoh atribut-atribut yang menggambarkan objek pelanggan dapat mencakup, misalnya ID pelanggan, nama, dan alamat. Nilai yang diamati untuk atribut tertentu dikenal sebagai nilai observasi. Sekumpulan atribut yang digunakan untuk menggambarkan objek disebut disebut dengan vektor atribut (atau vektor fitur. Distribusi data yang melibatkan satu atribut (atau variabel) disebut univariat. Distribusi bivariat melibatkan dua atribut, dan seterusnya. Jenis atribut ditentukan oleh nilai-nilai pada atribut tersebut yang mungkin nominal, biner,atau ordinal, atau numerik. Pada subbagian berikut, kami perkenalkan nilai nilai tersebut Macam macam tipe data atribut \u00b6 Atribut Nominal Nilai atribut nominal adalah simbol ataunama barang. Setiap nilai mewakili beberapa jenis kategori, kode, atau status, dan Atribut nominal juga disebut kategori. Nilai-nilainya tidak memiliki tingkatan nilai. Dalam ilmu komputer, nilainya juga dikenal sebagai enumerasi Contoh : Misalkan warna rambut dan status perkawinan adalah dua atribut dari data orang. Nilai yang mungkin untuk warna rambut adalah hitam, coklat, pirang, merah, hitam pucat, abu-abu, dan putih. Status perkawinan memiliki nilai atribut lajang, menikah, bercerai, dan janda. Baik warna rambut maupun status perkawinan adalah atribut nominal. Contoh lain dari atribut nominal adalah atribut pekerjaan dengan nilai-nilainya adalah guru, dokter gigi, programmer, petani, dan sebagainya Atribut Biner Atribut biner adalah atribut nominal dengan hanya dua kategori atau status: 0 atau 1, di mana 0 biasanya berarti atribut itu tidak ada, dan 1 berarti itu ada. Atribut Biner disebut sebagai Boolean jika dinyatakan dengan benar (true) dan salah(false) Contoh : Terdapat atribut yang menggambarkan merokok pada pasien, 1 menunjukkan bahwa pasien merokok,sementara 0 menunjukkan bahwa pasien tidak merokok. Demikian pula, seandainya ada pasien menjalani tes medis yang memiliki dua kemungkinan hasil. Atribut Tes medis bersifat biner, dengan nilai 1 berarti hasil tes untuk pasien positif, sedangkan 0 berarti hasilnya negatif. Atribut biner simetris jika keduanya emiliki nilai bobot yang sama; Artinya, tidak ada kekhususan mengenai hasil mana yang harus dikodekan sebagai 0 atau 1. Misalkan atribut gender yang dengan nila atributnya laki dan perempuan. Atribut biner adalah asimetris jika hasil dari nilai nilainya tidak sama pentingnya seperti hasil positif dan negatif dari tes medis untuk HIV. Dengan mengkodekan hasil yang paling penting, biasanya 1 (mis., HIV positif) dan yang lainnya dengan 0 (mis., HIV negatif) Atribut ordinal Atribut ordinal adalah atribut dengan nilai yang memiliki arti urutan atau peringkat di antara nilai nilai yang ada, tapi besarnya nilai yang berurutan tersebut tidak diketahui. Ukuran kecenderungan terpusat dari atribut ordinal dapat diwakili oleh modus dan median median (nilai tengah), tetapi tidak untuk nilai rata-rata.Perlu diperhatikan bahwa atribut nominal, biner, dan ordinal bersifat kualitatif. Artinya, atribut-atribut tersebut hanya menjelaskan sebuah fitur dari suatu objek tanpa memberikan ukuran atau kuantitas yang sebenarnya. Nilai-nilai atribut kualitatif biasanya merupakan katakata yang mewakili kategori Contoh : Atribut ordinal Misalkan ukuran minuman yang tersedia di sebuah restoran cepat saji. Atribut nominal ini memiliki tiga nilai yang mungkin: kecil, sedang, dan besar. Nilai memiliki arti urutan yang (yang sesuai dengan ukuran minuman). Contoh atribut ordinal lainnya adalah pangkat dan jabatan profesi. Atribut ordinal berguna untuk melakukan penilaian subjektif terhadap kualitas sesuatu objek yang tidak dapat diukur secara obyektif; atribut ordinal sering digunakan dalam survei untuk peringkat. Dalam satu survei, para peserta diminta untuk menilai tingkat kepuasan mereka sebagai pelanggan.Kepuasan pelanggan memiliki kategori ordinal berikut ini: 0: sangat tidak puas,1: agak tidak puas, 2: netral, 3: puas, dan 4: sangat puas. Atribut ordinal juga dapat diperoleh dari iskritisasi nilai atribut numerik dengan membagi rentang nilai menjadi urutan kategoria Atribut Numerik Atribut numerik bersifat kuantitatif; Artinya, ini adalah kuantitas yang terukur, yang dinyatakan dengan bilangan bulat atau nilai riel. Atribut numerik dapat Atribut Skala Interval(interval-scaled) atau skala ration (ratio-scaled) Atribut skala interval diukur pada dengan skala unit ukuran yang sama. Nilai - nilai Interval berskala memiliki urutan dan bisa positif, 0, atau negatif. Jadi, selain untuk memberikan peringkat nilai, atribut semacam itu memungkinkan kita untuk membandingkan dan mengukur perbedaan antar nilai Contoh : Atribut suhu adalah Skala interval. Misalkan kita memiliki nilai suhu di luar ruangan untuk beberapa hari yang berbeda dari suatu objek. Dengan mengurutkan nilai, kita mendapatkan peringkat objek yang berkenaan dengan suhu. Selain itu, kita bisa mengukur perbedaan antara nilai.Misalnya, a suhu 20o C adalah lima derajat lebih tinggi dari suhu 15oC. Contoh lain kalender tahun adalah. Misalnya, tahun 2002 dan 2010 terpisah delapan tahun. Karena atribut skala interval adalah numerik, kita dapat menghitung nilai ratarata, ukuran median dan modus dari kecenderungan terpusat Atribut Skala Ratio Atribut skala rasio adalah atribut numerik dengan melekat titik nol pada nilai atribut tersebut. Artinya, jika pengukuran adalah berskala rasio, kita dapat dapat mengatakan berapa kali dari nilai yang lain atau rasio dari nilai yang lain. Selain itu, nilai yang dipesan, dan kita juga bisa menghitung selisih antara nilai, serta mean, median, dan modus Contoh Atribut tentang pengukuran berat badan, tinggi badan, jumlah kata dalam dokumen Data Tidak Terstruktur \u00b6 Data tidak terstruktur adalah data yang tidak mudah dimasukkan ke dalam model data karena isi/kontennya spesifik atau bervariasi. Salah satu contoh data tidak terstruktur adalah data email. Meskipun email berisi elemen terstruktur seperti pengirim, judul, dan isi teks, terlalu banyak variasi dari isi yang terkandung dalamnya diantaranya dialek bahasa yang dipakai dan sebagainya. Email juga salah satu contoh data bahasa alami Gambar 2.2 Contoh Data email Bahasa Alami \u00b6 Dalam neuropsikologi , linguistik , dan filsafat bahasa , bahasa alami atau bahasa biasa adalah bahasa yang telah berevolusi secara alami pada manusia melalui penggunaan dan pengulangan tanpa perencanaan. Bahasa alami berbeda dengan bahasa yang dibangun untuk memprogramna komputer atau membangun logika nalar. Bahasa alami dikenal sebagai bahasa manusia misal bahasa indonesia, bahasa inggris dan lain lain. Didalam pemrosesan bahasa alami diperluangan pengetahuan ilmu linguistics, semantics, statistics and machine learning.Dengan pemrosesan bahasa alami membantu komputer untuk memahami bahasa yang telah diucapkan oleh manusia Data yang dibangkitkan oleh Mesin \u00b6 Data yang dibangkitkan oleh mesin secara otomatis tanpa intervensi manusia. Data ini terus menerus dibangkitkan selama proses tertentu sedang berjalan. Misalkan data weblog dari mesin server yang dihasilkan dari hasil transaksi user dengan sistem web. Contoh lain adalah data yang dihasilkan dari implementasi internet of things misal perekaman suhu udara dan kelembaban udara dari daerah tertentu yang terhubung dengan pusat penyimpanan data tersebut. Data jaringan atau data berbasis Graph \u00b6 Data graph adalah data yang dinyatakan dengan graph yang dalam matematika mengacu pada konsep teori graph. Data ini menunjukkan keterhubungan antara objek objek atau relasi antar objek objek dengan menggunakan struktur node, edge, dan karakteristik/sifat keterhubungan antar objek tersebut. Salah satu data graph adalah data keterhubungan orang dalam media sosial. Dengan memanfaatkan data graph media sosial kita dapat mengukur ukuran ukuran tertentu berdasarkan struktur yang dibentuknya. Misalkan menentukan pengaruh orang dalam struktur jaringan tersebut, apakah termasuk orang penting/berpengaruh atau bukan. Gambar berikut menunjukkan contoh data graph Gambar 2.3 .Pertemanan dalam media sosial yang dinyataka dengan data graph Database graph dapat digunakan untuk menyimpan data berbasis graph dan mengunakan query tertentu yaitu SPARQL Data Audio,Vidio dan Citra \u00b6 Dengan perkembangan teknologi implementasi multimedia yang sangat pesat saat,data audio,video dan citra cukup besar dihasilkan dari transaksi bisnis. Dengan besarnya data yang dihasilkan membutuhkan proses pengolahan spesifik dari data tersebut untuk dimanfaatkan terutama dalam analisa data sain. Diantara pemanfaatan data mulitimedia tersebut adalah pengenalan objek, pengenala suara, segmentasi citra satelit dan banyak analisa lain yang dihasilkan dari data multimeda tersebut. Data streamming \u00b6 Data Streaming adalah data yang dihasilkan secara terus-menerus oleh ribuan sumber data, yang biasanya mengirimkan catatan data secara bersamaan, dan dalam ukuran kecil (urutan Kilobyte). Data streaming mencakup berbagai macam data seperti logfile yang dihasilkan oleh pelanggan aplikasi seluler atau website Anda, transaksi e-commerce, informasi dari jejaring sosial, data geospasial, dan perangkat sensor yang terhubung atau instrumentasi di pusat data. Data ini perlu diproses secara berurutan dan bertahap secara record-by-record digunakan untuk berbagai macam analisis misalkan korelasi, agregasi, penyaringan, dan pengambilan sampel. Informasi yang diperoleh dari analisis tersebut memberikan petunjuk terhadap pelanggan mereka seperti penggunaan layanan mereka, aktivitas server, klik website, dan lain lain. Misalnya, dalam bisnis kita dapat melacak perubahan sentimen publik pada merek dan produk mereka dengan menganalisis aliran data media sosial, sehingga dapat merespons secara tepat baik waktu dan tindakan yang harus dilakukan Distribusi Data \u00b6 Karakteristik utama dari data adalah distribusi probabilitasnya. Distribusi data yang paling dikenal adalah distribusi normal atau Gaussian. Distribusi ini ditemukan pada sistem fisik dimana data dibangkitkan secara acak. Fungsi dinyatakan dalam bentuk fungsi padat probabilitas(probability density function) $$ f ( x ) = \\frac { 1 } { ( \\sigma \\sqrt { 2 } \\pi ) } \\frac { e ^ { - ( x - \\mu ) ^ { 2 } } } { ( 2 \\sigma ^ { 2 } ) } $$ Dimana \\sigma \\sigma adalah standar deviasi dan \\mu \\mu adalah mean. Persamaan ini menyatakan peluang variable acak dari suatu data x x . Kita menyatakan standar deviasi sebagai lebar kurva lonceng dan rata rata sebagai pusat. Kadangkala istilah variance digunakan dan ini adalah kuadrat dari standar deviasi. Standar deviasi pada dasarnya mengukur bagaimana sebaran data. Untuk memahami lebih jelasnya bagaimana fungsi tersebut digambarkan, berikut implementasinya data dengan distribusi normal yang memiliki rata-rata 1 dan variansinya 0.5 Gambar 2.4. Distribusi Data Statistik Deskriptif \u00b6 Ukuran Kecenderungan Terpusat \u00b6 Rata-rata (Mean) \u00b6 Pada bagian ini, kami melihat cara untuk mengukur kecenderungan pusat data. Misalkan kita mempunyai atribut hasil pretest yang dinyatakan dengan atribut X. Misalkan x_1, x_2, ..., x_N x_1, x_2, ..., x_N menjadi himpunan nilai N yang diamati atau pengamatan untuk X. Di sini, nilai-nilai ini juga dapat disebut set data (untuk X). Jika kita merencanakan pengamatan untuk nilai pretest, di mana sebagian besar nilai berada? Ini memberi kita gambaran tentang kecenderungan pusat dari data. Ukuran kecenderungan pusat data ukurannya adalah rata-rata(mean), median, modus (mode), dan midrange. Atribut numerik yang paling umum dan efektif dari \"pusat\" dari set data adalah mean (aritmatika). Misalkan x_1, x_2, ..., x_N x_1, x_2, ..., x_N menjadi satu set nilai N atau pengamatan, Rata-rata dari nilai pretes dinyatakan dengan \\overline{x}=\\frac{\\sum_{i=1}^{N} x_{i}}{N}=\\frac{x_{1}+x_{2}+\\cdots+x_{N}}{N} \\overline{x}=\\frac{\\sum_{i=1}^{N} x_{i}}{N}=\\frac{x_{1}+x_{2}+\\cdots+x_{N}}{N} Kadang-kadang, setiap nilai x_i x_i dalam satu data dapat dikaitkan dengan bobot w_i w_i untuk i= 1, .., N i= 1, .., N . Bobot tersebut mencerminkan signifikansi, kepentingan, atau frekuensi kejadian yang melekat pada masing masing nilai. Dalam hal ini, kita dapat menghitungnya dengan \\overline{x}=\\frac{\\sum_{i=1}^{N} w_{i} x_{i}}{\\sum_{i=1}^{N} w_{i}}=\\frac{w_{1} x_{1}+w_{2} x_{2}+\\cdots+w_{N} x_{N}}{w_{1}+w_{2}+\\cdots+w_{N}} \\overline{x}=\\frac{\\sum_{i=1}^{N} w_{i} x_{i}}{\\sum_{i=1}^{N} w_{i}}=\\frac{w_{1} x_{1}+w_{2} x_{2}+\\cdots+w_{N} x_{N}}{w_{1}+w_{2}+\\cdots+w_{N}} Meskipun rata-rata adalah jumlah yang sangat berguna untuk menggambarkan kumpulan data, itu tidak selalu cara terbaik untuk mengukur pusat data. Masalah utama dengan mean adalah sensitivitasnya terhadap nilai ekstrim (mis., outlier). Bahkan beberapa nilai ekstrem saja dapat merusak mean. Misalnya, gaji rata-rata di suatu perusahaan mungkin sangat besar didorong oleh beberapa manajer bergaji tinggi. Demikian pula, nilai rata-rata kelas di ujian dapat rata-rata rendah karena beberapa ada beberap skor nilai saja yang sangat rendah. Untuk mengimbangi efek tersebut kita bisa menggunakan rata-rata yang dipangkas (trimmed mean), yang merupakan rata-rata yang diperoleh setelah memangkas nilai paling tinggi dan nilai yang paling rendah. Untuk contoh, kita dapat mengurutkan nilai gaji yang diamati kemudian menghapus 2% atas dan bawah nilai tersebut sebelum menghitung mean. Kita harus menghindari pemotongan bagian yang terlalu besar (seperti 20%) pada kedua ujungnya, karena hal ini dapat mengakibatkan hilangnya informasi yang berharga) Median \u00b6 Untuk data miring (asimetris), ukuran pusat data yang lebih baik adalah median, yang merupakan nilai tengah dalam satu set nilai data yang diurutkan. Ini adalah nilai yang memisahkan separuh data yang lebih tinggi dari data tersebut dan sebagian data yang lebih rendah dari data tersebut. Dalam probabilitas dan statistik, median umumnya berlaku untuk data numerik; namun, kami dapat memperluas konsep menjadi data ordinal. Misalkan kumpulan N data yang diberikan untuk atribut X diurutkan dalam urutan naik. Jika N ganjil, maka median adalah nilai tengah dari data yang ordinal. Jika N adalah genap, maka mediannya tidak unik; dihitung dengan rata rata dari nilai $(\\frac{N}{2}+1) +(\\frac{N}{2}-1) $ Namun pada data berkelompok, dengan data yang berbentuk kelas interval, kita tidak bisa langsung mengetahui nilai median jika kelas mediannya sudah diketahui dengan formula $$ M e=x_{i j}+\\left(\\frac{\\frac{n}{2}-f_{k i j}}{f_{i}}\\right) p $$ \\begin{array}{l}{M e=\\text { median }} \\\\ {x_{i j}=\\text { batas bawah median }} \\\\ {n=\\text { jumlah data }} \\\\ {f_{k i j}=\\text { frekuensi kumulatif data di bawah kelas median }} \\\\ {f_{i}=\\text { frekuensi data pada kelas median }} \\\\ {p=\\text { panjang interval kelas }}\\end{array} \\begin{array}{l}{M e=\\text { median }} \\\\ {x_{i j}=\\text { batas bawah median }} \\\\ {n=\\text { jumlah data }} \\\\ {f_{k i j}=\\text { frekuensi kumulatif data di bawah kelas median }} \\\\ {f_{i}=\\text { frekuensi data pada kelas median }} \\\\ {p=\\text { panjang interval kelas }}\\end{array} Contoh Gambar 2.5. Data berkelompok Mode adalah ukuran lain dari kecenderungan sentral. Mode (modus) untuk satu set data adalah nilai yang paling sering terjadi di set. Oleh karena itu, dapat ditentukan untuk atribut kualitatif dan kuantitatif. Dimungkinkan untuk frekuensi terbesar untuk bersesuaian beberapa nilai berbeda, yang menghasilkan lebih dari satu mode. Kumpulan data dengan satu, dua, atau tiga mode masing-masing disebut unimodal, bimodal, dan trimodal. Jika data hanya mengandung nilai data terjadi hanya sekali, maka tidak ada modus Untuk data numerik unimodal yang cukup miring (asimetris), kami memiliki hubungan empiris: \\text { mean }-\\text { mode } \\approx 3 \\times(\\text { mean }-\\text { median }) \\text { mean }-\\text { mode } \\approx 3 \\times(\\text { mean }-\\text { median }) Ini menyiratkan bahwa mode untuk kurva frekuensi unimodal yang cukup miring dapat dengan mudah didekati jika nilai rata-rata dan median diketahui. Mengukur Sebaran Data \u00b6 Kita sekarang membahas ukuran ukuran untuk menilai dispersi atau penyebaran data numerik. Ukuran-ukuran itu adalah rentang (range), kuantil, kuartil, persentil, dan rentang interkuartil. Semua itu adalah ringkasan lima angka, yang dapat ditunjukkan dengan boxplot, berguna dalam mengidentifikasi pencilan (outlier). Varians dan standar deviasi juga menunjukkan sebaran distribusi data. Rentang (Range), Quartil, and Rentang Interquartile \u00b6 Misalkan x_1, x_2, .. x_N x_1, x_2, .. x_N adalah sekumpulan pengamatan untuk atribut numerik, X X . Rentang adalah selisih antara nilai terbesar (maks ()) dan terkecil (min ()). Misalkan data untuk atribut X diurutkan dalam urutan naik.Bagilah data berdasarkan titik titik tertentu sehingga membagi distribusi data ukuran yang sama, seperti pada Gambar dibawah. Titik data ini disebut kuantil. 2-quantile adalah titik data yang membagi bagian bawah dan atas dari distribusi data. Ini sama dengan median. 4-kuantil adalah tiga titik data yang membagi distribusi data menjadi empat bagian yang sama; setiap bagian mewakili seperempat dari distribusi data. Ini lebih sering disebut sebagai kuartil. 100-kuantil lebih sering disebut sebagai persentil; mereka membagi distribusi data menjadi 100 data berukuran sama. Median, kuartil, dan persentil adalah bentuk kuantil yang paling banyak digunakan. Gambar 2.6. Percentile data Kuartil memberikan gambaran pusat distribus, penyebaran, dan bentuk distribusi. Kuartil satu, dilambangkan oleh Q1, adalah persentil ke-25. Nilai ini menunjukan 25% terendah dari data. Kuartil ketiga, dilambangkan oleh Q3, adalah persentil ke-75 - itu memisahkan data 75% dari terendah data (atau 25% dari tertinggi data. Kuartil kedua adalah persentil ke-50 atau median dari distribusi data. Jarak antara kuartil pertama dan ketiga adalah ukuran yang menyatakan rentang yang dicakup oleh bagian tengah data. Jarak ini disebut rentang interkuartil (IQR) dan dinyatakan dengan I Q R = Q _ { 3 } - Q _ { 1 } I Q R = Q _ { 3 } - Q _ { 1 } Dengan ukuran a kuartil Q1 dan Q3, dan median kita dapat mengidentifikasikan ada tidaknya pencilan (outlier) pada suatu data. Data pencilan atau outlier nilai data biasanya ada di setidaknya 1,5 \u00d7 IQR di atas kuartil ketiga atau di bawah kuartil pertama Karena Q1, median, dan Q3 tidak berisi informasi tentang titik akhir (mis., Ekor) data, ringkasan yang lebih lengkap dari bentuk distribusi dapat diperoleh dengan memberikan nilai data terendah dan tertinggi juga. Ini dikenal sebagai ringkasan lima angka. Ringkasan lima nomor distribusi terdiri dari median (Q2), kuartil Q1 dan Q3, dan data terkecil dan terbesar( Minimum, Q1, Median, Q3, Maksimum) Boxplots adalah cara populer untuk memvisualisasikan distribusi. Boxplot menggabungkan ringkasan lima angka sebagai berikut: - Ujung kotak adalah kuartil dan panjang kotak adalah rentang interkuartil. - Median ditandai dengan garis di dalam kotak. - Dua garis (disebut whiskers) di luar kotak memanjang ke pengamatan terkecil (Minimum) dan terbesar (Maksimum) Outlier biasanya ada di dibawah Q_1 \u2013 1.5 \\times IQR Q_1 \u2013 1.5 \\times IQR dan diatas $ Q_3 + 1.5 \\times IQR$ Gambar 2.7. Boxplot Variansi dan Standar Deviasi \u00b6 Variansi dan standar deviasi adalah ukuran penyebaran data. Nilai-nilai tersebut menunjukkan bagaimana penyebaran distribusi data. Standar Deviasi yang rendah berarti bahwa pengamatan data cenderung sangat dekat dengan rata-rata, sedangkan deviasi standar yang tinggi menunjukkan data tersebar di sejumlah nilai-nilai besar. Varian dari pengamatan N, x_1, x_2, ..., x_N N, x_1, x_2, ..., x_N , untuk atribut numerik X adalah \\sigma ^ { 2 } = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } ( x _ { i } - \\overline { x } ) ^ { 2 } = ( \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } x _ { i } ^ { 2 } ) - \\overline { x } ^ { 2 } \\sigma ^ { 2 } = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } ( x _ { i } - \\overline { x } ) ^ { 2 } = ( \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } x _ { i } ^ { 2 } ) - \\overline { x } ^ { 2 } di mana $ \\overline { x } $ adalah nilai rata-rata dari pengamatan, Standar deviasi,$\\sigma $, dari pengamatan adalah akar kuadrat dari variansi, \\sigma^2 \\sigma^2 Sifat dasar dari standar deviasi, \\sigma \\sigma , sebagai ukuran penyebaran data adalah sebagai berikut: Ukuran \\sigma \\sigma mengeukur sebaran disekitar rata-rata dan harus dipertimbangkan bila rata-rata dipilih sebagai ukuran pusat data \\sigma = 0 \\sigma = 0 hanya jika tidak ada penyebaran data, hanya terjadi ketika semua pengamatan memiliki nilai sama, Jika tidak maka \\sigma > 0 \\sigma > 0 Skewness \u00b6 Derajat distorsi dari kurva lonceng simetris atau distribusi normal. Ini mengukur kurangnya simetri dalam distribusi data Untuk menghitung derajat distorisi dapat menggunakan Koefisien Kemencengan Pearson yang diperoleh dengan menggunakan nilai selisih rata-rata dengan modus dibagi simpangan baku. Koefisien Kemencengan Pearson dirumuskan sebagai berikut s k=\\frac{\\overline{X}-M o}{s} s k=\\frac{\\overline{X}-M o}{s} dengan $$ \\overline{X}-M o \\approx 3(\\overline{X}-M e) $$ maka s k \\approx \\frac{3(\\overline{X}-M e)}{s} s k \\approx \\frac{3(\\overline{X}-M e)}{s} Gambar 2.8. Macam macam Kemiringan data (Skewness) Implementasi \u00b6 Untuk implementasi silahkan unduh data .csv import pandas as pd from scipy import stats df = pd . read_csv ( \"data.csv\" , usecols = [ 0 ]) print ( \"jumlah data \" , df [ 'NilaiPreTest' ] . count ()) print ( \"rata-rata \" , df [ 'NilaiPreTest' ] . mean ()) print ( \"nila minimal \" , df [ 'NilaiPreTest' ] . min ()) print ( \"Q1 \" , df [ 'NilaiPreTest' ] . quantile ( 0.25 )) print ( \"Q2 \" , df [ 'NilaiPreTest' ] . quantile ( 0.5 )) print ( \"Q3 \" , df [ 'NilaiPreTest' ] . quantile ( 0.75 )) print ( \"Nilai Max \" , df [ 'NilaiPreTest' ] . max ()) print ( \"kemencengan\" , \" {0:.2f} \" . format ( round ( df [ 'NilaiPreTest' ] . skew (), 2 ))) mode = stats . mode ( df ) print ( \"Nilai modus {} dengan jumlah {} \" . format ( mode . mode [ 0 ], mode . count [ 0 ])) print ( \"kemencengan \" , \" {0:.6f} \" . format ( round ( df [ 'NilaiPreTest' ] . skew (), 6 ))) print ( \"Standar Deviasi \" , \" {0:.2f} \" . format ( round ( df [ 'NilaiPreTest' ] . std (), 2 ))) print ( \"Variansi \" , \" {0:.2f} \" . format ( round ( df [ 'NilaiPreTest' ] . var (), 2 ))) Mengukur Jarak Data \u00b6 Mengukur Jarak Tipe Numerik \u00b6 Shirkhorshidi, A. S., Aghabozorgi, S., & Wah, T. Y. (2015). A comparison study on similarity and dissimilarity measures in clustering continuous data. PloS one, 10(12), e0144059. Salah satu tantangan dalam era ini dengan datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan Sebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaiut v_1, v_2 v_1, v_2 menyatakandua vektor yang menyatakan v_1 = {x_1, x_2, . . ., x_n}, v_2 ={y_1, y_2, . . ., y_n}, v_1 = {x_1, x_2, . . ., x_n}, v_2 ={y_1, y_2, . . ., y_n}, dimana x_i, y_i x_i, y_i disebut attribut. Ada beberapa ukuran similaritas datau ukuran jarak, diantaranya Minkowski Distance \u00b6 Kelompk Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski distance. Minkowski distance dinyatakan dengan d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 diman m m adalah bilangan riel positif dan x_i x_i dan $ y_i$ adalah dua vektor dalam runang dimensi n n Implementasi ukuran jarak Minkowski pada model clustering data atribut dilakukan normalisasi untuk menghindari dominasi dari atribut yang memiliki skala data besar. Manhattan distance \u00b6 Manhattan distance adalah kasus khsusu dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. BIla ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| Euclidean distance \u00b6 Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini. Average Distance \u00b6 Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adala versi modikfikasid ari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x,y x,y dalam ruang dimensi n n , rata-rata jarak didefinisikan dengan d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } } d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } } Weighted euclidean distance \u00b6 Jika berdasarkan tingkatan penting dari masing masing atribut ditentukan, maka Weighted Euclidean distance adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. Ukuran ini dirumuskan dengan $$ d _ { w e } = \\left ( \\sum _ { i = 1 } ^ { n } w _ { i } ( x _ { i } - y _ { i } \\right) ^ { 2 } ) ^ { \\frac { 1 } { 2 } } $$ dimana w_i w_i adalah bobot yang diberikan pada atribut ke i. Chord distance \u00b6 Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan d _ { \\text {chord} } = \\left ( 2 - 2 \\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { \\| x \\| _ { 2 } \\| y \\| _ { 2 } } \\right) ^ { \\frac { 1 } { 2 } } d _ { \\text {chord} } = \\left ( 2 - 2 \\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { \\| x \\| _ { 2 } \\| y \\| _ { 2 } } \\right) ^ { \\frac { 1 } { 2 } } dimana \\| x \\|_ {2} \\| x \\|_ {2} adalah L^{2} \\text {-norm} \\| x \\|_{2} = \\sqrt { \\sum_{ i = 1 }^{ n }x_{i}^{2}} L^{2} \\text {-norm} \\| x \\|_{2} = \\sqrt { \\sum_{ i = 1 }^{ n }x_{i}^{2}} Mahalanobis distance \u00b6 Mahalanobis distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antra data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. Mahalanobis distance dinyatakan dengan d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } diman S S adalah matrik covariance data. Cosine measure \u00b6 Ukuran Cosine similarity lebih banyak digunakan dalam similaritas dokumen dan dinyatakan dengan Cosine(x,y)=\\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { \\| x \\| _ { 2 } \\| y \\| _ { 2 } } Cosine(x,y)=\\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { \\| x \\| _ { 2 } \\| y \\| _ { 2 } } dimana \\|y\\|_{2} \\|y\\|_{2} adalah Euclidean norm dari vektor y=(y_{1} , y_{2} , \\dots , y_{n} ) y=(y_{1} , y_{2} , \\dots , y_{n} ) didefinisikan dengan \\|y\\|_{2}=\\sqrt{ y _ { 1 } ^ { 2 } + y _ { 2 } ^ { 2 } + \\ldots + y _ { n } ^ { 2 } } \\|y\\|_{2}=\\sqrt{ y _ { 1 } ^ { 2 } + y _ { 2 } ^ { 2 } + \\ldots + y _ { n } ^ { 2 } } Pearson correlation \u00b6 Pearson correlation banyak digunakan dalam data expresi gen. Ukuran similaritas ini menghitung similaritas antara duan bentuk pola expresi gen. Pearson correlation didefinisikan dengan Pearson ( x , y ) = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\mu _ { x } ) ( y _ { i } - \\mu _ { y } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } } Pearson ( x , y ) = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\mu _ { x } ) ( y _ { i } - \\mu _ { y } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } } The Pearson correlation kelemahannya adalah sensitif terhadap outlier Mengukur Jarak Atribut Binary \u00b6 Mari kita lihat similaritas dan desimilirity untuk objek yang dijelaskan oleh atribut biner simetris atau asimetris. Aatribut biner hanya memiliki dua status: 0 dan 1 Contoh atribut perokok menggambarkan seorang pasien, misalnya, 1 menunjukkan bahwa pasien merokok, sedangkan 0 menunjukkan pasien tidak merokok. Memperlakukan atribut biner sebagai atribut numerik tidak diperkenankan. Oleh karena itu, metode khusus untuk data biner diperlukan untuk membedakan komputasi. Jadi, bagaimana kita bisa menghitung ketidaksamaan antara dua atribut biner? \u201dSatu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2 \\times 2 2 \\times 2 di mana q q adalah jumlah atribut yang sama dengan 1 untuk kedua objek i i dan j j , r r adalah jumlah atribut yang sama dengan 1 untuk objek i i tetapi 0 untuk objek j j , s s adalah jumlah atribut yang sama dengan 0 untuk objek i i tetapi 1 untuk objek j j , dan t t adalah jumlah atribut yang sama dengan 0 untuk kedua objek i i dan j j . Jumlah total atribut adalah p p , di mana p=q+r+s+t p=q+r+s+t Ingatlah bahwa untuk atribut biner simetris, masing-masing nilai bobot yang sama.Dissimilarity yang didasarkan pada atribut aymmetric binary disebut symmetric binary dissimilarity. Jika objek i dan j dinyatakan sebagai atribut biner simetris, maka dissimilarity antar i i dan j j adalah d ( i , j ) = \\frac { r + s } { q + r + s + t } d ( i , j ) = \\frac { r + s } { q + r + s + t } Untuk atribut biner asimetris, kedua kondisi tersebut tidak sama pentingnya, seperti hasil positif (1) dan negatif (0) dari tes penyakit. Diberikan dua atribut biner asimetris, pencocokan keduanya 1 (kecocokan positif) kemudian dianggap lebih signifikan daripada kecocokan negatif. Ketidaksamaan berdasarkan atribut-atribut ini disebut asimetris biner dissimilarity, di mana jumlah kecocokan negatif, t, dianggap tidak penting dan dengan demikian diabaikan. Berikut perhitungannya d ( i , j ) = \\frac { r + s } { q + r + s } d ( i , j ) = \\frac { r + s } { q + r + s } Kita dapat mengukur perbedaan antara dua atribut biner berdasarkan pada disimilarity. Misalnya, biner asimetris kesamaan antara objek i i dan j j dapat dihitung dengan \\operatorname { sim } ( i , j ) = \\frac { q } { q + r + s } = 1 - d ( i , j ) \\operatorname { sim } ( i , j ) = \\frac { q } { q + r + s } = 1 - d ( i , j ) Persamaan similarity ini disebut dengan Jaccard coefficient Mengukur Jarak Tipe categorical \u00b6 Li, C., & Li, H. (2010). A Survey of Distance Metrics for Nominal Attributes. JSW, 5(11), 1262-1269. Overlay Metric \u00b6 Ketika semua atribut adalah bertipe nominal, ukuran jarak yang paling sederhana adalah dengan Ovelay Metric (OM) yang dinyatakan dengan d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\delta ( a _ { i } ( x ) , a _ { i } ( y ) ) d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\delta ( a _ { i } ( x ) , a _ { i } ( y ) ) dimana n n adalah banyaknya atribut, a_i(x) a_i(x) dan a_i(y) a_i(y) adalah nilai atribut ke i i yaitu A_i A_i dari masing masing objek x x dan y y , \\delta \\ ( a_{ i } ( x ) , a_{ i } ( y ) ) \\delta \\ ( a_{ i } ( x ) , a_{ i } ( y ) ) adalah 0 jika a _ { i } ( x ) = a _ { i } ( y ) a _ { i } ( x ) = a _ { i } ( y ) dan 1 jika sebaliknya. OM banyak digunakan oleh instance-based learning dan locally weighted learning. Jelas sekali , ini sedikit beruk untuk mengukur jarak antara masing-masing pasangan sample, karena gagal memanfaatkan tambahan informasi yang diberikan oleh nilai atribut nominal yang bisa membantu dalam generalisasi. Value Difference Metric (VDM) \u00b6 VDM dikenalkan oleh Standfill and Waltz, versi sederhana dari VDM tanpa skema pembobotan didefinsisikan dengan d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\sum _ { c = 1 } ^ { C } \\left| P ( c | a _ { i } ( x ) ) - P ( c | a _ { i } ( y ) ) \\right | d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\sum _ { c = 1 } ^ { C } \\left| P ( c | a _ { i } ( x ) ) - P ( c | a _ { i } ( y ) ) \\right | dimana C C adalah banyaknya kelas, P(c|a_i(x)) P(c|a_i(x)) adalah probabilitas bersyarat dimana kelas x x adalah c c dari atribut A_i A_i , yang memiliki nilai a_i(x) a_i(x) , P(c|a_i(y)) P(c|a_i(y)) adalah probabilitas bersyarat dimana kelas y y adalah c c dengan atribut A_i A_i memiliki nilai a_i(y) a_i(y) VDM mengasumsikan bahwa dua nilai dari atribut adalah lebih dekat jika memiliki klasifikasi sama. Pendekatan lain berbasi probabilitas adalah SFM (Short and Fukunaga Metric) yang kemudian dikembangkan oleh Myles dan Hand dan didefinisikan dengan d ( x , y ) = \\sum _ { c = 1 } ^ { C } \\left | P ( c | x ) - P ( c | y ) \\right| d ( x , y ) = \\sum _ { c = 1 } ^ { C } \\left | P ( c | x ) - P ( c | y ) \\right| diman probabilitas keanggotaan kelas diestimasi dengan P(c|x) P(c|x) dan P(c|y) P(c|y) didekati dengan Naive Bayes, Minimum Risk Metric (MRM) \u00b6 Ukuran ini dipresentasikan oleh Blanzieri and Ricci, berbeda dari SFM yaitu meminimumkan selisih antara kesalahan berhingga dan kesalahan asymtotic. MRM meminimumkan risk of misclassification yang didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } P ( c | x ) ( 1 - P ( c | y ) ) $$ Mengukur Jarak Tipe Ordinal \u00b6 Han, J., Pei, J., & Kamber, M. (2011). Data mining: concepts and techniques. Elsevier . Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. Kategori-kategori ini disusun dalam peringkat. Yaitu, rentang atribut numerik dapat dipetakan ke atribut ordinal f f yang memiliki M_f M_f state. Misalnya, kisaran suhu atribut skala-skala (dalam Celcius)dapat diatur ke dalam status berikut: \u221230 hingga \u221210, \u221210 hingga 10, 10 hingga 30, masing-masing mewakili kategori suhu dingin, suhu sedang, dan suhu hangat. M M adalah jumlah keadaan yang dapat dilakukan oleh atribut ordinalmemiliki. State ini menentukan peringkat 1, ..., M_f 1, ..., M_f Perlakuan untuk atribut ordinal adalah cukup sama dengan atribut numerik ketika menghitung disimilarity antara objek. Misalkan f f adalah atribut-atribut dari atribut ordinal dari n n objek. Menghitung disimilarity terhadap f fitur sebagai berikut: Nilai f f untuk objek ke- i i adalah x_{if} x_{if} , dan f f memiliki M_f M_f status urutan , mewakili peringkat 1, .., M_f 1, .., M_f Ganti setiap x_{if} x_{if} dengan peringkatnya, r_{if} \\in \\{1...M_f\\} r_{if} \\in \\{1...M_f\\} Karena setiap atribut ordinal dapat memiliki jumlah state yang berbeda, diperlukan untuk memetakan rentang setiap atribut ke [0,0, 1.0] sehingga setiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data dengan mengganti peringkat r_{if} r_{if} dengan $$ z _ { i f } = \\frac { r _ { i f } - 1 } { M _ { f } - 1 } $$ Dissimilarity kemudian dihitung dengan menggunakan ukuran jarak seperti atribut numerik dengan data yang baru setelah ditransformasi $ z _ { i f }$ Menghitung Jarak Tipe Campuran \u00b6 Wilson, D. R., & Martinez, T. R. (1997). Improved heterogeneous distance functions. Journal of artificial intelligence research, 6, 1-34. Menghitung ketidaksamaan antara objek dengan atribut campuran yang berupa nominal, biner simetris, biner asimetris, numerik, atau ordinal yang ada pada kebanyakan databasae dapat dinyatakan dengan memproses semua tipe atribut secara bersamaan. Salah satu teknik tersebut menggabungkan atribut yang berbeda ke dalam matriks ketidaksamaan tunggal dan menyatakannya dengan skala interval antar [0,0, 1.0] [0,0, 1.0] . Misalkan data berisi atribut p p tipe campuran. Ketidaksamaan (disimilarity ) antara objek i i dan j j dinyatakan dengan d ( i , j ) = \\frac { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } d _ { i j } ^ { ( f ) } } { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } } d ( i , j ) = \\frac { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } d _ { i j } ^ { ( f ) } } { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } } dimana \\delta_{ij}^{f}=0 \\delta_{ij}^{f}=0 - jika x_{if} x_{if} atau x_{jf} x_{jf} adalah hilang (i.e., tidak ada pengukuran dari atribut f untuk objek i i atau objek j j ) jika x_{if}=x_{jf}=0 x_{if}=x_{jf}=0 dan atribut f f adalah binary asymmetric, selain itu \\delta_{ij}^{f}=1 \\delta_{ij}^{f}=1 Kontribusi dari atribut f f untuk dissimilarity antara i dan j (yaitu. d_{ij}^{f} d_{ij}^{f} ) dihitung bergantung pada tipenya, Jika f f adalah numerik, d_{ij}^{f}=\\frac{ \\|x _{if}-x_{jf}\\|}{max_hx_{hf}-min_hx{hf}} d_{ij}^{f}=\\frac{ \\|x _{if}-x_{jf}\\|}{max_hx_{hf}-min_hx{hf}} , di mana h menjalankan semua nilai objek yang tidak hilang untuk atribut f Jika f f adalah nominal atau binary,$d_{ij}^{f}=0 $jika x_{if}=x_{jf} x_{if}=x_{jf} , sebaliknya d_{ij}^{f}=1 d_{ij}^{f}=1 Jika f f adalah ordinal maka hitung rangking r_{if} r_{if} dan \\mathcal z_{if}=\\frac {r_{if}-1}{M_f-1} \\mathcal z_{if}=\\frac {r_{if}-1}{M_f-1} , dan perlakukan z_{if} z_{if} sebagai numerik.","title":"Memahami Data"},{"location":"memahami-data/#memahami-data","text":"","title":"Memahami Data"},{"location":"memahami-data/#macam-macam-data","text":"Dalam data data mining dan maha datar, Anda akan menemukan banyak jenis data yang berbeda, dan masing-masing cenderung membutuhkan alat dan teknik yang berbeda. Macam macam data dikelompokkan sebagai berikut: Data terstruktur (structured) Data tidak terstruktur(unstructured Data bahasa alami(Natural Language) Data yang dibangkit oleh Mesin (Machined-Generated) Data Audio, Video,Citra Data Streamming Data berbasis Graph(Graph-based)","title":"Macam macam Data"},{"location":"memahami-data/#data-terstruktur","text":"Data terstruktur adalah data yang bergantung pada model data dan yang dinyatakan dalam bentuk tabel dengan atribut} (kolom) dan baris. Data terstruktur mudah disimpan dalam database dalam bentuk tabel atau file excel (Ms Office), SQl (structure Query Language)sehingga mudah dilakukan query terhadap data tersebut. Tetapi realitanya banyak data yang ada dalam dalam bentuk data tidak terstruktur karena data dihasilkan oleh manusia dan mesin Gambar 2.1 Contoh data terstruktur","title":"Data Terstruktur"},{"location":"memahami-data/#macam-macam-atribut","text":"Atribut adalah data yang mewakili karakteristik atau fitur dari objek data. Atribut bisa disebut juga dengan dimensi, fitur, dan variabel yang istilah itu sering digunakan literatur. Dimensi istilah yang biasanya digunakan dalam data warehouse. Dalam literatur pembelajaran mesin cenderung menggunakan istilah fitur, sementara dalam bidang statistik lebih memilih menggunakan istilah variabel. Dalam penambangan data atau data miniing dan database biasa menggunakan istilah atribut atau fitur , dan dalam buku ini juga menggunakan istilah atribut atau fitur. Contoh atribut-atribut yang menggambarkan objek pelanggan dapat mencakup, misalnya ID pelanggan, nama, dan alamat. Nilai yang diamati untuk atribut tertentu dikenal sebagai nilai observasi. Sekumpulan atribut yang digunakan untuk menggambarkan objek disebut disebut dengan vektor atribut (atau vektor fitur. Distribusi data yang melibatkan satu atribut (atau variabel) disebut univariat. Distribusi bivariat melibatkan dua atribut, dan seterusnya. Jenis atribut ditentukan oleh nilai-nilai pada atribut tersebut yang mungkin nominal, biner,atau ordinal, atau numerik. Pada subbagian berikut, kami perkenalkan nilai nilai tersebut","title":"Macam- macam atribut"},{"location":"memahami-data/#macam-macam-tipe-data-atribut","text":"Atribut Nominal Nilai atribut nominal adalah simbol ataunama barang. Setiap nilai mewakili beberapa jenis kategori, kode, atau status, dan Atribut nominal juga disebut kategori. Nilai-nilainya tidak memiliki tingkatan nilai. Dalam ilmu komputer, nilainya juga dikenal sebagai enumerasi Contoh : Misalkan warna rambut dan status perkawinan adalah dua atribut dari data orang. Nilai yang mungkin untuk warna rambut adalah hitam, coklat, pirang, merah, hitam pucat, abu-abu, dan putih. Status perkawinan memiliki nilai atribut lajang, menikah, bercerai, dan janda. Baik warna rambut maupun status perkawinan adalah atribut nominal. Contoh lain dari atribut nominal adalah atribut pekerjaan dengan nilai-nilainya adalah guru, dokter gigi, programmer, petani, dan sebagainya Atribut Biner Atribut biner adalah atribut nominal dengan hanya dua kategori atau status: 0 atau 1, di mana 0 biasanya berarti atribut itu tidak ada, dan 1 berarti itu ada. Atribut Biner disebut sebagai Boolean jika dinyatakan dengan benar (true) dan salah(false) Contoh : Terdapat atribut yang menggambarkan merokok pada pasien, 1 menunjukkan bahwa pasien merokok,sementara 0 menunjukkan bahwa pasien tidak merokok. Demikian pula, seandainya ada pasien menjalani tes medis yang memiliki dua kemungkinan hasil. Atribut Tes medis bersifat biner, dengan nilai 1 berarti hasil tes untuk pasien positif, sedangkan 0 berarti hasilnya negatif. Atribut biner simetris jika keduanya emiliki nilai bobot yang sama; Artinya, tidak ada kekhususan mengenai hasil mana yang harus dikodekan sebagai 0 atau 1. Misalkan atribut gender yang dengan nila atributnya laki dan perempuan. Atribut biner adalah asimetris jika hasil dari nilai nilainya tidak sama pentingnya seperti hasil positif dan negatif dari tes medis untuk HIV. Dengan mengkodekan hasil yang paling penting, biasanya 1 (mis., HIV positif) dan yang lainnya dengan 0 (mis., HIV negatif) Atribut ordinal Atribut ordinal adalah atribut dengan nilai yang memiliki arti urutan atau peringkat di antara nilai nilai yang ada, tapi besarnya nilai yang berurutan tersebut tidak diketahui. Ukuran kecenderungan terpusat dari atribut ordinal dapat diwakili oleh modus dan median median (nilai tengah), tetapi tidak untuk nilai rata-rata.Perlu diperhatikan bahwa atribut nominal, biner, dan ordinal bersifat kualitatif. Artinya, atribut-atribut tersebut hanya menjelaskan sebuah fitur dari suatu objek tanpa memberikan ukuran atau kuantitas yang sebenarnya. Nilai-nilai atribut kualitatif biasanya merupakan katakata yang mewakili kategori Contoh : Atribut ordinal Misalkan ukuran minuman yang tersedia di sebuah restoran cepat saji. Atribut nominal ini memiliki tiga nilai yang mungkin: kecil, sedang, dan besar. Nilai memiliki arti urutan yang (yang sesuai dengan ukuran minuman). Contoh atribut ordinal lainnya adalah pangkat dan jabatan profesi. Atribut ordinal berguna untuk melakukan penilaian subjektif terhadap kualitas sesuatu objek yang tidak dapat diukur secara obyektif; atribut ordinal sering digunakan dalam survei untuk peringkat. Dalam satu survei, para peserta diminta untuk menilai tingkat kepuasan mereka sebagai pelanggan.Kepuasan pelanggan memiliki kategori ordinal berikut ini: 0: sangat tidak puas,1: agak tidak puas, 2: netral, 3: puas, dan 4: sangat puas. Atribut ordinal juga dapat diperoleh dari iskritisasi nilai atribut numerik dengan membagi rentang nilai menjadi urutan kategoria Atribut Numerik Atribut numerik bersifat kuantitatif; Artinya, ini adalah kuantitas yang terukur, yang dinyatakan dengan bilangan bulat atau nilai riel. Atribut numerik dapat Atribut Skala Interval(interval-scaled) atau skala ration (ratio-scaled) Atribut skala interval diukur pada dengan skala unit ukuran yang sama. Nilai - nilai Interval berskala memiliki urutan dan bisa positif, 0, atau negatif. Jadi, selain untuk memberikan peringkat nilai, atribut semacam itu memungkinkan kita untuk membandingkan dan mengukur perbedaan antar nilai Contoh : Atribut suhu adalah Skala interval. Misalkan kita memiliki nilai suhu di luar ruangan untuk beberapa hari yang berbeda dari suatu objek. Dengan mengurutkan nilai, kita mendapatkan peringkat objek yang berkenaan dengan suhu. Selain itu, kita bisa mengukur perbedaan antara nilai.Misalnya, a suhu 20o C adalah lima derajat lebih tinggi dari suhu 15oC. Contoh lain kalender tahun adalah. Misalnya, tahun 2002 dan 2010 terpisah delapan tahun. Karena atribut skala interval adalah numerik, kita dapat menghitung nilai ratarata, ukuran median dan modus dari kecenderungan terpusat Atribut Skala Ratio Atribut skala rasio adalah atribut numerik dengan melekat titik nol pada nilai atribut tersebut. Artinya, jika pengukuran adalah berskala rasio, kita dapat dapat mengatakan berapa kali dari nilai yang lain atau rasio dari nilai yang lain. Selain itu, nilai yang dipesan, dan kita juga bisa menghitung selisih antara nilai, serta mean, median, dan modus Contoh Atribut tentang pengukuran berat badan, tinggi badan, jumlah kata dalam dokumen","title":"Macam macam tipe data atribut"},{"location":"memahami-data/#data-tidak-terstruktur","text":"Data tidak terstruktur adalah data yang tidak mudah dimasukkan ke dalam model data karena isi/kontennya spesifik atau bervariasi. Salah satu contoh data tidak terstruktur adalah data email. Meskipun email berisi elemen terstruktur seperti pengirim, judul, dan isi teks, terlalu banyak variasi dari isi yang terkandung dalamnya diantaranya dialek bahasa yang dipakai dan sebagainya. Email juga salah satu contoh data bahasa alami Gambar 2.2 Contoh Data email","title":"Data Tidak Terstruktur"},{"location":"memahami-data/#bahasa-alami","text":"Dalam neuropsikologi , linguistik , dan filsafat bahasa , bahasa alami atau bahasa biasa adalah bahasa yang telah berevolusi secara alami pada manusia melalui penggunaan dan pengulangan tanpa perencanaan. Bahasa alami berbeda dengan bahasa yang dibangun untuk memprogramna komputer atau membangun logika nalar. Bahasa alami dikenal sebagai bahasa manusia misal bahasa indonesia, bahasa inggris dan lain lain. Didalam pemrosesan bahasa alami diperluangan pengetahuan ilmu linguistics, semantics, statistics and machine learning.Dengan pemrosesan bahasa alami membantu komputer untuk memahami bahasa yang telah diucapkan oleh manusia","title":"Bahasa Alami"},{"location":"memahami-data/#data-yang-dibangkitkan-oleh-mesin","text":"Data yang dibangkitkan oleh mesin secara otomatis tanpa intervensi manusia. Data ini terus menerus dibangkitkan selama proses tertentu sedang berjalan. Misalkan data weblog dari mesin server yang dihasilkan dari hasil transaksi user dengan sistem web. Contoh lain adalah data yang dihasilkan dari implementasi internet of things misal perekaman suhu udara dan kelembaban udara dari daerah tertentu yang terhubung dengan pusat penyimpanan data tersebut.","title":"Data yang dibangkitkan oleh Mesin"},{"location":"memahami-data/#data-jaringan-atau-data-berbasis-graph","text":"Data graph adalah data yang dinyatakan dengan graph yang dalam matematika mengacu pada konsep teori graph. Data ini menunjukkan keterhubungan antara objek objek atau relasi antar objek objek dengan menggunakan struktur node, edge, dan karakteristik/sifat keterhubungan antar objek tersebut. Salah satu data graph adalah data keterhubungan orang dalam media sosial. Dengan memanfaatkan data graph media sosial kita dapat mengukur ukuran ukuran tertentu berdasarkan struktur yang dibentuknya. Misalkan menentukan pengaruh orang dalam struktur jaringan tersebut, apakah termasuk orang penting/berpengaruh atau bukan. Gambar berikut menunjukkan contoh data graph Gambar 2.3 .Pertemanan dalam media sosial yang dinyataka dengan data graph Database graph dapat digunakan untuk menyimpan data berbasis graph dan mengunakan query tertentu yaitu SPARQL","title":"Data jaringan atau data berbasis Graph"},{"location":"memahami-data/#data-audiovidio-dan-citra","text":"Dengan perkembangan teknologi implementasi multimedia yang sangat pesat saat,data audio,video dan citra cukup besar dihasilkan dari transaksi bisnis. Dengan besarnya data yang dihasilkan membutuhkan proses pengolahan spesifik dari data tersebut untuk dimanfaatkan terutama dalam analisa data sain. Diantara pemanfaatan data mulitimedia tersebut adalah pengenalan objek, pengenala suara, segmentasi citra satelit dan banyak analisa lain yang dihasilkan dari data multimeda tersebut.","title":"Data Audio,Vidio dan Citra"},{"location":"memahami-data/#data-streamming","text":"Data Streaming adalah data yang dihasilkan secara terus-menerus oleh ribuan sumber data, yang biasanya mengirimkan catatan data secara bersamaan, dan dalam ukuran kecil (urutan Kilobyte). Data streaming mencakup berbagai macam data seperti logfile yang dihasilkan oleh pelanggan aplikasi seluler atau website Anda, transaksi e-commerce, informasi dari jejaring sosial, data geospasial, dan perangkat sensor yang terhubung atau instrumentasi di pusat data. Data ini perlu diproses secara berurutan dan bertahap secara record-by-record digunakan untuk berbagai macam analisis misalkan korelasi, agregasi, penyaringan, dan pengambilan sampel. Informasi yang diperoleh dari analisis tersebut memberikan petunjuk terhadap pelanggan mereka seperti penggunaan layanan mereka, aktivitas server, klik website, dan lain lain. Misalnya, dalam bisnis kita dapat melacak perubahan sentimen publik pada merek dan produk mereka dengan menganalisis aliran data media sosial, sehingga dapat merespons secara tepat baik waktu dan tindakan yang harus dilakukan","title":"Data streamming"},{"location":"memahami-data/#distribusi-data","text":"Karakteristik utama dari data adalah distribusi probabilitasnya. Distribusi data yang paling dikenal adalah distribusi normal atau Gaussian. Distribusi ini ditemukan pada sistem fisik dimana data dibangkitkan secara acak. Fungsi dinyatakan dalam bentuk fungsi padat probabilitas(probability density function) $$ f ( x ) = \\frac { 1 } { ( \\sigma \\sqrt { 2 } \\pi ) } \\frac { e ^ { - ( x - \\mu ) ^ { 2 } } } { ( 2 \\sigma ^ { 2 } ) } $$ Dimana \\sigma \\sigma adalah standar deviasi dan \\mu \\mu adalah mean. Persamaan ini menyatakan peluang variable acak dari suatu data x x . Kita menyatakan standar deviasi sebagai lebar kurva lonceng dan rata rata sebagai pusat. Kadangkala istilah variance digunakan dan ini adalah kuadrat dari standar deviasi. Standar deviasi pada dasarnya mengukur bagaimana sebaran data. Untuk memahami lebih jelasnya bagaimana fungsi tersebut digambarkan, berikut implementasinya data dengan distribusi normal yang memiliki rata-rata 1 dan variansinya 0.5 Gambar 2.4. Distribusi Data","title":"Distribusi Data"},{"location":"memahami-data/#statistik-deskriptif","text":"","title":"Statistik Deskriptif"},{"location":"memahami-data/#ukuran-kecenderungan-terpusat","text":"","title":"Ukuran Kecenderungan Terpusat"},{"location":"memahami-data/#rata-rata-mean","text":"Pada bagian ini, kami melihat cara untuk mengukur kecenderungan pusat data. Misalkan kita mempunyai atribut hasil pretest yang dinyatakan dengan atribut X. Misalkan x_1, x_2, ..., x_N x_1, x_2, ..., x_N menjadi himpunan nilai N yang diamati atau pengamatan untuk X. Di sini, nilai-nilai ini juga dapat disebut set data (untuk X). Jika kita merencanakan pengamatan untuk nilai pretest, di mana sebagian besar nilai berada? Ini memberi kita gambaran tentang kecenderungan pusat dari data. Ukuran kecenderungan pusat data ukurannya adalah rata-rata(mean), median, modus (mode), dan midrange. Atribut numerik yang paling umum dan efektif dari \"pusat\" dari set data adalah mean (aritmatika). Misalkan x_1, x_2, ..., x_N x_1, x_2, ..., x_N menjadi satu set nilai N atau pengamatan, Rata-rata dari nilai pretes dinyatakan dengan \\overline{x}=\\frac{\\sum_{i=1}^{N} x_{i}}{N}=\\frac{x_{1}+x_{2}+\\cdots+x_{N}}{N} \\overline{x}=\\frac{\\sum_{i=1}^{N} x_{i}}{N}=\\frac{x_{1}+x_{2}+\\cdots+x_{N}}{N} Kadang-kadang, setiap nilai x_i x_i dalam satu data dapat dikaitkan dengan bobot w_i w_i untuk i= 1, .., N i= 1, .., N . Bobot tersebut mencerminkan signifikansi, kepentingan, atau frekuensi kejadian yang melekat pada masing masing nilai. Dalam hal ini, kita dapat menghitungnya dengan \\overline{x}=\\frac{\\sum_{i=1}^{N} w_{i} x_{i}}{\\sum_{i=1}^{N} w_{i}}=\\frac{w_{1} x_{1}+w_{2} x_{2}+\\cdots+w_{N} x_{N}}{w_{1}+w_{2}+\\cdots+w_{N}} \\overline{x}=\\frac{\\sum_{i=1}^{N} w_{i} x_{i}}{\\sum_{i=1}^{N} w_{i}}=\\frac{w_{1} x_{1}+w_{2} x_{2}+\\cdots+w_{N} x_{N}}{w_{1}+w_{2}+\\cdots+w_{N}} Meskipun rata-rata adalah jumlah yang sangat berguna untuk menggambarkan kumpulan data, itu tidak selalu cara terbaik untuk mengukur pusat data. Masalah utama dengan mean adalah sensitivitasnya terhadap nilai ekstrim (mis., outlier). Bahkan beberapa nilai ekstrem saja dapat merusak mean. Misalnya, gaji rata-rata di suatu perusahaan mungkin sangat besar didorong oleh beberapa manajer bergaji tinggi. Demikian pula, nilai rata-rata kelas di ujian dapat rata-rata rendah karena beberapa ada beberap skor nilai saja yang sangat rendah. Untuk mengimbangi efek tersebut kita bisa menggunakan rata-rata yang dipangkas (trimmed mean), yang merupakan rata-rata yang diperoleh setelah memangkas nilai paling tinggi dan nilai yang paling rendah. Untuk contoh, kita dapat mengurutkan nilai gaji yang diamati kemudian menghapus 2% atas dan bawah nilai tersebut sebelum menghitung mean. Kita harus menghindari pemotongan bagian yang terlalu besar (seperti 20%) pada kedua ujungnya, karena hal ini dapat mengakibatkan hilangnya informasi yang berharga)","title":"Rata-rata (Mean)"},{"location":"memahami-data/#median","text":"Untuk data miring (asimetris), ukuran pusat data yang lebih baik adalah median, yang merupakan nilai tengah dalam satu set nilai data yang diurutkan. Ini adalah nilai yang memisahkan separuh data yang lebih tinggi dari data tersebut dan sebagian data yang lebih rendah dari data tersebut. Dalam probabilitas dan statistik, median umumnya berlaku untuk data numerik; namun, kami dapat memperluas konsep menjadi data ordinal. Misalkan kumpulan N data yang diberikan untuk atribut X diurutkan dalam urutan naik. Jika N ganjil, maka median adalah nilai tengah dari data yang ordinal. Jika N adalah genap, maka mediannya tidak unik; dihitung dengan rata rata dari nilai $(\\frac{N}{2}+1) +(\\frac{N}{2}-1) $ Namun pada data berkelompok, dengan data yang berbentuk kelas interval, kita tidak bisa langsung mengetahui nilai median jika kelas mediannya sudah diketahui dengan formula $$ M e=x_{i j}+\\left(\\frac{\\frac{n}{2}-f_{k i j}}{f_{i}}\\right) p $$ \\begin{array}{l}{M e=\\text { median }} \\\\ {x_{i j}=\\text { batas bawah median }} \\\\ {n=\\text { jumlah data }} \\\\ {f_{k i j}=\\text { frekuensi kumulatif data di bawah kelas median }} \\\\ {f_{i}=\\text { frekuensi data pada kelas median }} \\\\ {p=\\text { panjang interval kelas }}\\end{array} \\begin{array}{l}{M e=\\text { median }} \\\\ {x_{i j}=\\text { batas bawah median }} \\\\ {n=\\text { jumlah data }} \\\\ {f_{k i j}=\\text { frekuensi kumulatif data di bawah kelas median }} \\\\ {f_{i}=\\text { frekuensi data pada kelas median }} \\\\ {p=\\text { panjang interval kelas }}\\end{array} Contoh Gambar 2.5. Data berkelompok Mode adalah ukuran lain dari kecenderungan sentral. Mode (modus) untuk satu set data adalah nilai yang paling sering terjadi di set. Oleh karena itu, dapat ditentukan untuk atribut kualitatif dan kuantitatif. Dimungkinkan untuk frekuensi terbesar untuk bersesuaian beberapa nilai berbeda, yang menghasilkan lebih dari satu mode. Kumpulan data dengan satu, dua, atau tiga mode masing-masing disebut unimodal, bimodal, dan trimodal. Jika data hanya mengandung nilai data terjadi hanya sekali, maka tidak ada modus Untuk data numerik unimodal yang cukup miring (asimetris), kami memiliki hubungan empiris: \\text { mean }-\\text { mode } \\approx 3 \\times(\\text { mean }-\\text { median }) \\text { mean }-\\text { mode } \\approx 3 \\times(\\text { mean }-\\text { median }) Ini menyiratkan bahwa mode untuk kurva frekuensi unimodal yang cukup miring dapat dengan mudah didekati jika nilai rata-rata dan median diketahui.","title":"Median"},{"location":"memahami-data/#mengukur-sebaran-data","text":"Kita sekarang membahas ukuran ukuran untuk menilai dispersi atau penyebaran data numerik. Ukuran-ukuran itu adalah rentang (range), kuantil, kuartil, persentil, dan rentang interkuartil. Semua itu adalah ringkasan lima angka, yang dapat ditunjukkan dengan boxplot, berguna dalam mengidentifikasi pencilan (outlier). Varians dan standar deviasi juga menunjukkan sebaran distribusi data.","title":"Mengukur Sebaran Data"},{"location":"memahami-data/#rentang-range-quartil-and-rentang-interquartile","text":"Misalkan x_1, x_2, .. x_N x_1, x_2, .. x_N adalah sekumpulan pengamatan untuk atribut numerik, X X . Rentang adalah selisih antara nilai terbesar (maks ()) dan terkecil (min ()). Misalkan data untuk atribut X diurutkan dalam urutan naik.Bagilah data berdasarkan titik titik tertentu sehingga membagi distribusi data ukuran yang sama, seperti pada Gambar dibawah. Titik data ini disebut kuantil. 2-quantile adalah titik data yang membagi bagian bawah dan atas dari distribusi data. Ini sama dengan median. 4-kuantil adalah tiga titik data yang membagi distribusi data menjadi empat bagian yang sama; setiap bagian mewakili seperempat dari distribusi data. Ini lebih sering disebut sebagai kuartil. 100-kuantil lebih sering disebut sebagai persentil; mereka membagi distribusi data menjadi 100 data berukuran sama. Median, kuartil, dan persentil adalah bentuk kuantil yang paling banyak digunakan. Gambar 2.6. Percentile data Kuartil memberikan gambaran pusat distribus, penyebaran, dan bentuk distribusi. Kuartil satu, dilambangkan oleh Q1, adalah persentil ke-25. Nilai ini menunjukan 25% terendah dari data. Kuartil ketiga, dilambangkan oleh Q3, adalah persentil ke-75 - itu memisahkan data 75% dari terendah data (atau 25% dari tertinggi data. Kuartil kedua adalah persentil ke-50 atau median dari distribusi data. Jarak antara kuartil pertama dan ketiga adalah ukuran yang menyatakan rentang yang dicakup oleh bagian tengah data. Jarak ini disebut rentang interkuartil (IQR) dan dinyatakan dengan I Q R = Q _ { 3 } - Q _ { 1 } I Q R = Q _ { 3 } - Q _ { 1 } Dengan ukuran a kuartil Q1 dan Q3, dan median kita dapat mengidentifikasikan ada tidaknya pencilan (outlier) pada suatu data. Data pencilan atau outlier nilai data biasanya ada di setidaknya 1,5 \u00d7 IQR di atas kuartil ketiga atau di bawah kuartil pertama Karena Q1, median, dan Q3 tidak berisi informasi tentang titik akhir (mis., Ekor) data, ringkasan yang lebih lengkap dari bentuk distribusi dapat diperoleh dengan memberikan nilai data terendah dan tertinggi juga. Ini dikenal sebagai ringkasan lima angka. Ringkasan lima nomor distribusi terdiri dari median (Q2), kuartil Q1 dan Q3, dan data terkecil dan terbesar( Minimum, Q1, Median, Q3, Maksimum) Boxplots adalah cara populer untuk memvisualisasikan distribusi. Boxplot menggabungkan ringkasan lima angka sebagai berikut: - Ujung kotak adalah kuartil dan panjang kotak adalah rentang interkuartil. - Median ditandai dengan garis di dalam kotak. - Dua garis (disebut whiskers) di luar kotak memanjang ke pengamatan terkecil (Minimum) dan terbesar (Maksimum) Outlier biasanya ada di dibawah Q_1 \u2013 1.5 \\times IQR Q_1 \u2013 1.5 \\times IQR dan diatas $ Q_3 + 1.5 \\times IQR$ Gambar 2.7. Boxplot","title":"Rentang (Range), Quartil, and Rentang Interquartile"},{"location":"memahami-data/#variansi-dan-standar-deviasi","text":"Variansi dan standar deviasi adalah ukuran penyebaran data. Nilai-nilai tersebut menunjukkan bagaimana penyebaran distribusi data. Standar Deviasi yang rendah berarti bahwa pengamatan data cenderung sangat dekat dengan rata-rata, sedangkan deviasi standar yang tinggi menunjukkan data tersebar di sejumlah nilai-nilai besar. Varian dari pengamatan N, x_1, x_2, ..., x_N N, x_1, x_2, ..., x_N , untuk atribut numerik X adalah \\sigma ^ { 2 } = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } ( x _ { i } - \\overline { x } ) ^ { 2 } = ( \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } x _ { i } ^ { 2 } ) - \\overline { x } ^ { 2 } \\sigma ^ { 2 } = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } ( x _ { i } - \\overline { x } ) ^ { 2 } = ( \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } x _ { i } ^ { 2 } ) - \\overline { x } ^ { 2 } di mana $ \\overline { x } $ adalah nilai rata-rata dari pengamatan, Standar deviasi,$\\sigma $, dari pengamatan adalah akar kuadrat dari variansi, \\sigma^2 \\sigma^2 Sifat dasar dari standar deviasi, \\sigma \\sigma , sebagai ukuran penyebaran data adalah sebagai berikut: Ukuran \\sigma \\sigma mengeukur sebaran disekitar rata-rata dan harus dipertimbangkan bila rata-rata dipilih sebagai ukuran pusat data \\sigma = 0 \\sigma = 0 hanya jika tidak ada penyebaran data, hanya terjadi ketika semua pengamatan memiliki nilai sama, Jika tidak maka \\sigma > 0 \\sigma > 0","title":"Variansi dan Standar Deviasi"},{"location":"memahami-data/#skewness","text":"Derajat distorsi dari kurva lonceng simetris atau distribusi normal. Ini mengukur kurangnya simetri dalam distribusi data Untuk menghitung derajat distorisi dapat menggunakan Koefisien Kemencengan Pearson yang diperoleh dengan menggunakan nilai selisih rata-rata dengan modus dibagi simpangan baku. Koefisien Kemencengan Pearson dirumuskan sebagai berikut s k=\\frac{\\overline{X}-M o}{s} s k=\\frac{\\overline{X}-M o}{s} dengan $$ \\overline{X}-M o \\approx 3(\\overline{X}-M e) $$ maka s k \\approx \\frac{3(\\overline{X}-M e)}{s} s k \\approx \\frac{3(\\overline{X}-M e)}{s} Gambar 2.8. Macam macam Kemiringan data (Skewness)","title":"Skewness"},{"location":"memahami-data/#implementasi","text":"Untuk implementasi silahkan unduh data .csv import pandas as pd from scipy import stats df = pd . read_csv ( \"data.csv\" , usecols = [ 0 ]) print ( \"jumlah data \" , df [ 'NilaiPreTest' ] . count ()) print ( \"rata-rata \" , df [ 'NilaiPreTest' ] . mean ()) print ( \"nila minimal \" , df [ 'NilaiPreTest' ] . min ()) print ( \"Q1 \" , df [ 'NilaiPreTest' ] . quantile ( 0.25 )) print ( \"Q2 \" , df [ 'NilaiPreTest' ] . quantile ( 0.5 )) print ( \"Q3 \" , df [ 'NilaiPreTest' ] . quantile ( 0.75 )) print ( \"Nilai Max \" , df [ 'NilaiPreTest' ] . max ()) print ( \"kemencengan\" , \" {0:.2f} \" . format ( round ( df [ 'NilaiPreTest' ] . skew (), 2 ))) mode = stats . mode ( df ) print ( \"Nilai modus {} dengan jumlah {} \" . format ( mode . mode [ 0 ], mode . count [ 0 ])) print ( \"kemencengan \" , \" {0:.6f} \" . format ( round ( df [ 'NilaiPreTest' ] . skew (), 6 ))) print ( \"Standar Deviasi \" , \" {0:.2f} \" . format ( round ( df [ 'NilaiPreTest' ] . std (), 2 ))) print ( \"Variansi \" , \" {0:.2f} \" . format ( round ( df [ 'NilaiPreTest' ] . var (), 2 )))","title":"Implementasi"},{"location":"memahami-data/#mengukur-jarak-data","text":"","title":"Mengukur Jarak Data"},{"location":"memahami-data/#mengukur-jarak-tipe-numerik","text":"Shirkhorshidi, A. S., Aghabozorgi, S., & Wah, T. Y. (2015). A comparison study on similarity and dissimilarity measures in clustering continuous data. PloS one, 10(12), e0144059. Salah satu tantangan dalam era ini dengan datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan Sebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaiut v_1, v_2 v_1, v_2 menyatakandua vektor yang menyatakan v_1 = {x_1, x_2, . . ., x_n}, v_2 ={y_1, y_2, . . ., y_n}, v_1 = {x_1, x_2, . . ., x_n}, v_2 ={y_1, y_2, . . ., y_n}, dimana x_i, y_i x_i, y_i disebut attribut. Ada beberapa ukuran similaritas datau ukuran jarak, diantaranya","title":"Mengukur Jarak  Tipe Numerik"},{"location":"memahami-data/#minkowski-distance","text":"Kelompk Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski distance. Minkowski distance dinyatakan dengan d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 diman m m adalah bilangan riel positif dan x_i x_i dan $ y_i$ adalah dua vektor dalam runang dimensi n n Implementasi ukuran jarak Minkowski pada model clustering data atribut dilakukan normalisasi untuk menghindari dominasi dari atribut yang memiliki skala data besar.","title":"Minkowski Distance"},{"location":"memahami-data/#manhattan-distance","text":"Manhattan distance adalah kasus khsusu dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. BIla ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right|","title":"Manhattan distance"},{"location":"memahami-data/#euclidean-distance","text":"Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini.","title":"Euclidean distance"},{"location":"memahami-data/#average-distance","text":"Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adala versi modikfikasid ari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x,y x,y dalam ruang dimensi n n , rata-rata jarak didefinisikan dengan d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } } d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } }","title":"Average Distance"},{"location":"memahami-data/#weighted-euclidean-distance","text":"Jika berdasarkan tingkatan penting dari masing masing atribut ditentukan, maka Weighted Euclidean distance adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. Ukuran ini dirumuskan dengan $$ d _ { w e } = \\left ( \\sum _ { i = 1 } ^ { n } w _ { i } ( x _ { i } - y _ { i } \\right) ^ { 2 } ) ^ { \\frac { 1 } { 2 } } $$ dimana w_i w_i adalah bobot yang diberikan pada atribut ke i.","title":"Weighted euclidean distance"},{"location":"memahami-data/#chord-distance","text":"Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan d _ { \\text {chord} } = \\left ( 2 - 2 \\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { \\| x \\| _ { 2 } \\| y \\| _ { 2 } } \\right) ^ { \\frac { 1 } { 2 } } d _ { \\text {chord} } = \\left ( 2 - 2 \\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { \\| x \\| _ { 2 } \\| y \\| _ { 2 } } \\right) ^ { \\frac { 1 } { 2 } } dimana \\| x \\|_ {2} \\| x \\|_ {2} adalah L^{2} \\text {-norm} \\| x \\|_{2} = \\sqrt { \\sum_{ i = 1 }^{ n }x_{i}^{2}} L^{2} \\text {-norm} \\| x \\|_{2} = \\sqrt { \\sum_{ i = 1 }^{ n }x_{i}^{2}}","title":"Chord distance"},{"location":"memahami-data/#mahalanobis-distance","text":"Mahalanobis distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antra data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. Mahalanobis distance dinyatakan dengan d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } diman S S adalah matrik covariance data.","title":"Mahalanobis distance"},{"location":"memahami-data/#cosine-measure","text":"Ukuran Cosine similarity lebih banyak digunakan dalam similaritas dokumen dan dinyatakan dengan Cosine(x,y)=\\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { \\| x \\| _ { 2 } \\| y \\| _ { 2 } } Cosine(x,y)=\\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { \\| x \\| _ { 2 } \\| y \\| _ { 2 } } dimana \\|y\\|_{2} \\|y\\|_{2} adalah Euclidean norm dari vektor y=(y_{1} , y_{2} , \\dots , y_{n} ) y=(y_{1} , y_{2} , \\dots , y_{n} ) didefinisikan dengan \\|y\\|_{2}=\\sqrt{ y _ { 1 } ^ { 2 } + y _ { 2 } ^ { 2 } + \\ldots + y _ { n } ^ { 2 } } \\|y\\|_{2}=\\sqrt{ y _ { 1 } ^ { 2 } + y _ { 2 } ^ { 2 } + \\ldots + y _ { n } ^ { 2 } }","title":"Cosine measure"},{"location":"memahami-data/#pearson-correlation","text":"Pearson correlation banyak digunakan dalam data expresi gen. Ukuran similaritas ini menghitung similaritas antara duan bentuk pola expresi gen. Pearson correlation didefinisikan dengan Pearson ( x , y ) = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\mu _ { x } ) ( y _ { i } - \\mu _ { y } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } } Pearson ( x , y ) = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\mu _ { x } ) ( y _ { i } - \\mu _ { y } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } } The Pearson correlation kelemahannya adalah sensitif terhadap outlier","title":"Pearson correlation"},{"location":"memahami-data/#mengukur-jarak-atribut-binary","text":"Mari kita lihat similaritas dan desimilirity untuk objek yang dijelaskan oleh atribut biner simetris atau asimetris. Aatribut biner hanya memiliki dua status: 0 dan 1 Contoh atribut perokok menggambarkan seorang pasien, misalnya, 1 menunjukkan bahwa pasien merokok, sedangkan 0 menunjukkan pasien tidak merokok. Memperlakukan atribut biner sebagai atribut numerik tidak diperkenankan. Oleh karena itu, metode khusus untuk data biner diperlukan untuk membedakan komputasi. Jadi, bagaimana kita bisa menghitung ketidaksamaan antara dua atribut biner? \u201dSatu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2 \\times 2 2 \\times 2 di mana q q adalah jumlah atribut yang sama dengan 1 untuk kedua objek i i dan j j , r r adalah jumlah atribut yang sama dengan 1 untuk objek i i tetapi 0 untuk objek j j , s s adalah jumlah atribut yang sama dengan 0 untuk objek i i tetapi 1 untuk objek j j , dan t t adalah jumlah atribut yang sama dengan 0 untuk kedua objek i i dan j j . Jumlah total atribut adalah p p , di mana p=q+r+s+t p=q+r+s+t Ingatlah bahwa untuk atribut biner simetris, masing-masing nilai bobot yang sama.Dissimilarity yang didasarkan pada atribut aymmetric binary disebut symmetric binary dissimilarity. Jika objek i dan j dinyatakan sebagai atribut biner simetris, maka dissimilarity antar i i dan j j adalah d ( i , j ) = \\frac { r + s } { q + r + s + t } d ( i , j ) = \\frac { r + s } { q + r + s + t } Untuk atribut biner asimetris, kedua kondisi tersebut tidak sama pentingnya, seperti hasil positif (1) dan negatif (0) dari tes penyakit. Diberikan dua atribut biner asimetris, pencocokan keduanya 1 (kecocokan positif) kemudian dianggap lebih signifikan daripada kecocokan negatif. Ketidaksamaan berdasarkan atribut-atribut ini disebut asimetris biner dissimilarity, di mana jumlah kecocokan negatif, t, dianggap tidak penting dan dengan demikian diabaikan. Berikut perhitungannya d ( i , j ) = \\frac { r + s } { q + r + s } d ( i , j ) = \\frac { r + s } { q + r + s } Kita dapat mengukur perbedaan antara dua atribut biner berdasarkan pada disimilarity. Misalnya, biner asimetris kesamaan antara objek i i dan j j dapat dihitung dengan \\operatorname { sim } ( i , j ) = \\frac { q } { q + r + s } = 1 - d ( i , j ) \\operatorname { sim } ( i , j ) = \\frac { q } { q + r + s } = 1 - d ( i , j ) Persamaan similarity ini disebut dengan Jaccard coefficient","title":"Mengukur Jarak Atribut Binary"},{"location":"memahami-data/#mengukur-jarak-tipe-categorical","text":"Li, C., & Li, H. (2010). A Survey of Distance Metrics for Nominal Attributes. JSW, 5(11), 1262-1269.","title":"Mengukur Jarak Tipe categorical"},{"location":"memahami-data/#overlay-metric","text":"Ketika semua atribut adalah bertipe nominal, ukuran jarak yang paling sederhana adalah dengan Ovelay Metric (OM) yang dinyatakan dengan d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\delta ( a _ { i } ( x ) , a _ { i } ( y ) ) d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\delta ( a _ { i } ( x ) , a _ { i } ( y ) ) dimana n n adalah banyaknya atribut, a_i(x) a_i(x) dan a_i(y) a_i(y) adalah nilai atribut ke i i yaitu A_i A_i dari masing masing objek x x dan y y , \\delta \\ ( a_{ i } ( x ) , a_{ i } ( y ) ) \\delta \\ ( a_{ i } ( x ) , a_{ i } ( y ) ) adalah 0 jika a _ { i } ( x ) = a _ { i } ( y ) a _ { i } ( x ) = a _ { i } ( y ) dan 1 jika sebaliknya. OM banyak digunakan oleh instance-based learning dan locally weighted learning. Jelas sekali , ini sedikit beruk untuk mengukur jarak antara masing-masing pasangan sample, karena gagal memanfaatkan tambahan informasi yang diberikan oleh nilai atribut nominal yang bisa membantu dalam generalisasi.","title":"Overlay Metric"},{"location":"memahami-data/#value-difference-metric-vdm","text":"VDM dikenalkan oleh Standfill and Waltz, versi sederhana dari VDM tanpa skema pembobotan didefinsisikan dengan d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\sum _ { c = 1 } ^ { C } \\left| P ( c | a _ { i } ( x ) ) - P ( c | a _ { i } ( y ) ) \\right | d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\sum _ { c = 1 } ^ { C } \\left| P ( c | a _ { i } ( x ) ) - P ( c | a _ { i } ( y ) ) \\right | dimana C C adalah banyaknya kelas, P(c|a_i(x)) P(c|a_i(x)) adalah probabilitas bersyarat dimana kelas x x adalah c c dari atribut A_i A_i , yang memiliki nilai a_i(x) a_i(x) , P(c|a_i(y)) P(c|a_i(y)) adalah probabilitas bersyarat dimana kelas y y adalah c c dengan atribut A_i A_i memiliki nilai a_i(y) a_i(y) VDM mengasumsikan bahwa dua nilai dari atribut adalah lebih dekat jika memiliki klasifikasi sama. Pendekatan lain berbasi probabilitas adalah SFM (Short and Fukunaga Metric) yang kemudian dikembangkan oleh Myles dan Hand dan didefinisikan dengan d ( x , y ) = \\sum _ { c = 1 } ^ { C } \\left | P ( c | x ) - P ( c | y ) \\right| d ( x , y ) = \\sum _ { c = 1 } ^ { C } \\left | P ( c | x ) - P ( c | y ) \\right| diman probabilitas keanggotaan kelas diestimasi dengan P(c|x) P(c|x) dan P(c|y) P(c|y) didekati dengan Naive Bayes,","title":"Value Difference Metric (VDM)"},{"location":"memahami-data/#minimum-risk-metric-mrm","text":"Ukuran ini dipresentasikan oleh Blanzieri and Ricci, berbeda dari SFM yaitu meminimumkan selisih antara kesalahan berhingga dan kesalahan asymtotic. MRM meminimumkan risk of misclassification yang didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } P ( c | x ) ( 1 - P ( c | y ) ) $$","title":"Minimum Risk Metric (MRM)"},{"location":"memahami-data/#mengukur-jarak-tipe-ordinal","text":"Han, J., Pei, J., & Kamber, M. (2011). Data mining: concepts and techniques. Elsevier . Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. Kategori-kategori ini disusun dalam peringkat. Yaitu, rentang atribut numerik dapat dipetakan ke atribut ordinal f f yang memiliki M_f M_f state. Misalnya, kisaran suhu atribut skala-skala (dalam Celcius)dapat diatur ke dalam status berikut: \u221230 hingga \u221210, \u221210 hingga 10, 10 hingga 30, masing-masing mewakili kategori suhu dingin, suhu sedang, dan suhu hangat. M M adalah jumlah keadaan yang dapat dilakukan oleh atribut ordinalmemiliki. State ini menentukan peringkat 1, ..., M_f 1, ..., M_f Perlakuan untuk atribut ordinal adalah cukup sama dengan atribut numerik ketika menghitung disimilarity antara objek. Misalkan f f adalah atribut-atribut dari atribut ordinal dari n n objek. Menghitung disimilarity terhadap f fitur sebagai berikut: Nilai f f untuk objek ke- i i adalah x_{if} x_{if} , dan f f memiliki M_f M_f status urutan , mewakili peringkat 1, .., M_f 1, .., M_f Ganti setiap x_{if} x_{if} dengan peringkatnya, r_{if} \\in \\{1...M_f\\} r_{if} \\in \\{1...M_f\\} Karena setiap atribut ordinal dapat memiliki jumlah state yang berbeda, diperlukan untuk memetakan rentang setiap atribut ke [0,0, 1.0] sehingga setiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data dengan mengganti peringkat r_{if} r_{if} dengan $$ z _ { i f } = \\frac { r _ { i f } - 1 } { M _ { f } - 1 } $$ Dissimilarity kemudian dihitung dengan menggunakan ukuran jarak seperti atribut numerik dengan data yang baru setelah ditransformasi $ z _ { i f }$","title":"Mengukur Jarak Tipe Ordinal"},{"location":"memahami-data/#menghitung-jarak-tipe-campuran","text":"Wilson, D. R., & Martinez, T. R. (1997). Improved heterogeneous distance functions. Journal of artificial intelligence research, 6, 1-34. Menghitung ketidaksamaan antara objek dengan atribut campuran yang berupa nominal, biner simetris, biner asimetris, numerik, atau ordinal yang ada pada kebanyakan databasae dapat dinyatakan dengan memproses semua tipe atribut secara bersamaan. Salah satu teknik tersebut menggabungkan atribut yang berbeda ke dalam matriks ketidaksamaan tunggal dan menyatakannya dengan skala interval antar [0,0, 1.0] [0,0, 1.0] . Misalkan data berisi atribut p p tipe campuran. Ketidaksamaan (disimilarity ) antara objek i i dan j j dinyatakan dengan d ( i , j ) = \\frac { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } d _ { i j } ^ { ( f ) } } { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } } d ( i , j ) = \\frac { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } d _ { i j } ^ { ( f ) } } { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } } dimana \\delta_{ij}^{f}=0 \\delta_{ij}^{f}=0 - jika x_{if} x_{if} atau x_{jf} x_{jf} adalah hilang (i.e., tidak ada pengukuran dari atribut f untuk objek i i atau objek j j ) jika x_{if}=x_{jf}=0 x_{if}=x_{jf}=0 dan atribut f f adalah binary asymmetric, selain itu \\delta_{ij}^{f}=1 \\delta_{ij}^{f}=1 Kontribusi dari atribut f f untuk dissimilarity antara i dan j (yaitu. d_{ij}^{f} d_{ij}^{f} ) dihitung bergantung pada tipenya, Jika f f adalah numerik, d_{ij}^{f}=\\frac{ \\|x _{if}-x_{jf}\\|}{max_hx_{hf}-min_hx{hf}} d_{ij}^{f}=\\frac{ \\|x _{if}-x_{jf}\\|}{max_hx_{hf}-min_hx{hf}} , di mana h menjalankan semua nilai objek yang tidak hilang untuk atribut f Jika f f adalah nominal atau binary,$d_{ij}^{f}=0 $jika x_{if}=x_{jf} x_{if}=x_{jf} , sebaliknya d_{ij}^{f}=1 d_{ij}^{f}=1 Jika f f adalah ordinal maka hitung rangking r_{if} r_{if} dan \\mathcal z_{if}=\\frac {r_{if}-1}{M_f-1} \\mathcal z_{if}=\\frac {r_{if}-1}{M_f-1} , dan perlakukan z_{if} z_{if} sebagai numerik.","title":"Menghitung Jarak Tipe Campuran"},{"location":"memahami/","text":"Memahami Data dan Pengambilan data \u00b6 Macam macam Data \u00b6 Dalam data data mining dan maha data, Anda akan menemukan banyak jenis data yang berbeda, dan masing-masing cenderung membutuhkan alat dan teknik yang berbeda. Macam macam data dikelompokkan sebagai berikut 1 : Data terstruktur (structured) Data tidak terstruktur(unstructured Data bahasa alami(Natural Language) Data yang dibangkit oleh Mesin (Machined-Generated) Data Audio, Video,Citra Data Streamming Data berbasis Graph(Graph-based) Data Terstruktur \u00b6 Data terstruktur adalah data yang bergantung pada model data dan yang dinyatakan dalam bentuk tabel dengan atribut} (kolom) dan baris. Data terstruktur mudah disimpan dalam database dalam bentuk tabel atau file excel (Ms Office), SQl (structure Query Language)sehingga mudah dilakukan query terhadap data tersebut. Tetapi realitanya banyak data yang ada dalam dalam bentuk data tidak terstruktur karena data dihasilkan oleh manusia dan mesin Gambar 2.1 Contoh data terstruktur.. Macam- macam atribut \u00b6 Atribut adalah data yang mewakili karakteristik atau fitur dari objek data. Atribut bisa disebut juga dengan dimensi, fitur, dan variabel yang istilah itu sering digunakan literatur. Dimensi istilah yang biasanya digunakan dalam data warehouse. Dalam literatur pembelajaran mesin cenderung menggunakan istilah fitur, sementara dalam bidang statistik lebih memilih menggunakan istilah variabel. Dalam penambangan data atau data miniing dan database biasa menggunakan istilah atribut atau fitur , dan dalam buku ini juga menggunakan istilah atribut atau fitur. Contoh atribut-atribut yang menggambarkan objek pelanggan dapat mencakup, misalnya ID pelanggan, nama, dan alamat. Nilai yang diamati untuk atribut tertentu dikenal sebagai nilai observasi. Sekumpulan atribut yang digunakan untuk menggambarkan objek disebut disebut dengan vektor atribut (atau vektor fitur. Distribusi data yang melibatkan satu atribut (atau variabel) disebut univariat. Distribusi bivariat melibatkan dua atribut, dan seterusnya. Jenis atribut ditentukan oleh nilai-nilai pada atribut tersebut yang mungkin nominal, biner,atau ordinal, atau numerik. Pada subbagian berikut, kami perkenalkan nilai nilai tersebut Macam macam tipe data atribut Atribut Nominal Nilai atribut nominal adalah simbol ataunama barang. Setiap nilai mewakili beberapa jenis kategori, kode, atau status, dan Atribut nominal juga disebut kategori. Nilai-nilainya tidak memiliki tingkatan nilai. Dalam ilmu komputer, nilainya juga dikenal sebagai enumerasi Contoh : Misalkan warna rambut dan status perkawinan adalah dua atribut dari data orang. Nilai yang mungkin untuk warna rambut adalah hitam, coklat, pirang, merah, hitam pucat, abu-abu, dan putih. Status perkawinan memiliki nilai atribut lajang, menikah, bercerai, dan janda. Baik warna rambut maupun status perkawinan adalah atribut nominal. Contoh lain dari atribut nominal adalah atribut pekerjaan dengan nilai-nilainya adalah guru, dokter gigi, programmer, petani, dan sebagainya Atribut Biner Atribut biner adalah atribut nominal dengan hanya dua kategori atau status: 0 atau 1, di mana 0 biasanya berarti atribut itu tidak ada, dan 1 berarti itu ada. Atribut Biner disebut sebagai Boolean jika dinyatakan dengan benar (true) dan salah(false) Contoh : Terdapat atribut yang menggambarkan merokok pada pasien, 1 menunjukkan bahwa pasien merokok,sementara 0 menunjukkan bahwa pasien tidak merokok. Demikian pula, seandainya ada pasien menjalani tes medis yang memiliki dua kemungkinan hasil. Atribut Tes medis bersifat biner, dengan nilai 1 berarti hasil tes untuk pasien positif, sedangkan 0 berarti hasilnya negatif. Atribut biner simetris jika keduanya emiliki nilai bobot yang sama; Artinya, tidak ada kekhususan mengenai hasil mana yang harus dikodekan sebagai 0 atau 1. Misalkan atribut gender yang dengan nila atributnya laki dan perempuan. Atribut biner adalah asimetris jika hasil dari nilai nilainya tidak sama pentingnya seperti hasil positif dan negatif dari tes medis untuk HIV. Dengan mengkodekan hasil yang paling penting, biasanya 1 (mis., HIV positif) dan yang lainnya dengan 0 (mis., HIV negatif) Atribut ordinal Atribut ordinal adalah atribut dengan nilai yang memiliki arti urutan atau peringkat di antara nilai nilai yang ada, tapi besarnya nilai yang berurutan tersebut tidak diketahui. Ukuran kecenderungan terpusat dari atribut ordinal dapat diwakili oleh modus dan median median (nilai tengah), tetapi tidak untuk nilai rata-rata.Perlu diperhatikan bahwa atribut nominal, biner, dan ordinal bersifat kualitatif. Artinya, atribut-atribut tersebut hanya menjelaskan sebuah fitur dari suatu objek tanpa memberikan ukuran atau kuantitas yang sebenarnya. Nilai-nilai atribut kualitatif biasanya merupakan katakata yang mewakili kategori Contoh : Atribut ordinal Misalkan ukuran minuman yang tersedia di sebuah restoran cepat saji. Atribut nominal ini memiliki tiga nilai yang mungkin: kecil, sedang, dan besar. Nilai memiliki arti urutan yang (yang sesuai dengan ukuran minuman). Contoh atribut ordinal lainnya adalah pangkat dan jabatan profesi. Atribut ordinal berguna untuk melakukan penilaian subjektif terhadap kualitas sesuatu objek yang tidak dapat diukur secara obyektif; atribut ordinal sering digunakan dalam survei untuk peringkat. Dalam satu survei, para peserta diminta untuk menilai tingkat kepuasan mereka sebagai pelanggan.Kepuasan pelanggan memiliki kategori ordinal berikut ini: 0: sangat tidak puas,1: agak tidak puas, 2: netral, 3: puas, dan 4: sangat puas. Atribut ordinal juga dapat diperoleh dari iskritisasi nilai atribut numerik dengan membagi rentang nilai menjadi urutan kategoria Atribut Numerik Atribut numerik bersifat kuantitatif; Artinya, ini adalah kuantitas yang terukur, yang dinyatakan dengan bilangan bulat atau nilai riel. Atribut numerik dapat Atribut Skala Interval(interval-scaled) atau skala ration (ratio-scaled) Atribut skala interval diukur pada dengan skala unit ukuran yang sama. Nilai - nilai Interval berskala memiliki urutan dan bisa positif, 0, atau negatif. Jadi, selain untuk memberikan peringkat nilai, atribut semacam itu memungkinkan kita untuk membandingkan dan mengukur perbedaan antar nilai Contoh : Atribut suhu adalah Skala interval. Misalkan kita memiliki nilai suhu di luar ruangan untuk beberapa hari yang berbeda dari suatu objek. Dengan mengurutkan nilai, kita mendapatkan peringkat objek yang berkenaan dengan suhu. Selain itu, kita bisa mengukur perbedaan antara nilai.Misalnya, a suhu 20o C adalah lima derajat lebih tinggi dari suhu 15oC. Contoh lain kalender tahun adalah. Misalnya, tahun 2002 dan 2010 terpisah delapan tahun. Karena atribut skala interval adalah numerik, kita dapat menghitung nilai ratarata, ukuran median dan modus dari kecenderungan terpusat Atribut Skala Ratio Atribut skala rasio adalah atribut numerik dengan melekat titik nol pada nilai atribut tersebut. Artinya, jika pengukuran adalah berskala rasio, kita dapat dapat mengatakan berapa kali dari nilai yang lain atau rasio dari nilai yang lain. Selain itu, nilai yang dipesan, dan kita juga bisa menghitung selisih antara nilai, serta mean, median, dan modus Contoh Atribut tentang pengukuran berat badan, tinggi badan, jumlah kata dalam dokumen Data Tidak Terstruktur \u00b6 Data tidak terstruktur adalah data yang tidak mudah dimasukkan ke dalam model data karena isi/kontennya spesifik atau bervariasi. Salah satu contoh data tidak terstruktur adalah data email. Meskipun email berisi elemen terstruktur seperti pengirim, judul, dan isi teks, terlalu banyak variasi dari isi yang terkandung dalamnya diantaranya dialek bahasa yang dipakai dan sebagainya. Email juga salah satu contoh data bahasa alami Gambar 2.2 Contoh Data email Bahasa Alami \u00b6 Dalam neuropsikologi , linguistik , dan filsafat bahasa , bahasa alami atau bahasa biasa adalah bahasa yang telah berevolusi secara alami pada manusia melalui penggunaan dan pengulangan tanpa perencanaan. Bahasa alami berbeda dengan bahasa yang dibangun untuk memprogramna komputer atau membangun logika nalar. Bahasa alami dikenal sebagai bahasa manusia misal bahasa indonesia, bahasa inggris dan lain lain. Didalam pemrosesan bahasa alami diperluangan pengetahuan ilmu linguistics, semantics, statistics and machine learning.Dengan pemrosesan bahasa alami membantu komputer untuk memahami bahasa yang telah diucapkan oleh manusia Data yang dibangkitkan oleh Mesin \u00b6 Data yang dibangkitkan oleh mesin secara otomatis tanpa intervensi manusia. Data ini terus menerus dibangkitkan selama proses tertentu sedang berjalan. Misalkan data weblog dari mesin server yang dihasilkan dari hasil transaksi user dengan sistem web. Contoh lain adalah data yang dihasilkan dari implementasi internet of things misal perekaman suhu udara dan kelembaban udara dari daerah tertentu yang terhubung dengan pusat penyimpanan data tersebut. Data jaringan atau data berbasis Graph \u00b6 Data graph adalah data yang dinyatakan dengan graph yang dalam matematika mengacu pada konsep teori graph. Data ini menunjukkan keterhubungan antara objek objek atau relasi antar objek objek dengan menggunakan struktur node, edge, dan karakteristik/sifat keterhubungan antar objek tersebut. Salah satu data graph adalah data keterhubungan orang dalam media sosial. Dengan memanfaatkan data graph media sosial kita dapat mengukur ukuran ukuran tertentu berdasarkan struktur yang dibentuknya. Misalkan menentukan pengaruh orang dalam struktur jaringan tersebut, apakah termasuk orang penting/berpengaruh atau bukan. Gambar berikut menunjukkan contoh data graph Gambar 2.3 .Pertemanan dalam media sosial yang dinyataka dengan data graph Database graph dapat digunakan untuk menyimpan data berbasis graph dan mengunakan query tertentu yaitu SPARQL Data Audio,Vidio dan Citra \u00b6 Dengan perkembangan teknologi implementasi multimedia yang sangat pesat saat,data audio,video dan citra cukup besar dihasilkan dari transaksi bisnis. Dengan besarnya data yang dihasilkan membutuhkan proses pengolahan spesifik dari data tersebut untuk dimanfaatkan terutama dalam analisa data sain. Diantara pemanfaatan data mulitimedia tersebut adalah pengenalan objek, pengenala suara, segmentasi citra satelit dan banyak analisa lain yang dihasilkan dari data multimeda tersebut. Data streamming \u00b6 Data Streaming adalah data yang dihasilkan secara terus-menerus oleh ribuan sumber data, yang biasanya mengirimkan catatan data secara bersamaan, dan dalam ukuran kecil (urutan Kilobyte). Data streaming mencakup berbagai macam data seperti logfile yang dihasilkan oleh pelanggan aplikasi seluler atau website Anda, transaksi e-commerce, informasi dari jejaring sosial, data geospasial, dan perangkat sensor yang terhubung atau instrumentasi di pusat data. Data ini perlu diproses secara berurutan dan bertahap secara record-by-record digunakan untuk berbagai macam analisis misalkan korelasi, agregasi, penyaringan, dan pengambilan sampel. Informasi yang diperoleh dari analisis tersebut memberikan petunjuk terhadap pelanggan mereka seperti penggunaan layanan mereka, aktivitas server, klik website, dan lain lain. Misalnya, dalam bisnis kita dapat melacak perubahan sentimen publik pada merek dan produk mereka dengan menganalisis aliran data media sosial, sehingga dapat merespons secara tepat baik waktu dan tindakan yang harus dilakukan Distribusi Data \u00b6 Karakteristik utama dari data adalah distribusi probabilitasnya. Distribusi data yang paling dikenal adalah distribusi normal atau Gaussian. Distribusi ini ditemukan pada sistem fisik dimana data dibangkitkan secara acak. Fungsi dinyatakan dalam bentuk fungsi padat probabilitas(probability density function) $$ f ( x ) = \\frac { 1 } { ( \\sigma \\sqrt { 2 } \\pi ) } \\frac { e ^ { - ( x - \\mu ) ^ { 2 } } } { ( 2 \\sigma ^ { 2 } ) } $$ Dimana \\sigma \\sigma adalah standar deviasi dan \\mu \\mu adalah mean. Persamaan ini menyatakan peluang variable acak dari suatu data x x . Kita menyatakan standar deviasi sebagai lebar kurva lonceng dan rata rata sebagai pusat. Kadangkala istilah variance digunakan dan ini adalah kuadrat dari standar deviasi. Standar deviasi pada dasarnya mengukur bagaimana sebaran data. Untuk memahami lebih jelasnya bagaimana fungsi tersebut digambarkan, berikut implementasinya data dengan distribusi normal yang memiliki rata-rata 1 dan variansinya 0.5 Gambar 2.4. Distribusi Data mu = 1 # rata-rata sigma = np . sqrt ( 0.5 ) # standar deviasi (akar dari varians) s = np . random . normal ( mu , sigma , 1000 ) # membangkitkan 1000 bilangan acak dgn distribusi norma import matplotlib.pyplot as plt plt . plot ( bins , 1 / ( sigma * np . sqrt ( 2 * np . pi )) * np . exp ( - ( bins - mu ) ** 2 / ( 2 * sigma ** 2 ) ), linewidth = 2 , color = 'blue' ) plt . show () Ekplorasi data tipe Numerik \u00b6 Pada bagian ini kita membahas metode statistik dasar untuk analisis ekploarasi data atribut numerik. Kita membahas ukuran kecenderungan pusat (central tendency), ukuran dispersi atau sebaran, dan ukuran ketergantungan linier atau hubungan antara atribut. Kita menekankan hubungan antara probabilistik dan geometris dan aljabar dari sudut pandang data matriks Analisa univariat \u00b6 Analisis univariat dilakukan pada atribut tunggal ( X X ); dengan demikian matriks data D bisa dianggap sebagai matriks n \u00d7 1 n \u00d7 1 , atau sebagai vektor kolom, yang dianyatakan dengan kkk X=\\begin {pmatrix} \\begin{array} { c } { X } \\\\ \\hline x _ { 1 } \\\\ { x _ { 2 } } \\\\ { \\vdots } \\\\ { x _ { n } } \\end{array} \\end {pmatrix} X=\\begin {pmatrix} \\begin{array} { c } { X } \\\\ \\hline x _ { 1 } \\\\ { x _ { 2 } } \\\\ { \\vdots } \\\\ { x _ { n } } \\end{array} \\end {pmatrix} dimana X X adalah atribut numerik yang dimaksudkan, dengan x_i \\in \\mathbb R x_i \\in \\mathbb R . X X diasumsikan adalah variabel random, dengan setiap titik x_i(1\\leq i \\leq n) x_i(1\\leq i \\leq n) , merupakan variabel acak. Kita asumsikan bawa data pengamatan adalah. Kami berasumsi bahwa data yang diamati adalah sampel acak yang diambil dari X X , artinya, setiap variabel x_i x_i adalah saling bebas dan berdistribus sama (iid). Dalam sudut pandang vektor, kami memperlakukan sampel sebagai vektor n-dimensi, dan menulis X \\in \\mathbb R^n X \\in \\mathbb R^n Secara umum, fungsi padat probabilitas atau fungsi mass f(x) f(x) dan fungsi distribusi kumulatif F(x) F(x) untuk atribut X X keduanya tidak diketahui. Akan tetapi, kita dapat mengestimasi distribusi ini langsung dar data sample, juga juga memungkinkan kita untuk menghitung beberapa parameter penting populasi. Secara umum, fungsi padat probabilitas atau fungsi mass f(x) f(x) dan fungsi distribusi kumulatif F(x) F(x) untuk F ^ { - 1 } ( q ) = \\operatorname { min } { x | \\hat { F } ( x ) \\geq q } \\quad \\text { for } q \\in [ 0,1 ] F ^ { - 1 } ( q ) = \\operatorname { min } { x | \\hat { F } ( x ) \\geq q } \\quad \\text { for } q \\in [ 0,1 ] atribut X X keduanya tidak diketahui. Akan tetapi, kita dapat mengestimasi distribusi ini langsung dar data sample, juga juga memungkinkan kita untuk menghitung beberapa parameter penting populasi. Fungsi distribusi Kumulatif Empiris \u00b6 Fungsi distribusi kumulatif empiris (CDF ) dari X X dinyatakan dengan $$ \\hat { F } ( x ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i } \\leq x ) $$ dimana I(x_i\\le x)=\\Biggl\\{\\begin{array}={} 1 & {\\text {if }x_i\\le x }\\\\ 0 & {\\text {if }x_i > x}\\end{array} I(x_i\\le x)=\\Biggl\\{\\begin{array}={} 1 & {\\text {if }x_i\\le x }\\\\ 0 & {\\text {if }x_i > x}\\end{array} adalah variabel indikator biner yang menyatakan variabel indikator biner yang menunjukkan apakah kondisi yang diberikan terpenuhi atau tidak. Fungsi distribusi kumulatif Invers \u00b6 Definisi fungsi distribusi kumulatif invers atau fungsi quantile untuk variabel acak X X sebagai berikut : $$ F ^ { - 1 } ( q ) = \\operatorname { min } { x | \\hat { F } ( x ) \\geq q } \\quad \\text { for } q \\in [ 0,1 ] $$ Fungsi distribusi kumulatif Invers empiris dapat diperoleh dari persamaan (2) Fungsi massa Probabilitas Empiris \u00b6 Fungsi massa probabilitas empiris dari X X dinyatakan dengan $$ \\hat { f } ( x ) = P ( X = x ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i } = x ) $$ dimana I(x_i\\le x)=\\Biggl\\{\\begin{array}={} 1 & {\\text {if }x_i= x }\\\\ 0 & {\\text {if }x_i \\neq x}\\end{array} I(x_i\\le x)=\\Biggl\\{\\begin{array}={} 1 & {\\text {if }x_i= x }\\\\ 0 & {\\text {if }x_i \\neq x}\\end{array} Fungsi massa probabilitas empiris juga menempatkan massa probabitas \\frac {1}{n} \\frac {1}{n} pada setipa titik x_i x_i Mengukur kecenduran terpusat \u00b6 Ukuran ini memberikan indikasi tentang konsentrasi massa probabilitas , nilai tengah dan lainnya. Mean \u00b6 Mean juga disebut dengan nilai harapan dari variabel acak X X adalah rata rata aritmetika dari nilai X X . Itu merupakan salah satu dari kecenderungan terpusat dari X X . Mean atau nilai harapan dari variabel acak X X didefinisikan dengan $$ \\mu = E [ X ] = \\sum _ { x } x f ( x ) $$ diman f(x) f(x) adalah fungsi massa probabilitas dari X X . Nilai harapan dari variabel acak kontinu X X dinyakan dengan $$ \\mu = E [ X ] = \\int _ { - \\infty } ^ { \\infty } x f ( x ) d x $$ dimana f(x) f(x) adalah fungsi padat probabilitas dari X X . Sample Mean . Sample mean adalah statistik, yaitu fungsi $ \\hat { \\mu } : { x _ { 1 } , x _ { 2 } , \\ldots , x _ { n } } \\rightarrow \\mathbb R$, didefinisikan sebagai nilai rata-rata dari x_i x_i : $$ \\hat { \\mu } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } $$ nilai adalah sebagai pengestimasi nilai mean yang tidak diketahui dari X X . Nilai tersebut diperoleh dengan memasukkan dalam fungsi massa probabilitas empiris dalam persamaan (7) $$ \\hat { \\mu } = \\sum _ { x } x \\hat { f } ( x ) = \\sum _ { x } x ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i } = x ) ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } $$ Sample mean adalah tidak bias . Estimator \\hat { \\theta } \\hat { \\theta } disebut dengan unbiased estimatore (stimator tidak bias) untuk parameter \\theta \\theta jika E[\\hat \\theta] = \\theta E[\\hat \\theta] = \\theta untuk setiap kemungkinan nilai dari \\theta \\theta . Sample mean \\hat \\mu \\hat \\mu adalah unbiased estimator untuk mean populasi \\mu \\mu sehingga $$ E [ \\hat { \\mu } ] = E [ \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } ] = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } E [ x _ { i } ] = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } \\mu = \\mu $$ dimana kita gunakan fakta bahwa variabel acak x_i x_i adalah IID sesuai dengan X X , yang berarti bahwa mereka memiliki rata-rata \\mu \\mu yang sama seperti X X , yaitu,$ E [x_i] =\\mu$ untuk semua x_i x_i . Kita juga menggunakan fakta bahwa fungsi ekpektasi E E adalah linier operator yaitu untuk suatu dua bilangan acak X X dan Y Y dan bilangan real a a dan b b , kita memiliki E [ a X + b Y ] = a E [ X ] + b E [ Y ] E [ a X + b Y ] = a E [ X ] + b E [ Y ] Robustnes Kita mengatakan bahwa statistik adalah robust jika tidak dipengaruhi oleh suatu nilai ekstrim ( misal outlier/pencilan) dalam data. Rata-rata sampel sayangnya tidak kuat karena ada satu nilai besar (outlier) dapat mejadikan rata-rata yang tidak sebenarnya. Ukuran yang lebih robust adalah trimmed mean yang didapatkan setalah mengabaikan sebagian kecil dari nilai nilai ekstrim pada salah satu ujungnya. Median \u00b6 Median dari suatu variabel acak didefinisikan dengan nilai m m sehingga $$ P ( X \\leq m ) \\geq \\frac { 1 } { 2 } \\text { and } P ( X \\geq m ) \\geq \\frac { 1 } { 2 } $$ Degan kata lain, median m m adalah nilai paling tengan (middle-most). Dalam istliah (invers) cumulatif distribution function , median m m dinyatakan dengan $$ F ( m ) = 0.5 \\text { or } m = F ^ { - 1 } ( 0.5 ) $$ Sample median dapat diperoleh dari Fungsi distribusi kumulatif invers atau fungsi distribusi kumulatif invers empiris dengan dihitung $$ \\hat { F } ( m ) = 0.5 \\text { atau } m = \\hat { F } ^ { - 1 } ( 0.5 ) $$ Pendekatan paling sederhan untuk menghitung sample median adalah pertama kai dari mengurutkan semua nilai x_i x_i (i \\in [1,n]) (i \\in [1,n]) dengan urutan naik. Jika n n adalah ganjil , media adalah nilai pada posisi \\frac {n+1}{2} \\frac {n+1}{2} . Jika n n adalah genap, nilai padan posisi \\frac {n}{2} \\frac {n}{2} dan \\frac {n}{2}+1 \\frac {n}{2}+1 adalah keduanaya median. Tidak seperti mean, media adalah robust, sehingga ia tidak dipengaruhi oleh banyak nilai extrim. Juga nilai tersebut terjadi dalam sample dan nilai yang bisa diasumsikan oleh variabel acak. Mode \u00b6 Nilai mode dari variabel acak adalah nilai dimana fungsi massa probabilitas atau fungsi padat probabilitas mencapai nilai maximumnya, bergantung pada apakah X X adalah diskrit atau kontinu. Sample mode adalah nila untuk fungsi probabilitas empiris mencapai nilai maksimum, dinyatakan dengan $$ mode(X) =\\arg \\underset{x}{max} {\\hat f(x)} $$ Mode ini mungkin bukan ukuran kecenderungan sentral yang sangat berguna untuk sampel, karena kemungkinan elemen yang tidak representatif menjadi elemen yang paling sering muncul. Selanjutnya, jika semua nilai dalam sampel berbeda, maka masing-masing akan menjadi mode Contoh . (Sample Mean, Median, dan Mode) . Perhatikan atribut sepal length (Xi) (Xi) dalam data iris. Data iris, dimana nilainya seperti yang ditunjukkan dalam tebel 1.2 . Sample mean dinyatakan dengan $$ \\hat { \\mu } = \\frac { 1 } { 150 } ( 5.9 + 6.9 + \\cdots + 7.7 + 5.1 ) = \\frac { 876.5 } { 150 } = 5.843 $$ Gambar 2.1 menunjukkan semua dari 150 nilai sepal length dan sample mean. Gambar 2.2a menunjukkan fungsi distribusi kumulatif empiri dan gambar 2.2b menunjukkan fungsi distribusi kumulatif empiris untuk sepal length Karena n=150 n=150 adalah genap, sample median adalah nilai pada posisi \\frac {n}{2}=75 \\frac {n}{2}=75 dan \\frac {n}{2}+1=76 \\frac {n}{2}+1=76 setelah diurutkan. Untuk sepal length kedua nilainya adalah 5.8, kemudian sample media adalah 5.8 . Dari fungsi distribusi kumulatif invers dalam gambar 2.2b, kita dapat melihat bahwa $$ \\hat { F } ( 5.8 ) = 0.5 \\text { or } 5.8 = \\hat { F } ^ { - 1 } ( 0.5 ) $$ Sample mode untuk sepal length adalah 5. yang dapat dilihat dari frequency dari 5 dalam gambar 2.1. Massa probabilitas empiris pada x=5 x=5 adalah $$ \\hat { f } ( 5 ) = \\frac { 10 } { 150 } = 0.067 $$ Mengukur Sebaran Data \u00b6 Kita sekarang membahas ukuran ukuran untuk menilai dispersi atau penyebaran data numerik. Ukuran-ukuran itu adalah rentang (range), kuantil, kuartil, persentil, dan rentang interkuartil. Semua itu adalah ringkasan lima angka, yang dapat ditunjukkan dengan boxplot, berguna dalam mengidentifikasi pencilan (outlier). Varians dan standar deviasi juga menunjukkan sebaran distribusi data. Rentang (Range), Quartil, and Rentang Interquartile \u00b6 Misalkan x_1, x_2, .. x_N x_1, x_2, .. x_N adalah sekumpulan pengamatan untuk atribut numerik, X X . Rentang adalah selisih antara nilai terbesar (maks ()) dan terkecil (min ()). Misalkan data untuk atribut X diurutkan dalam urutan naik.Bagilah data berdasarkan titik titik tertentu sehingga membagi distribusi data ukuran yang sama, seperti pada Gambar dibawah. Titik data ini disebut kuantil. 2-quantile adalah titik data yang membagi bagian bawah dan atas dari distribusi data. Ini sama dengan median. 4-kuantil adalah tiga titik data yang membagi distribusi data menjadi empat bagian yang sama; setiap bagian mewakili seperempat dari distribusi data. Ini lebih sering disebut sebagai kuartil. 100-kuantil lebih sering disebut sebagai persentil; mereka membagi distribusi data menjadi 100 data berukuran sama. Median, kuartil, dan persentil adalah bentuk kuantil yang paling banyak digunakan. Gambar 2.6. Percentile data Kuartil memberikan gambaran pusat distribus, penyebaran, dan bentuk distribusi. Kuartil satu, dilambangkan oleh Q1, adalah persentil ke-25. Nilai ini menunjukan 25% terendah dari data. Kuartil ketiga, dilambangkan oleh Q3, adalah persentil ke-75 - itu memisahkan data 75% dari terendah data (atau 25% dari tertinggi data. Kuartil kedua adalah persentil ke-50 atau median dari distribusi data. Jarak antara kuartil pertama dan ketiga adalah ukuran yang menyatakan rentang yang dicakup oleh bagian tengah data. Jarak ini disebut rentang interkuartil (IQR) dan dinyatakan dengan I Q R = Q _ { 3 } - Q _ { 1 } I Q R = Q _ { 3 } - Q _ { 1 } Dengan ukuran a kuartil Q1 dan Q3, dan median kita dapat mengidentifikasikan ada tidaknya pencilan (outlier) pada suatu data. Data pencilan atau outlier nilai data biasanya ada di setidaknya 1,5 \u00d7 IQR di atas kuartil ketiga atau di bawah kuartil pertama Karena Q1, median, dan Q3 tidak berisi informasi tentang titik akhir (mis., Ekor) data, ringkasan yang lebih lengkap dari bentuk distribusi dapat diperoleh dengan memberikan nilai data terendah dan tertinggi juga. Ini dikenal sebagai ringkasan lima angka. Ringkasan lima nomor distribusi terdiri dari median (Q2), kuartil Q1 dan Q3, dan data terkecil dan terbesar( Minimum, Q1, Median, Q3, Maksimum) Boxplots adalah cara populer untuk memvisualisasikan distribusi. Boxplot menggabungkan ringkasan lima angka sebagai berikut: - Ujung kotak adalah kuartil dan panjang kotak adalah rentang interkuartil. - Median ditandai dengan garis di dalam kotak. - Dua garis (disebut whiskers) di luar kotak memanjang ke pengamatan terkecil (Minimum) dan terbesar (Maksimum) Outlier biasanya ada di dibawah Q_1 \u2013 1.5 \\times IQR Q_1 \u2013 1.5 \\times IQR dan diatas $ Q_3 + 1.5 \\times IQR$ Gambar 2.7. Boxplot Variansi dan Standar Deviasi \u00b6 Variansi dan standar deviasi adalah ukuran penyebaran data. Nilai-nilai tersebut menunjukkan bagaimana penyebaran distribusi data. Standar Deviasi yang rendah berarti bahwa pengamatan data cenderung sangat dekat dengan rata-rata, sedangkan deviasi standar yang tinggi menunjukkan data tersebar di sejumlah nilai-nilai besar. Varian dari pengamatan N, x_1, x_2, ..., x_N N, x_1, x_2, ..., x_N , untuk atribut numerik X adalah \\sigma ^ { 2 } = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } ( x _ { i } - \\overline { x } ) ^ { 2 } = ( \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } x _ { i } ^ { 2 } ) - \\overline { x } ^ { 2 } \\sigma ^ { 2 } = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } ( x _ { i } - \\overline { x } ) ^ { 2 } = ( \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } x _ { i } ^ { 2 } ) - \\overline { x } ^ { 2 } di mana $ \\overline { x } $ adalah nilai rata-rata dari pengamatan, Standar deviasi,$\\sigma $, dari pengamatan adalah akar kuadrat dari variansi, \\sigma^2 \\sigma^2 Sifat dasar dari standar deviasi, \\sigma \\sigma , sebagai ukuran penyebaran data adalah sebagai berikut: Ukuran \\sigma \\sigma mengeukur sebaran disekitar rata-rata dan harus dipertimbangkan bila rata-rata dipilih sebagai ukuran pusat data \\sigma = 0 \\sigma = 0 hanya jika tidak ada penyebaran data, hanya terjadi ketika semua pengamatan memiliki nilai sama, Jika tidak maka \\sigma > 0 \\sigma > 0 Skewness \u00b6 Derajat distorsi dari kurva lonceng simetris atau distribusi normal. Ini mengukur kurangnya simetri dalam distribusi data Untuk menghitung derajat distorisi dapat menggunakan Koefisien Kemencengan Pearson yang diperoleh dengan menggunakan nilai selisih rata-rata dengan modus dibagi simpangan baku. Koefisien Kemencengan Pearson dirumuskan sebagai berikut s k=\\frac{\\overline{X}-M o}{s} s k=\\frac{\\overline{X}-M o}{s} dengan $$ \\overline{X}-M o \\approx 3(\\overline{X}-M e) $$ maka s k \\approx \\frac{3(\\overline{X}-M e)}{s} s k \\approx \\frac{3(\\overline{X}-M e)}{s} Gambar 2.8. Macam macam Kemiringan data (Skewness) Implementasi \u00b6 Untuk implementasi silahkan unduh data .csv import pandas as pd from scipy import stats df = pd . read_csv ( \"data.csv\" , usecols = [ 0 ]) print ( \"jumlah data \" , df [ 'NilaiPreTest' ] . count ()) print ( \"rata-rata \" , df [ 'NilaiPreTest' ] . mean ()) print ( \"nila minimal \" , df [ 'NilaiPreTest' ] . min ()) print ( \"Q1 \" , df [ 'NilaiPreTest' ] . quantile ( 0.25 )) print ( \"Q2 \" , df [ 'NilaiPreTest' ] . quantile ( 0.5 )) print ( \"Q3 \" , df [ 'NilaiPreTest' ] . quantile ( 0.75 )) print ( \"Nilai Max \" , df [ 'NilaiPreTest' ] . max ()) print ( \"kemencengan\" , \" {0:.2f} \" . format ( round ( df [ 'NilaiPreTest' ] . skew (), 2 ))) mode = stats . mode ( df ) print ( \"Nilai modus {} dengan jumlah {} \" . format ( mode . mode [ 0 ], mode . count [ 0 ])) print ( \"kemencengan \" , \" {0:.6f} \" . format ( round ( df [ 'NilaiPreTest' ] . skew (), 6 ))) print ( \"Standar Deviasi \" , \" {0:.2f} \" . format ( round ( df [ 'NilaiPreTest' ] . std (), 2 ))) print ( \"Variansi \" , \" {0:.2f} \" . format ( round ( df [ 'NilaiPreTest' ] . var (), 2 ))) Analisa Bivariate \u00b6 Dalam analisa bivariate, kita memandang dua atribut pada waktu yang sama. Kita fokus untuk memahami keterkaitan atau kebergantunga antara dua variabel atau atribut tersebut, jika ada. Kita lalu membatasi pada dua variabel X_1 X_1 dan X_2 X_2 , dengan D D dinyatakan sebagai matrik dengan ukuran n\\times2 n\\times2 X=\\begin {pmatrix} \\begin{array}{ c c } { X _ { 1 } } & { X _ { 2 } } \\\\ \\hline x _ { 11 } & { x _ { 12 } } \\\\ { x _ { 21 } } & { x _ { 22 } } \\\\ { \\vdots } & { \\vdots } \\\\ { x _ { n 1 } } & { x _ { n 2 } } \\end{array} \\end {pmatrix} X=\\begin {pmatrix} \\begin{array}{ c c } { X _ { 1 } } & { X _ { 2 } } \\\\ \\hline x _ { 11 } & { x _ { 12 } } \\\\ { x _ { 21 } } & { x _ { 22 } } \\\\ { \\vdots } & { \\vdots } \\\\ { x _ { n 1 } } & { x _ { n 2 } } \\end{array} \\end {pmatrix} Secara geometri, kita dapat memandang D D dalam dua cara. Itu dapat dianggap sebagai n n titik atau vektor dalam 2-ruang dimensi terhadap atribut X_1 X_1 dan X_2 X_2 yaitu x_i =(x_{i1},x_{i2})^T \\in \\mathbb R^2 x_i =(x_{i1},x_{i2})^T \\in \\mathbb R^2 .Selain itu dapat dilihat sebagai 2 titik atau vektor dalam n n -ruang dimensi yang berisi titik, yaitu setiap kolom adalah vektor dalam \\mathbb R^{n} \\mathbb R^{n} sebagai berikut : $$ \\left. \\begin{array} { l } { X _ { 1 } = ( x _ { 11 } , x _ { 21 } , \\ldots , x _ { n 1 } ) ^ { T } } \\ { X _ { 2 } = ( x _ { 12 } , x _ { 22 } , \\ldots , x _ { n 2 } ) ^ { T } } \\end{array} \\right. $$ Dalam sudut pandang probabilistik, vektor kolom X=(X_1,X_2)^T X=(X_1,X_2)^T dianggapa variabel acak bivariate dan titik titik x _ { i } ( 1 \\leq i \\leq n ) x _ { i } ( 1 \\leq i \\leq n ) dinyatakan sebagai sampel acak yang diperoleh dari X X , yaitu x_i x_i dianggap independent and identically distributed (iid) seperti X X . Fungsi Massa Probabilitas Gabungan Empiris \u00b6 Fungsi Massa Probabilitas Gabungan Empiris untuk X X dinyatakan dengan $$ \\hat { f } ( x ) = P ( X = x ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i } = x ) $$ \\hat { f } ( x _ { 1 } , x _ { 2 } ) = P ( X _ { 1 } = x _ { 1 } , X _ { 2 } = x _ { 2 } ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i 1 } = x _ { 1 } , x _ { i 2 } = x _ { 2 } ) \\hat { f } ( x _ { 1 } , x _ { 2 } ) = P ( X _ { 1 } = x _ { 1 } , X _ { 2 } = x _ { 2 } ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i 1 } = x _ { 1 } , x _ { i 2 } = x _ { 2 } ) dimana I I adalah variabel indikator yang bernilai 1 jika argumen argumennya benar $$ I ( x _ { i } = x ) = \\left{ \\begin{array} { l l } { 1 } & { \\text { jika } x _ { i 1 } = x _ { 1 } \\text { dan } x _ { i 2 } = x _ { 2 } } \\ { 0 } & { \\text { untuk yang lainnya } } \\end{array} \\right. $$ Seperti dalam kasus univariate, fungsi probabilitas menempatkan massa probabilitas \\frac {1}{n} \\frac {1}{n} pada setiap objek dalam data sampel. Mengukur Dispersi \u00b6 Mean \u00b6 Rata rata bivariate didefinisikan sebagai nilai harapan dari variabel acak vektor X X , didefinisikan sebagai berikut : $$ \\mu = E [ X ] = E \\left[ \\left( \\begin{array} { l } { X _ { 1 } } \\ { X _ { 2 } } \\end{array} \\right) \\right] = \\left( \\begin{array} { l } { E [ X _ { 1 } ] } \\ { E [ X _ { 2 } ] } \\end{array} \\right) = \\left( \\begin{array} { l } { \\mu _ { 1 } } \\ { \\mu _ { 2 } } \\end{array} \\right) $$ Dengan kata lain, rata-rata bivariate adalah nilai harapan dari masing masing atribut. Rata-rata sampel dapat diperoleh dari \\hat f_{x_1} \\hat f_{x_1} dan \\hat f_{x_2} \\hat f_{x_2} , fungsi massa probabilitas empiris dari X_1 X_1 dan X_2 X_2 , menggunakan persamaan (2.5). Dapat juga dihitung dari gabungan fungsi massa probabilitas empiris dalam persamaan (2.17) $$ \\hat { \\mu } = \\sum _ { x } x \\hat { f } ( x ) = \\sum _ { x } x \\left( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i } = x )\\right ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } $$ Variansi \u00b6 Kita dapat menghitung variansi masing masing atribut, yaitu \\sigma_1^2 \\sigma_1^2 untuk X_1 X_1 dan \\sigma_2^2 \\sigma_2^2 untuk X_2 X_2 mengggunkan persamaan (2.8). Variansi secara keseluruhan (1.4) dinyatakan dengan $$ var(D)=\\sigma_1^2 +\\sigma_2^2 $$ Variansi sampel \\hat \\sigma_1^2 + \\hat \\sigma_2^2 \\hat \\sigma_1^2 + \\hat \\sigma_2^2 dapat diestimasi dengan menggunakanpersamaan (2.10) dan jumlah variansi sample adalah \\sigma_1^2 +\\sigma_2^2 \\sigma_1^2 +\\sigma_2^2 Mengukur keterkaitan \u00b6 Covarian \u00b6 Kovarian antara dua atribut X_1 X_1 dan X_2 X_2 mengukur keterkaitan antara kebergantungan linier diantaranya dan didefinisikan dengan $$ \\sigma _ { 12 } = E [ ( X _ { 1 } - \\mu _ { 1 } ) ( X _ { 2 } - \\mu _ { 2 } ) ] $$ Dengan linieraritas dari harapan, kita miliki $$ \\left. \\begin{array}{l}{ \\sigma _ { 12 } = E [ ( X _ { 1 } - \\mu _ { 1 } ) ( X _ { 2 } - \\mu _ { 2 } ) ] }\\{ = E [ X _ { 1 } X _ { 2 } - X _ { 1 } \\mu _ { 2 } - X _ { 2 } \\mu _ { 1 } + \\mu _ { 1 } \\mu _ { 2 } ] }\\{ = E [ X _ { 1 } X _ { 2 } ] - \\mu _ { 2 } E [ X _ { 1 } ] - \\mu _ { 1 } E [ X _ { 2 } ] + \\mu _ { 1 } \\mu _ { 2 } }\\{ = E [ X _ { 1 } X _ { 2 } ] - \\mu _ { 1 } \\mu _ { 2 } }\\{ = E [ X _ { 1 } X _ { 2 } ] - E [ X _ { 1 } ] E [ X _ { 2 } ] }\\end{array} \\right. $$ Persamaan (2.21) dapat dianggap sebagai generalisasi dari variansi univariate persamaan (2.9) pada kasus bivariate. Jika X_1 X_1 dan X_2 X_2 adalah variabel acak saling bebas, maka kita dapat simpulkan bahwa covariannya adalah nol. Ini karena jika X_1 X_1 dan X_2 X_2 adalah saling bebas, maka kita memiliki $$ E [ X _ { 1 } X _ { 2 } ] = E [ X _ { 1 } ] \\cdot E [ X _ { 2 } ] $$ yang pada akhirnya menyiratkan bahwa $$ \\sigma{12}= 0 $$ Namaun sebaliknya tidak benar. Yaitu jika \\sigma_{12}=0 \\sigma_{12}=0 , kita tidak dapat mengklaim bahwa $X_1 $ dan X_2 X_2 adalah saling bebas. Semuanya kita katakan bahwa tidak adalah kebergantung linier antara keduanya. Kovarian sampel antra X1 X1 dan X_2 X_2 dinyatakan dengan $$ \\hat { \\sigma } _ { 12 } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i 1 } - \\hat { \\mu } _ { 1 } ) ( x _ { i 2 } - \\hat { \\mu } _ { 2 } ) $$ Korelasi \u00b6 Korelasi antara variabel X_1 X_1 dan X_2 X_2 adalah standarisasi kovarian, yang didapatkan dengan menormalisasi kovarian dengan standar deviasi masing masing variabel dinyatakan dengan \\rho _ { 12 } = \\frac { \\sigma _ { 12 } } { \\sigma _ { 1 } \\sigma _ { 2 } } = \\frac { \\sigma _ { 12 } } { \\sqrt { \\sigma _ { 1 } ^ { 2 } \\sigma _ { 2 } ^ { 2 } } } $$ Korelasi sample untuk atribut $X_1$ dan $X_2$ dinyatakan dengan $$ \\hat { \\rho } _ { 12 } = \\frac { \\hat { \\sigma } _ { 12 } } { \\hat { \\sigma } _ { 1 } \\hat { \\sigma } _ { 2 } } = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i 1 } - \\hat { \\mu } _ { 1 } ) ( x _ { i 2 } - \\hat { \\mu } _ { 2 } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i 1 } - \\hat { \\mu } _ { 1 } ) ^ { 2 } \\sum _ { i = 1 } ^ { m } ( x _ { i 2 } - \\hat { \\mu } _ { 2 } ) ^ { 2 } } } \\rho _ { 12 } = \\frac { \\sigma _ { 12 } } { \\sigma _ { 1 } \\sigma _ { 2 } } = \\frac { \\sigma _ { 12 } } { \\sqrt { \\sigma _ { 1 } ^ { 2 } \\sigma _ { 2 } ^ { 2 } } } $$ Korelasi sample untuk atribut $X_1$ dan $X_2$ dinyatakan dengan $$ \\hat { \\rho } _ { 12 } = \\frac { \\hat { \\sigma } _ { 12 } } { \\hat { \\sigma } _ { 1 } \\hat { \\sigma } _ { 2 } } = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i 1 } - \\hat { \\mu } _ { 1 } ) ( x _ { i 2 } - \\hat { \\mu } _ { 2 } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i 1 } - \\hat { \\mu } _ { 1 } ) ^ { 2 } \\sum _ { i = 1 } ^ { m } ( x _ { i 2 } - \\hat { \\mu } _ { 2 } ) ^ { 2 } } } Matrik Kovarian \u00b6 Variansi dari untuk dua atribut X_1 X_1 dan X_2 X_2 dapat diringkas dalam matrik covarianse bujursangkar denga ukuran $2 \\times 2 $ dinyatakan dengan $$ \\left. \\begin{array}{l}{ \\Sigma = E [ ( X - \\mu ) ( X - \\mu ) ^ { T } ] }\\{ = E \\left[ \\left( \\begin{array} { c } { X _ { 1 } - \\mu _ { 1 } } \\ { X _ { 2 } - \\mu _ { 2 } } \\end{array} \\right) ( X _ { 1 } - \\mu _ { 1 } \\quad X _ { 2 } - \\mu _ { 2 } ) \\right ] }\\{ = \\left( \\begin{array} { c c } { E [ ( X _ { 1 } - \\mu _ { 1 } ) ( X _ { 1 } - \\mu _ { 1 } ) ] } & { E [ ( X _ { 1 } - \\mu _ { 1 } ) ( X _ { 2 } - \\mu _ { 2 } ) ] } \\ { E [ ( X _ { 2 } - \\mu _ { 2 } ) ( X _ { 1 } - \\mu _ { 1 } ) ] } & { E [ ( X _ { 2 } - \\mu _ { 2 } ) ( X _ { 2 } - \\mu _ { 2 } ) ] } \\end{array} \\right) }\\{ = \\left( \\begin{array} { c c } { \\sigma _ { 1 } ^ { 2 } } & { \\sigma _ { 12 } } \\ { \\sigma _ { 21 } } & { \\sigma _ { 2 } ^ { 2 } } \\end{array} \\right) }\\end{array} \\right. $$ Karena \\sigma_{12}=\\sigma_{21} \\sigma_{12}=\\sigma_{21} , $\\Sigma $ adalah matrik simetris. Matrik vovarian merekam variansi tertentu atribut pada diagonal utamanya, dan informasi covarian pada elemen element bukan diagonal. Total variance dari dua atribut dinyatakan sebagai jumlah elemen elemen diagonal dari $ \\Sigma $ , yang juga disebut trace dari $ \\Sigma $ dinyatakan dengan $$ \\operatorname { var } ( D ) = \\operatorname { tr } ( \\Sigma ) = \\sigma _ { 1 } ^ { 2 } + \\sigma _ { 2 } ^ { 2 } $$ Kita segera memiliki $ tr(\\Sigma)\\geq 0$ Secara umum covarian adalah non-negatif, karena $$ | \\Sigma | = \\operatorname { det } ( \\Sigma ) = \\sigma _ { 1 } ^ { 2 } \\sigma _ { 2 } ^ { 2 } - \\sigma _ { 12 } ^ { 2 } = \\sigma _ { 1 } ^ { 2 } \\sigma _ { 2 } ^ { 2 } - \\rho _ { 12 } ^ { 2 } \\sigma _ { 1 } ^ { 2 } \\sigma _ { 2 } ^ { 2 } = ( 1 - \\rho _ { 12 } ^ { 2 } ) \\sigma _ { 1 } ^ { 2 } \\sigma _ { 2 } ^ { 2 } $$ dimana kitu gunakan persamaan (2.23), yaiut \\rho_{12}\\sigma_1\\sigma_2 \\rho_{12}\\sigma_1\\sigma_2 . dengan |\\Sigma| |\\Sigma| adalah determinan dari matrik kovarian. Perhatikan bahwa |\\rho_{12}|\\leq 1 |\\rho_{12}|\\leq 1 menyebabkan \\rho_{12}^2 \\leq 1 \\rho_{12}^2 \\leq 1 sehingga det (\\Sigma) \\geq 1 (\\Sigma) \\geq 1 furthermore determinannya adalah non-negative. Matrik kovarian sampel dinyatakan dengan $$ \\hat { \\Sigma } = \\left( \\begin{array} { l l } { \\hat { \\sigma } _ { 1 } ^ { 2 } } & { \\hat { \\sigma } _ { 12 } } \\ { \\hat { \\sigma } _ { 12 } } & { \\hat { \\sigma } _ { 2 } ^ { 2 } } \\end{array} \\right) $$ Matrik kovarian sampe $ \\hat \\Sigma$ memilki karakteristik sama seperti \\Sigma \\Sigma , yaitu simetris dan |\\hat \\Sigma| \\geq 0 |\\hat \\Sigma| \\geq 0 dan itu dapat digunakan untum memudahkan mendapatkan total sampel dan variansi secara umum Contoh (Rata rata Sampel dan Covarian) Perhatikan atribut sepal length dan sepal width untuk data iris, seperti yang diplot dalam gambar 2.4. Ada n=150 data dalam d=2 d=2 ruang dimensi. Rata rata sampel adalah $$ \\hat { \\mu } = \\left( \\begin{array} { l } { 5.843 } \\ { 3.054 } \\end{array} \\right) $$ Matrik covarian dinyatakan dengan $$ \\hat { \\Sigma } = \\left( \\begin{array} { r r } { 0.681 } & { - 0.039 } \\ { - 0.039 } & { 0.187 } \\end{array} \\right) $$ Variansi untuk sepal length adalah \\hat \\sigma_1^2=0.681 \\hat \\sigma_1^2=0.681 dan sepal width adalah \\hat \\sigma_2^2=0.187 \\hat \\sigma_2^2=0.187 . Covarian antara dua atribut adalah \\hat \\sigma_{12}=-0.039 \\hat \\sigma_{12}=-0.039 dan korelasi antara dua atribut tersebut adalah $$ \\hat { \\rho } _ { 12 } = \\frac { - 0.039 } { \\sqrt { 0.681 \\cdot 0.187 } } = - 0.109 $$ Lalu, ada korelasi yang sangat lemah antara dua atribut tersebut Total variansi sampel dinyatakan dengan $$ \\operatorname { tr } ( \\hat { \\Sigma } ) = 0.681 + 0.187 = 0.868 $$ dan variansi secara umum dinyatakan dengan $$ \\hat { \\Sigma } | = \\operatorname { det } ( \\hat { \\Sigma } ) = 0.681 \\cdot 0.187 - ( - 0.039 ) ^ { 2 } = 0.126 $$ Analisa Multivariate \u00b6 Dalam analisa multivariate, kita melihat atribut numerik dengan d d dimensi X_1,X_2,...X_d X_1,X_2,...X_d . Data dinyatakan degan matrik n\\times d n\\times d seperti berikut $$ D = \\left( \\begin{array} { c c c c } { X _ { 1 } } & { X _ { 2 } } & { \\cdots } & { X _ { d } } \\ \\hline x _ { 11 } & { x _ { 12 } } & { \\cdots } & { x _ { 1 d } } \\ { x _ { 21 } } & { x _ { 22 } } & { \\cdots } & { x _ { 2 d } } \\ { \\vdots } & { \\vdots } & { \\ddots } & { \\vdots } \\ { x _ { n 1 } } & { x _ { n 2 } } & { \\cdots } & { x _ { n d } } \\end{array} \\right) $$ Jika dilihat dari baris data memiliki n n objek atatu vektor dalam d d ruang dimensi atribut $$ x _ { i } = ( x _ { i 1 } , x _ { i 2 } , \\ldots , x _ { i d } ) ^ { T } \\in \\mathbb R ^ { d } $$ Jika dilihat dari sudut pandang kolom, data diangga sebagai d d objek atau vektor dalam n n dimensi ruang dengan titik-titik data $$ X _ { j } = ( x _ { 1 j } , x _ { 2 j } , \\ldots , x _ { n j } ) ^ { T } \\in R ^ { n } $$ Jika dilihat dari sudut pandang probabilitas, d d atribut dimodelkan dengan variabel acak vektor X=(X_1,X_2,...X_d)^T X=(X_1,X_2,...X_d)^T dan titik titik x_i x_i dianggap sebagai sampel acak yang diperoleh dari X X , atribut atribut tersebut independent and identfically distributed dari X X (i.i.d X X ) Mean \u00b6 Generalisasi persamaan (2.18) rata-rata vektor multivariate diperoleh dari masing-masing atribut yang dinyatakan dengan $$ \\mu = E [ X ] = \\left( \\begin{array} { c } { E [ X _ { 1 } ] } \\ { E [ X _ { 2 } ] } \\ { \\vdots } \\ { E [ X _ { d } ] } \\end{array} \\right) = \\left( \\begin{array} { c } { \\mu _ { 1 } } \\ { \\mu _ { 2 } } \\ { \\vdots } \\ { \\mu _ { d } } \\end{array} \\right) $$ Generalisasi persamaan (2.19) rata-rata sampel dinyatakan dengan $$ \\hat { \\mu } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } $$ Matrik Kovarian \u00b6 Generalisasi persamaan (2.26) untuk d d dimensi, kovarian multicovariate di dinyatakan dengan matrik kovarian simetris $ d\\times d $yang menyatakan kovarian untuk setiap pasangan atribut $$ \\Sigma = E [ ( X - \\mu ) ( X - \\mu ) ^ { T } ] = \\left( \\begin{array} { c c c c } { \\sigma _ { 1 } ^ { 2 } } & { \\sigma _ { 12 } } & { \\cdots } & { \\sigma _ { 1 d } } \\ { \\sigma _ { 21 } } & { \\sigma _ { 2 } ^ { 2 } } & { \\cdots } & { \\sigma _ { 2 d } } \\ { \\cdots } & { \\cdots } & { \\cdots } & { \\cdots } \\ { \\sigma _ { d 1 } } & { \\sigma _ { d 2 } } & { \\cdots } & { \\sigma _ { d } ^ { 2 } } \\end{array} \\right) $$ Elemen diagonal $\\sigma_i^2 $ menyatakan variansi atribut X_i X_i , dimana elemen-elemen bukan diagonal \\sigma_{ij} = \\sigma_{ji} \\sigma_{ij} = \\sigma_{ji} menyatakan kovarian antara atribut pasangan X_i X_i dan X_j X_j . Matrik kovarian adalah positif semidefinite Contoh Rata-rata sample dan matrik covarian. Perhatikan semua atribut numerik untuk data iris, namanya sepal length, petal length, dan petal width. Rata rata multivarean dinyatakan dengan \\hat { \\mu } = ( 5.843 \\quad 3.054 \\quad 3.759 \\quad 1.199 ) ^ { T } $$ dan matrik covarian nya adalah $$ \\hat { \\Sigma } = \\left( \\begin{array} { r r r r } { 0.681 } & { - 0.039 } & { 1.265 } & { 0.513 } \\\\ { - 0.039 } & { 0.187 } & { - 0.320 } & { - 0.117 } \\\\ { 1.265 } & { - 0.320 } & { 3.092 } & { 1.288 } \\\\ { 0.513 } & { - 0.117 } & { 1.288 } & { 0.579 } \\end{array} \\right) $$ Jumlah variansi adalah $$ \\operatorname { var } ( D ) = \\operatorname { tr } ( \\hat { \\Sigma } ) = 0.681 + 0.187 + 3.092 + 0.579 = 4.539 \\hat { \\mu } = ( 5.843 \\quad 3.054 \\quad 3.759 \\quad 1.199 ) ^ { T } $$ dan matrik covarian nya adalah $$ \\hat { \\Sigma } = \\left( \\begin{array} { r r r r } { 0.681 } & { - 0.039 } & { 1.265 } & { 0.513 } \\\\ { - 0.039 } & { 0.187 } & { - 0.320 } & { - 0.117 } \\\\ { 1.265 } & { - 0.320 } & { 3.092 } & { 1.288 } \\\\ { 0.513 } & { - 0.117 } & { 1.288 } & { 0.579 } \\end{array} \\right) $$ Jumlah variansi adalah $$ \\operatorname { var } ( D ) = \\operatorname { tr } ( \\hat { \\Sigma } ) = 0.681 + 0.187 + 3.092 + 0.579 = 4.539 Contoh Perkalian dalam dan perkalian luar . Untuk mengdeskripsikan komputasi perkalian dalam dan perkalian luar dari matrik covarian, perhatikan data 2-dimensi $$ D = \\left( \\begin{array} { l l } { A _ { 1 } } & { A _ { 2 } } \\ \\hline 1 & { 0.8 } \\ { 5 } & { 2.4 } \\ { 9 } & { 5.5 } \\end{array} \\right) $$ Rata-rata vektor adalah sebagai berikut $$ \\hat { \\mu } = \\left( \\begin{array} { l } { \\hat { \\mu } _ { 1 } } \\ { \\hat { \\mu } _ { 2 } } \\end{array} \\right) = \\left( \\begin{array} { l } { 15 / 3 } \\ { 8.7 / 3 } \\end{array} \\right) = \\left( \\begin{array} { c } { 5 } \\ { 2.9 } \\end{array} \\right) $$ dan matrik data terpusat dinyatakan $$ Z = D - 1 \\cdot \\mu ^ { T } = \\left( \\begin{array} { l l } { 1 } & { 0.8 } \\ { 5 } & { 2.4 } \\ { 9 } & { 5.5 } \\end{array} \\right) - \\left( \\begin{array} { l } { 1 } \\ { 1 } \\ { 1 } \\end{array} \\right) \\left( \\begin{array} { l l } { 5 } & { 2.9 } \\end{array} \\right) = \\left( \\begin{array} { r r } { - 4 } & { - 2.1 } \\ { 0 } & { - 0.5 } \\ { 4 } & { 2.6 } \\end{array} \\right) $$ Pendekatan perkalian dalam [pers. 2.30] untuk menghitung matrik kovarian adalah $$ \\left. \\begin{array}{l}{ \\hat { \\Sigma } = \\frac { 1 } { n } Z ^ { T } Z = \\frac { 1 } { 3 } \\left( \\begin{array} { c c c } { - 4 } & { 0 } & { 4 } \\ { - 2.1 } & { - 0.5 } & { 2.6 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { - 4 } & { - 2.1 } \\ { 0 } & { - 0.5 } \\ { 4 } & { 2.6 } \\end{array} \\right) }\\{ = \\frac { 1 } { 3 } \\left( \\begin{array} { c c } { 32 } & { 18.8 } \\ { 18.8 } & { 11.42 } \\end{array} \\right) = \\left( \\begin{array} { c c } { 10.67 } & { 6.27 } \\ { 6.27 } & { 3.81 } \\end{array} \\right) }\\end{array} \\right. $$ Pendekatan lain yaitu dengan perkalian luar [pers. 2.31] dibyatakan dengan $$ \\hat { \\Sigma } = \\frac { 1 } { n } \\sum _ { j = 1 } ^ { n } z _ { i } \\cdot z _ { i } ^ { T } $$ = \\frac { 1 } { 3 } \\left [ \\left( \\begin{array} { c } { - 4 } \\\\ { - 2.1 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { - 4 } & { - 2.1 } \\end{array} \\right) + \\left( \\begin{array} { r r } { 0 } \\\\ { - 0.5 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { 0 } & { - 0.5 } \\end{array} \\right) + \\left( \\begin{array} { c } { 4 } \\\\ { 2.6 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { 4 } & { 2.6 } \\end{array} \\right)\\right ] = \\frac { 1 } { 3 } \\left [ \\left( \\begin{array} { c } { - 4 } \\\\ { - 2.1 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { - 4 } & { - 2.1 } \\end{array} \\right) + \\left( \\begin{array} { r r } { 0 } \\\\ { - 0.5 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { 0 } & { - 0.5 } \\end{array} \\right) + \\left( \\begin{array} { c } { 4 } \\\\ { 2.6 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { 4 } & { 2.6 } \\end{array} \\right)\\right ] \\left. \\begin{array} { l } { = \\frac { 1 } { 3 } [ \\left( \\begin{array} { c c } { 16.0 } & { 8.4 } \\\\ { 8.4 } & { 4.41 } \\end{array} \\right) + \\left( \\begin{array} { c c } { 0.0 } & { 0.0 } \\\\ { 0.0 } & { 0.25 } \\end{array} \\right) + \\left( \\begin{array} { c c } { 16.0 } & { 10.4 } \\\\ { 10.4 } & { 6.76 } \\end{array} \\right) ] } \\\\ { = \\frac { 1 } { 3 } \\left( \\begin{array} { c c } { 32.0 } & { 18.8 } \\\\ { 18.8 } & { 11.42 } \\end{array} \\right) = \\left( \\begin{array} { c c } { 10.67 } & { 6.27 } \\\\ { 6.27 } & { 3.81 } \\end{array} \\right) } \\end{array} \\right. \\left. \\begin{array} { l } { = \\frac { 1 } { 3 } [ \\left( \\begin{array} { c c } { 16.0 } & { 8.4 } \\\\ { 8.4 } & { 4.41 } \\end{array} \\right) + \\left( \\begin{array} { c c } { 0.0 } & { 0.0 } \\\\ { 0.0 } & { 0.25 } \\end{array} \\right) + \\left( \\begin{array} { c c } { 16.0 } & { 10.4 } \\\\ { 10.4 } & { 6.76 } \\end{array} \\right) ] } \\\\ { = \\frac { 1 } { 3 } \\left( \\begin{array} { c c } { 32.0 } & { 18.8 } \\\\ { 18.8 } & { 11.42 } \\end{array} \\right) = \\left( \\begin{array} { c c } { 10.67 } & { 6.27 } \\\\ { 6.27 } & { 3.81 } \\end{array} \\right) } \\end{array} \\right. dimana data terpusat z_i z_i adalah baris dari Z Z Atribut Kategorikal \u00b6 Kita asumsikan bahwa data terdiri dari satu atribut X X . Domain dari X X terdiri dari m m nilai simbolis dom(X)={a_1,a_2,...a_m} dom(X)={a_1,a_2,...a_m} . Data D D adalah n\\times 1 n\\times 1 matrik data simbolis yang dinyatakan dengan $$ D = \\left( \\begin{array} { c } { X } \\ { x _ { 1 } } \\ { x _ { 2 } } \\ { \\vdots } \\ { x _ { n } } \\end{array} \\right) $$ dimana setiap nilai x_i \\in dom(X) x_i \\in dom(X) Variabel Bernouli \u00b6 Marilah kita lihat kasus ketika atribut kategorikal X X memililik domain $ {a_1,a_2}$ dengan m=2 m=2 . Kita dapat memodelkan X X sebagai variabel acak Bernouli, yang didasarkan pada dua nilai berbeda yaitu 1 dan 0, sesuai dengan pemetaan $$ X ( v ) = \\left{ \\begin{array} { l l } { 1 } & { \\text { if } v = a _ { 1 } } \\ { 0 } & { \\text { if } v = a _ { 2 } } \\end{array} \\right. $$ Fungsi massa probabilitas (PMF) dari X X dinyatakan dengan $$ P ( X = x ) = f ( x ) = \\left{ \\begin{array} { l l } { p _ { 1 } } & { \\text { if } x = 1 } \\ { p _ { 0 } } & { \\text { if } x = 0 } \\end{array} \\right. $$ dimana p_1 p_1 dan p_0 p_0 adalah parameter distribusi, yang harus memenuhi kondisi $$ p_1+p_0=1 $$ Karena hanya ada satu parameter bebas, biasanya menotasikan p_1=p p_1=p maka p_0=1-p p_0=1-p . Fungsi Massa Probabilitas dari variabel acak Bernouli X X dapat kemudian ditulis dengan $$ P ( X = x ) = f ( x ) = p ^ { x } ( 1 - p ) ^ { 1 - x } $$ Kita dapat melihat bahwa P ( X = 1 ) = p ^ { 1 } ( 1 - p ) ^ { 0 } = p \\text { and } P ( X = 0 ) = p ^ { 0 } ( 1 - p ) ^ { 1 } = 1 - p P ( X = 1 ) = p ^ { 1 } ( 1 - p ) ^ { 0 } = p \\text { and } P ( X = 0 ) = p ^ { 0 } ( 1 - p ) ^ { 1 } = 1 - p seperti yand diharapkan Mean dan Variansi Nilai harapan dari X X dinyatakan dengan $$ \\mu = E [ X ] = 1 \\cdot p + 0 \\cdot ( 1 - p ) = p $$ dan variansi dari X X dinyatakan dengan $$ \\left. \\begin{array}{l}{ \\sigma ^ { 2 } = \\operatorname { var } ( X ) = E [ X ^ { 2 } ] - ( E [ X ] ) ^ { 2 } }\\ \\hspace{7mm}= ( 1 ^ { 2 } \\cdot p + 0 ^ { 2 } \\cdot ( 1 - p ) ) - p ^ { 2 } = p - p ^ { 2 } = p ( 1 - p ) \\\\end{array} \\right. $$ Rata-rata sampel dan Variansi Untuk mengestimasi parameter dari variabel Bernouli X X , kita asumsikan bahwa setiap simbol dipetakan ke nilai biner. Sehingga, sekumpulan nilai {x_1,x_2,...x_n} {x_1,x_2,...x_n} diasumsikan menjadi sampel acak yang diperoleh dari X X (yaitu setiap $ x_i$ adalah IID dengan X X . Rata-rata sampel dinyatakan dengan $$ \\hat { \\mu } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } = \\frac { n _ { 1 } } { n } = \\hat { p } $$ dimana n_1 n_1 adalah banyaknya titik dengan x_1=1 x_1=1 dalam sampel acak (sama dengan banyak kejadian dari simbol a_1 a_1 ) Misal n_0=n-n_1 n_0=n-n_1 menyatakan banyak titik dengan x_i=0 x_i=0 dalam sampel acak. Variansi sample dinyatakan dengan $$ \\left. \\begin{array}{l}{ \\hat { \\sigma } ^ { 2 } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\hat { \\mu } ) ^ { 2 } }\\ \\hspace{7mm}{ = \\frac { n _ { 1 } } { n } ( 1 - \\hat { p } ) ^ { 2 } + \\frac { n - n _ { 1 } } { n } ( - \\hat { p } ) ^ { 2 } }\\\\hspace{7mm}{ = \\hat { p } ( 1 - \\hat { p } ) ^ { 2 } + ( 1 - \\hat { p } ) \\hat { p } ^ { 2 } }\\\\hspace{7mm}{ = \\hat { p } ( 1 - \\hat { p } ) ( 1 - \\hat { p } + \\hat { p } ) }\\\\hspace{7mm}{ = \\hat { p } ( 1 - \\hat { p } ) }\\end{array} \\right. $$ Variansi sampel dapat juga diperoleh langsung dari persamaan(3.1) dengan mensubsitusikan \\hat p \\hat p untuk p p . Contoh Perhatikan atribut sepal length ( X X ) untuk dataset iris dalam tabel 1.1. Marilah kita definisikan bunga iris dengan Long jika bunga itu sepal length dalam range [7, \\infty ] [7, \\infty ] , dan short jika sepal length dalam range [-\\infty,7] [-\\infty,7] . Kemudian X_1 X_1 dapat dinyatakan dengan atribut kategorikan dengan domain {Long,Short}. Dari sampel yang diamati ukuran n=150 n=150 , kita menemukan 13 iris long. Rata-rata sampel dari X_1 X_1 adalah $$ \\hat { \\mu } = \\hat { p } = 13 / 150 = 0.087 $$ dan variansinya adalah $$ \\hat { \\sigma } ^ { 2 } = \\hat { p } ( 1 - \\hat { p } ) = 0.087 ( 1 - 0.087 ) = 0.087 \\cdot 0.913 = 0.079 $$ Ditribusi binomial : banyaknya kejadian \u00b6 Diberikan variabel Bernoulli X X , misal \\{x_1,x_2,...x_n\\} \\{x_1,x_2,...x_n\\} menyatakan sampel acak dari ukuran n n yang diperoleh dari X X . Misal N N adalah variabel acak yang menyatakan numlah kejadi dari simbol a_1 a_1 (nilai X=1 X=1 ) dalam sampe. N adalah distribusi binomial yang dinyatakan dengan $$ f ( N = n _ { 1 } | n , p ) = \\left( \\begin{array} { l } { n } \\ { n _ { 1 } } \\end{array} \\right) p ^ { n _ { 1 } } ( 1 - p ) ^ { n - n _ { 1 } } $$ Dalam kenyataannya, N N adalah jumlah dari n n variabel acak Bernoulli x_i x_i yang saling bebas dan (IID) dengan X X yaitu N=\\sum_{i=1}^n x_i N=\\sum_{i=1}^n x_i . Dengan liniearitas dari ekpektasi, mean atau jumlah harapan dari kejadian simbol a_i a_i dinyatakan dengan $$ \\mu _ { N } = E [ N ] = E \\left[ \\sum _ { i = 1 } ^ { n } x _ { i } \\right] = \\sum _ { i = 1 } ^ { n } E [ x _ { i } ] = \\sum _ { i = 1 } ^ { n } p = n p $$ Karena x_i x_i adalah semuanya saling bebas, variansi dari N N dinyatakan dengan $$ \\sigma _ { N } ^ { 2 } = \\operatorname { var } ( N ) = \\sum _ { i = 1 } ^ { n } \\operatorname { var } ( x _ { i } ) = \\sum _ { i = 1 } ^ { n } p ( 1 - p ) = n p ( 1 - p ) $$ Contoh 3.2. Dengan meneruskan contoh 3.1, kita dapat menggunakan parameter yang telah diestimasi \\hat p=0.087 \\hat p=0.087 untuk menghitung banyaknya kejadian yang diharapkan N long dari sepal length. distribusi binomial Iris $$ E [ N ] = n \\hat { p } = 150 \\cdot 0.087 = 13 $$ Dalam kasus ini, karena p p dihitung dari sample melalui \\hat p \\hat p , tidak mengherankan bahwa jumlah kejadian diharapkan dari Long Iris sama dengan kejadian yang sebenarnya. Akan tetapi yang lebih menarik adalah kita dapat menghitung variansi jumlah kejadian $$ \\operatorname { var } ( N ) = n \\hat { p } ( 1 - \\hat { p } ) = 150 \\cdot 0.079 = 11.9 $$ Meningkatnya ukuran sample, distribusi binomial seperti yang diberikan dapalam persamaan 3.3 cenderung ke distribusi normal dengan \\mu=13 \\mu=13 dan \\sigma=\\sqrt{11.9}=3.45 \\sigma=\\sqrt{11.9}=3.45 . Sehingga dengan kepercaan lebih besar dari 95%, kita dapat mengklam bahwa jumlah kejadian dari a_i a_i akan terletak dalam rentang \\mu \\pm 2 \\sigma = [ 9.55,16.45 ] \\mu \\pm 2 \\sigma = [ 9.55,16.45 ] yang mengikuti dari fakta bahwa untuk distribusi normal 95,45% dari massa probabilitas terletak dalam dua standar deviasi dari rata-rata. Variable multivariate Bernoulli \u00b6 Sekarang kita memandang kasus umum ketika X X adalah atribut kategorical dengan domain \\{a_1,a_2,...a_m\\} \\{a_1,a_2,...a_m\\} . Kita dapat memodelkan X X sebagai variabel acak Bernoulli m m -dimensi X = ( A _ { 1 } , A _ { 2 } , \\ldots , A _ { m } ) ^ { T } X = ( A _ { 1 } , A _ { 2 } , \\ldots , A _ { m } ) ^ { T } dimana setiap A_i A_i adalah variabel Bernoulli dengan parameter p_i p_i yang menotasikan probabilitas dari pengamatan simbol a_i a_i . Akan tetapi karena X X dapat mengasumsikan hanya satu dari nilai simbolik pada suatu waktum jika X=a_i X=a_i maka A_i=1 A_i=1 dan A_j=0 A_j=0 untuk semua j \\neq i j \\neq i . Variabel acak X \\in {0,1}^m X \\in {0,1}^m , dan jika X=a_i X=a_i , maka X=e_i X=e_i , dimana e_i e_i adalah standar vektor basis ke i, e_i\\in\\mathbb R^m e_i\\in\\mathbb R^m yang dinyatakan dengan $$ e _ { i } = ( \\overbrace { 0 , \\ldots , 0 } ^ { i - 1 } , 1 , \\overbrace { 0 , \\ldots , 0 } ^ { m - i } ) ^ { T } $$ Pada e_i e_i hanya elemen ke i adalah 1 ( e_{ii}=1 e_{ii}=1 ) , sedangkan semua elemen yang lain adalah nol, ( e_{ij}=0, \\forall j \\neq i e_{ij}=0, \\forall j \\neq i ). Disini, definis yang lebih tepat dari variabel Bernoulli multivariate , yaitu generalisasi dari variabel Bernoullii dari dua hasil ke m m hasil. Kita kemudian memodelkan atribut kategorical X X sebagai variabel Bernoulli multivariate X X didefinisikan dengan $$ X ( v ) = e _ { i } \\text { if } v = a _ { i } $$ Rentang dari X X terdiri dari m m nilai vektor berbeda \\{e_1,e_2,...e_m\\} \\{e_1,e_2,...e_m\\} dengan fungsi massa probabilitas dari X X dinyatakan dengan $$ P ( X = e _ { i } ) = f ( e _ { i } ) = p _ { i } $$ dimana p_i p_i adalah probabilitas dari nilai pengamatan a_i a_i . Parameter ini harus memenuhi kondisi $$ \\sum _ { i = 1 } ^ { m } p _ { i } = 1 $$ Fungsi massa prababilitas dapat ditulis secara utuh sebagai berikut $$ P ( X = e _ { i } ) = f ( e _ { i } ) = \\prod _ { j = 1 } ^ { m } p _ { j } ^ { e _ { i j } }Ka $$ Kareana e_ii=1 e_ii=1 dan e_ij=0 e_ij=0 funtuk $ j\\neq i$, kita dapat melihat bahwa, seperti yang diharapkan, kita miliki $$ f ( e _ { i } ) = \\prod _ { j=1 } ^ { m } p _ { j } ^ { e _ { i j } } = p _ { 1 } ^ { e _ { i 0 } } \\times \\cdots p _ { i } ^ { e _ { i i } } \\cdots \\times p _ { m } ^ { e _ { i m } } = p _ { 1 } ^ { 0 } \\times \\cdots p _ { i } ^ { 1 } \\cdots \\times p _ { m } ^ { 0 } = p _ { i } $$ \\left. \\begin{array} { | l | l | l | } \\hline \\text { Bins } & { { \\text { Domain } } } & { { \\text { Counts } } } \\\\ \\hline [ 4.3,5.2 ] & { \\text { Very Short } ( a _ { 1 } ) } & { n _ { 1 } = 45 } \\\\ { ( 5.2,6.1 ] } & { \\text { Short } ( a _ { 2 } ) } & { n _ { 2 } = 50 } \\\\ { ( 6.1,7.0 ] } & { \\text { Long } ( a _ { 3 } ) } & { n _ { 3 } = 43 } \\\\ { ( 7.0,7.9 ] } & { \\text { Very Long } ( a _ { 4 } ) } & { n _ { 4 } = 12 } \\\\ \\hline \\end{array} \\right. \\left. \\begin{array} { | l | l | l | } \\hline \\text { Bins } & { { \\text { Domain } } } & { { \\text { Counts } } } \\\\ \\hline [ 4.3,5.2 ] & { \\text { Very Short } ( a _ { 1 } ) } & { n _ { 1 } = 45 } \\\\ { ( 5.2,6.1 ] } & { \\text { Short } ( a _ { 2 } ) } & { n _ { 2 } = 50 } \\\\ { ( 6.1,7.0 ] } & { \\text { Long } ( a _ { 3 } ) } & { n _ { 3 } = 43 } \\\\ { ( 7.0,7.9 ] } & { \\text { Very Long } ( a _ { 4 } ) } & { n _ { 4 } = 12 } \\\\ \\hline \\end{array} \\right. Contoh : Marilah kita lihat atribut sepal length ( X_1 X_1 ) untuk data Iris seperti yang ditunjukkan dalam tabel 1.2. Kita membagi sepal length kedalam empat interval yang sama, dan memberikan nama untuk setiap interval seperti yang diunjukkan dalam tabel 3.1. Kita lihat X_1 X_1 sebagai atribut kategorical dengan domain $$ {a _ { 2 } = \\text { VeryShort, } a _ { 2 } = \\text { Short, } a _ { 3 } = \\operatorname { Long } , a _ { 4 } = \\operatorname{Very Long}} $$ Kita memodelkan atribut kategorical X_1 X_1 sebagai variabel X X Bernoulli multivariate, didefinisikan dengan $$ X ( v ) = \\left{ \\begin{array} { l l } { e _ { 1 } = ( 1,0,0,0 ) } & { \\text { jika } v = a _ { 1 } } \\ { e _ { 2 } = ( 0,1,0,0 ) } & { \\text { jika } v = a _ { 2 } } \\ { e _ { 3 } = ( 0,0,1,0 ) } & { \\text { jika } v = a _ { 3 } } \\ { e _ { 4 } = ( 0,0,0,1 ) } & { \\text { jika } v = a _ { 4 } } \\end{array} \\right. $$ Misalkan, simbol x_1=Short=a_2 x_1=Short=a_2 dinyatakan dengan (0,1,0,0)^T=e_2 (0,1,0,0)^T=e_2 Mean Mean atau nilai harapan dari X X dapat diperoleh dengan $$ \\mu = E [ X ] = \\sum _ { i = 1 } ^ { m } e _ { i } f ( e _ { i } ) = \\sum _ { i = 1 } ^ { m } e _ { i } p _ { i } = \\left( \\begin{array} { l } { 1 } \\ { 0 } \\ { \\vdots } \\ { 0 } \\end{array} \\right) p _ { 1 } + \\cdots + \\left( \\begin{array} { l } { 0 } \\ { 0 } \\ { \\vdots } \\ { 1 } \\end{array} \\right) p _ { m } = \\left( \\begin{array} { c } { p _ { 1 } } \\ { p _ { 2 } } \\ { \\vdots } \\ { p _ { m } } \\end{array} \\right) = p $$ Mengukur Jarak Data \u00b6 Mengukur Jarak Tipe Numerik \u00b6 Salah satu tantangan dalam era ini dengan datatabase yang memiliki banyak tipe data. Mngukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan Sebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaiut v_1, v_2 v_1, v_2 menyatakandua vektor yang menyatakan v_1 = {x_1, x_2, . . ., x_n}, v_2 ={y_1, y_2, . . ., y_n}, v_1 = {x_1, x_2, . . ., x_n}, v_2 ={y_1, y_2, . . ., y_n}, dimana x_i, y_i x_i, y_i disebut attribut. Ada beberapa ukuran similaritas datau ukuran jarak, diantaranya 2 Minkowski Distance \u00b6 Kelompk Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski distance. Minkowski distance dinyatakan dengan d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 diman m m adalah bilangan riel positif dan x_i x_i dan $ y_i$ adalah dua vektor dalam runang dimensi n n Implementasi ukuran jarak Minkowski pada model clustering data atribut dilakukan normalisasi untuk menghindari dominasi dari atribut yang memiliki skala data besar. Manhattan distance \u00b6 Manhattan distance adalah kasus khsusu dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. BIla ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| Euclidean distance \u00b6 Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini. Average Distance \u00b6 Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adala versi modikfikasid ari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x,y x,y dalam ruang dimensi n n , rata-rata jarak didefinisikan dengan d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } } d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } } Weighted euclidean distance \u00b6 Jika berdasarkan tingkatan penting dari masing masing atribut ditentukan, maka Weighted Euclidean distance adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. Ukuran ini dirumuskan dengan $$ d _ { w e } = \\left ( \\sum _ { i = 1 } ^ { n } w _ { i } ( x _ { i } - y _ { i } \\right) ^ { 2 } ) ^ { \\frac { 1 } { 2 } } $$ dimana w_i w_i adalah bobot yang diberikan pada atribut ke i. Chord distance \u00b6 Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan d _ { \\text {chord} } = \\left ( 2 - 2 \\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { \\| x \\| _ { 2 } \\| y \\| _ { 2 } } \\right) ^ { \\frac { 1 } { 2 } } d _ { \\text {chord} } = \\left ( 2 - 2 \\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { \\| x \\| _ { 2 } \\| y \\| _ { 2 } } \\right) ^ { \\frac { 1 } { 2 } } dimana \\| x \\|_ {2} \\| x \\|_ {2} adalah L^{2} \\text {-norm} \\| x \\|_{2} = \\sqrt { \\sum_{ i = 1 }^{ n }x_{i}^{2}} L^{2} \\text {-norm} \\| x \\|_{2} = \\sqrt { \\sum_{ i = 1 }^{ n }x_{i}^{2}} Mahalanobis distance \u00b6 Mahalanobis distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antra data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. Mahalanobis distance dinyatakan dengan d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } diman S S adalah matrik covariance data. Cosine measure \u00b6 Ukuran Cosine similarity lebih banyak digunakan dalam similaritas dokumen dan dinyatakan dengan Cosine(x,y)=\\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { \\| x \\| _ { 2 } \\| y \\| _ { 2 } } Cosine(x,y)=\\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { \\| x \\| _ { 2 } \\| y \\| _ { 2 } } dimana \\|y\\|_{2} \\|y\\|_{2} adalah Euclidean norm dari vektor y=(y_{1} , y_{2} , \\dots , y_{n} ) y=(y_{1} , y_{2} , \\dots , y_{n} ) didefinisikan dengan \\|y\\|_{2}=\\sqrt{ y _ { 1 } ^ { 2 } + y _ { 2 } ^ { 2 } + \\ldots + y _ { n } ^ { 2 } } \\|y\\|_{2}=\\sqrt{ y _ { 1 } ^ { 2 } + y _ { 2 } ^ { 2 } + \\ldots + y _ { n } ^ { 2 } } Pearson correlation \u00b6 Pearson correlation banyak digunakan dalam data expresi gen. Ukuran similaritas ini menghitung similaritas antara duan bentuk pola expresi gen. Pearson correlation didefinisikan dengan Pearson ( x , y ) = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\mu _ { x } ) ( y _ { i } - \\mu _ { y } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } } Pearson ( x , y ) = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\mu _ { x } ) ( y _ { i } - \\mu _ { y } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } } The Pearson correlation kelemahannya adalah sensitif terhadap outlier Mengukur Jarak Atribut Binary \u00b6 Mari kita lihat similaritas dan desimilirity untuk objek yang dijelaskan oleh atribut biner simetris atau asimetris. Aatribut biner hanya memiliki dua status: 0 dan 1 Contoh atribut perokok menggambarkan seorang pasien, misalnya, 1 menunjukkan bahwa pasien merokok, sedangkan 0 menunjukkan pasien tidak merokok. Memperlakukan atribut biner sebagai atribut numerik tidak diperkenankan. Oleh karena itu, metode khusus untuk data biner diperlukan untuk membedakan komputasi. Jadi, bagaimana kita bisa menghitung ketidaksamaan antara dua atribut biner? \u201dSatu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2 \\times 2 2 \\times 2 di mana q q adalah jumlah atribut yang sama dengan 1 untuk kedua objek i i dan j j , r r adalah jumlah atribut yang sama dengan 1 untuk objek i i tetapi 0 untuk objek j j , s s adalah jumlah atribut yang sama dengan 0 untuk objek i i tetapi 1 untuk objek j j , dan t t adalah jumlah atribut yang sama dengan 0 untuk kedua objek i i dan j j . Jumlah total atribut adalah p p , di mana p=q+r+s+t p=q+r+s+t Ingatlah bahwa untuk atribut biner simetris, masing-masing nilai bobot yang sama.Dissimilarity yang didasarkan pada atribut aymmetric binary disebut symmetric binary dissimilarity. Jika objek i dan j dinyatakan sebagai atribut biner simetris, maka dissimilarity antar i i dan j j adalah d ( i , j ) = \\frac { r + s } { q + r + s + t } d ( i , j ) = \\frac { r + s } { q + r + s + t } Untuk atribut biner asimetris, kedua kondisi tersebut tidak sama pentingnya, seperti hasil positif (1) dan negatif (0) dari tes penyakit. Diberikan dua atribut biner asimetris, pencocokan keduanya 1 (kecocokan positif) kemudian dianggap lebih signifikan daripada kecocokan negatif. Ketidaksamaan berdasarkan atribut-atribut ini disebut asimetris biner dissimilarity, di mana jumlah kecocokan negatif, t, dianggap tidak penting dan dengan demikian diabaikan. Berikut perhitungannya d ( i , j ) = \\frac { r + s } { q + r + s } d ( i , j ) = \\frac { r + s } { q + r + s } Kita dapat mengukur perbedaan antara dua atribut biner berdasarkan pada disimilarity. Misalnya, biner asimetris kesamaan antara objek i i dan j j dapat dihitung dengan \\operatorname { sim } ( i , j ) = \\frac { q } { q + r + s } = 1 - d ( i , j ) \\operatorname { sim } ( i , j ) = \\frac { q } { q + r + s } = 1 - d ( i , j ) Persamaan similarity ini disebut dengan Jaccard coefficient Mengukur Jarak Tipe categorical \u00b6 Ada beberapa macam pengukuran untuk tipe data categorical 3 Overlay Metric \u00b6 Ketika semua atribut adalah bertipe nominal, ukuran jarak yang paling sederhana adalah dengan Ovelay Metric (OM) yang dinyatakan dengan d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\delta ( a _ { i } ( x ) , a _ { i } ( y ) ) d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\delta ( a _ { i } ( x ) , a _ { i } ( y ) ) dimana n n adalah banyaknya atribut, a_i(x) a_i(x) dan a_i(y) a_i(y) adalah nilai atribut ke i i yaitu A_i A_i dari masing masing objek x x dan y y , \\delta \\ ( a_{ i } ( x ) , a_{ i } ( y ) ) \\delta \\ ( a_{ i } ( x ) , a_{ i } ( y ) ) adalah 0 jika a _ { i } ( x ) = a _ { i } ( y ) a _ { i } ( x ) = a _ { i } ( y ) dan 1 jika sebaliknya. OM banyak digunakan oleh instance-based learning dan locally weighted learning. Jelas sekali , ini sedikit beruk untuk mengukur jarak antara masing-masing pasangan sample, karena gagal memanfaatkan tambahan informasi yang diberikan oleh nilai atribut nominal yang bisa membantu dalam generalisasi. Value Difference Metric (VDM) \u00b6 VDM dikenalkan oleh Standfill and Waltz, versi sederhana dari VDM tanpa skema pembobotan didefinsisikan dengan d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\sum _ { c = 1 } ^ { C } \\left| P ( c | a _ { i } ( x ) ) - P ( c | a _ { i } ( y ) ) \\right | d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\sum _ { c = 1 } ^ { C } \\left| P ( c | a _ { i } ( x ) ) - P ( c | a _ { i } ( y ) ) \\right | dimana C C adalah banyaknya kelas, P(c|a_i(x)) P(c|a_i(x)) adalah probabilitas bersyarat dimana kelas x x adalah c c dari atribut A_i A_i , yang memiliki nilai a_i(x) a_i(x) , P(c|a_i(y)) P(c|a_i(y)) adalah probabilitas bersyarat dimana kelas y y adalah c c dengan atribut A_i A_i memiliki nilai a_i(y) a_i(y) VDM mengasumsikan bahwa dua nilai dari atribut adalah lebih dekat jika memiliki klasifikasi sama. Pendekatan lain berbasi probabilitas adalah SFM (Short and Fukunaga Metric) yang kemudian dikembangkan oleh Myles dan Hand dan didefinisikan dengan d ( x , y ) = \\sum _ { c = 1 } ^ { C } \\left | P ( c | x ) - P ( c | y ) \\right| d ( x , y ) = \\sum _ { c = 1 } ^ { C } \\left | P ( c | x ) - P ( c | y ) \\right| diman probabilitas keanggotaan kelas diestimasi dengan P(c|x) P(c|x) dan P(c|y) P(c|y) didekati dengan Naive Bayes, Minimum Risk Metric (MRM) \u00b6 Ukuran ini dipresentasikan oleh Blanzieri and Ricci, berbeda dari SFM yaitu meminimumkan selisih antara kesalahan berhingga dan kesalahan asymtotic. MRM meminimumkan risk of misclassification yang didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } P ( c | x ) ( 1 - P ( c | y ) ) $$ Mengukur Jarak Tipe Ordinal \u00b6 Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. Kategori-kategori ini disusun dalam peringkat. Yaitu, rentang atribut numerik dapat dipetakan ke atribut ordinal f f yang memiliki M_f M_f state. Misalnya, kisaran suhu atribut skala-skala (dalam Celcius)dapat diatur ke dalam status berikut: \u221230 hingga \u221210, \u221210 hingga 10, 10 hingga 30, masing-masing mewakili kategori suhu dingin, suhu sedang, dan suhu hangat. M M adalah jumlah keadaan yang dapat dilakukan oleh atribut ordinalmemiliki. State ini menentukan peringkat 1, ..., M_f 1, ..., M_f Perlakuan untuk atribut ordinal adalah cukup sama dengan atribut numerik ketika menghitung disimilarity antara objek 4 . Misalkan f f adalah atribut-atribut dari atribut ordinal dari n n objek. Menghitung disimilarity terhadap f fitur sebagai berikut: Nilai f f untuk objek ke- i i adalah x_{if} x_{if} , dan f f memiliki M_f M_f status urutan , mewakili peringkat 1, .., M_f 1, .., M_f Ganti setiap x_{if} x_{if} dengan peringkatnya, r_{if} \\in \\{1...M_f\\} r_{if} \\in \\{1...M_f\\} Karena setiap atribut ordinal dapat memiliki jumlah state yang berbeda, diperlukan untuk memetakan rentang setiap atribut ke [0,0, 1.0] sehingga setiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data dengan mengganti peringkat r_{if} r_{if} dengan $$ z _ { i f } = \\frac { r _ { i f } - 1 } { M _ { f } - 1 } $$ Dissimilarity kemudian dihitung dengan menggunakan ukuran jarak seperti atribut numerik dengan data yang baru setelah ditransformasi $ z _ { i f }$ Menghitung Jarak Tipe Campuran \u00b6 Menghitung ketidaksamaan antara objek dengan atribut campuran yang berupa nominal, biner simetris, biner asimetris, numerik, atau ordinal yang ada pada kebanyakan databasae dapat dinyatakan dengan memproses semua tipe atribut secara bersamaan 5 . Salah satu teknik tersebut menggabungkan atribut yang berbeda ke dalam matriks ketidaksamaan tunggal dan menyatakannya dengan skala interval antar [0,0, 1.0] [0,0, 1.0] . Misalkan data berisi atribut p p tipe campuran. Ketidaksamaan (disimilarity ) antara objek i i dan j j dinyatakan dengan d ( i , j ) = \\frac { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } d _ { i j } ^ { ( f ) } } { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } } d ( i , j ) = \\frac { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } d _ { i j } ^ { ( f ) } } { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } } dimana \\delta_{ij}^{f}=0 \\delta_{ij}^{f}=0 - jika x_{if} x_{if} atau x_{jf} x_{jf} adalah hilang (i.e., tidak ada pengukuran dari atribut f untuk objek i i atau objek j j ) jika x_{if}=x_{jf}=0 x_{if}=x_{jf}=0 dan atribut f f adalah binary asymmetric, selain itu \\delta_{ij}^{f}=1 \\delta_{ij}^{f}=1 Kontribusi dari atribut f f untuk dissimilarity antara i dan j (yaitu. d_{ij}^{f} d_{ij}^{f} ) dihitung bergantung pada tipenya, Jika f f adalah numerik, d_{ij}^{f}=\\frac{ \\|x _{if}-x_{jf}\\|}{max_hx_{hf}-min_hx{hf}} d_{ij}^{f}=\\frac{ \\|x _{if}-x_{jf}\\|}{max_hx_{hf}-min_hx{hf}} , di mana h menjalankan semua nilai objek yang tidak hilang untuk atribut f Jika f f adalah nominal atau binary,$d_{ij}^{f}=0 $jika x_{if}=x_{jf} x_{if}=x_{jf} , sebaliknya d_{ij}^{f}=1 d_{ij}^{f}=1 Jika f f adalah ordinal maka hitung rangking r_{if} r_{if} dan \\mathcal z_{if}=\\frac {r_{if}-1}{M_f-1} \\mathcal z_{if}=\\frac {r_{if}-1}{M_f-1} , dan perlakukan z_{if} z_{if} sebagai numerik. Referensi \u00b6 Cielen, Davy, Arno Meysman, and Mohamed Ali. Introducing data science: big data, machine learning, and more, using Python tools . Manning Publications Co., 2016. \u21a9 Shirkhorshidi, Ali Seyed, Saeed Aghabozorgi, and Teh Ying Wah. \"A comparison study on similarity and dissimilarity measures in clustering continuous data.\" PloS one 10.12 (2015): e0144059. \u21a9 Li, Chaoqun, and Hongwei Li. \"A Survey of Distance Metrics for Nominal Attributes.\" JSW 5.11 (2010): 1262-1269. \u21a9 Han, Jiawei, Jian Pei, and Micheline Kamber. Data mining: concepts and techniques . Elsevier, 2011. \u21a9 Wilson, D. Randall, and Tony R. Martinez. \"Improved heterogeneous distance functions.\" Journal of artificial intelligence research 6 (1997): 1-34. \u21a9","title":"Memahami"},{"location":"memahami/#memahami-data-dan-pengambilan-data","text":"","title":"Memahami Data dan Pengambilan data"},{"location":"memahami/#macam-macam-data","text":"Dalam data data mining dan maha data, Anda akan menemukan banyak jenis data yang berbeda, dan masing-masing cenderung membutuhkan alat dan teknik yang berbeda. Macam macam data dikelompokkan sebagai berikut 1 : Data terstruktur (structured) Data tidak terstruktur(unstructured Data bahasa alami(Natural Language) Data yang dibangkit oleh Mesin (Machined-Generated) Data Audio, Video,Citra Data Streamming Data berbasis Graph(Graph-based)","title":"Macam macam Data"},{"location":"memahami/#data-terstruktur","text":"Data terstruktur adalah data yang bergantung pada model data dan yang dinyatakan dalam bentuk tabel dengan atribut} (kolom) dan baris. Data terstruktur mudah disimpan dalam database dalam bentuk tabel atau file excel (Ms Office), SQl (structure Query Language)sehingga mudah dilakukan query terhadap data tersebut. Tetapi realitanya banyak data yang ada dalam dalam bentuk data tidak terstruktur karena data dihasilkan oleh manusia dan mesin Gambar 2.1 Contoh data terstruktur..","title":"Data Terstruktur"},{"location":"memahami/#macam-macam-atribut","text":"Atribut adalah data yang mewakili karakteristik atau fitur dari objek data. Atribut bisa disebut juga dengan dimensi, fitur, dan variabel yang istilah itu sering digunakan literatur. Dimensi istilah yang biasanya digunakan dalam data warehouse. Dalam literatur pembelajaran mesin cenderung menggunakan istilah fitur, sementara dalam bidang statistik lebih memilih menggunakan istilah variabel. Dalam penambangan data atau data miniing dan database biasa menggunakan istilah atribut atau fitur , dan dalam buku ini juga menggunakan istilah atribut atau fitur. Contoh atribut-atribut yang menggambarkan objek pelanggan dapat mencakup, misalnya ID pelanggan, nama, dan alamat. Nilai yang diamati untuk atribut tertentu dikenal sebagai nilai observasi. Sekumpulan atribut yang digunakan untuk menggambarkan objek disebut disebut dengan vektor atribut (atau vektor fitur. Distribusi data yang melibatkan satu atribut (atau variabel) disebut univariat. Distribusi bivariat melibatkan dua atribut, dan seterusnya. Jenis atribut ditentukan oleh nilai-nilai pada atribut tersebut yang mungkin nominal, biner,atau ordinal, atau numerik. Pada subbagian berikut, kami perkenalkan nilai nilai tersebut Macam macam tipe data atribut Atribut Nominal Nilai atribut nominal adalah simbol ataunama barang. Setiap nilai mewakili beberapa jenis kategori, kode, atau status, dan Atribut nominal juga disebut kategori. Nilai-nilainya tidak memiliki tingkatan nilai. Dalam ilmu komputer, nilainya juga dikenal sebagai enumerasi Contoh : Misalkan warna rambut dan status perkawinan adalah dua atribut dari data orang. Nilai yang mungkin untuk warna rambut adalah hitam, coklat, pirang, merah, hitam pucat, abu-abu, dan putih. Status perkawinan memiliki nilai atribut lajang, menikah, bercerai, dan janda. Baik warna rambut maupun status perkawinan adalah atribut nominal. Contoh lain dari atribut nominal adalah atribut pekerjaan dengan nilai-nilainya adalah guru, dokter gigi, programmer, petani, dan sebagainya Atribut Biner Atribut biner adalah atribut nominal dengan hanya dua kategori atau status: 0 atau 1, di mana 0 biasanya berarti atribut itu tidak ada, dan 1 berarti itu ada. Atribut Biner disebut sebagai Boolean jika dinyatakan dengan benar (true) dan salah(false) Contoh : Terdapat atribut yang menggambarkan merokok pada pasien, 1 menunjukkan bahwa pasien merokok,sementara 0 menunjukkan bahwa pasien tidak merokok. Demikian pula, seandainya ada pasien menjalani tes medis yang memiliki dua kemungkinan hasil. Atribut Tes medis bersifat biner, dengan nilai 1 berarti hasil tes untuk pasien positif, sedangkan 0 berarti hasilnya negatif. Atribut biner simetris jika keduanya emiliki nilai bobot yang sama; Artinya, tidak ada kekhususan mengenai hasil mana yang harus dikodekan sebagai 0 atau 1. Misalkan atribut gender yang dengan nila atributnya laki dan perempuan. Atribut biner adalah asimetris jika hasil dari nilai nilainya tidak sama pentingnya seperti hasil positif dan negatif dari tes medis untuk HIV. Dengan mengkodekan hasil yang paling penting, biasanya 1 (mis., HIV positif) dan yang lainnya dengan 0 (mis., HIV negatif) Atribut ordinal Atribut ordinal adalah atribut dengan nilai yang memiliki arti urutan atau peringkat di antara nilai nilai yang ada, tapi besarnya nilai yang berurutan tersebut tidak diketahui. Ukuran kecenderungan terpusat dari atribut ordinal dapat diwakili oleh modus dan median median (nilai tengah), tetapi tidak untuk nilai rata-rata.Perlu diperhatikan bahwa atribut nominal, biner, dan ordinal bersifat kualitatif. Artinya, atribut-atribut tersebut hanya menjelaskan sebuah fitur dari suatu objek tanpa memberikan ukuran atau kuantitas yang sebenarnya. Nilai-nilai atribut kualitatif biasanya merupakan katakata yang mewakili kategori Contoh : Atribut ordinal Misalkan ukuran minuman yang tersedia di sebuah restoran cepat saji. Atribut nominal ini memiliki tiga nilai yang mungkin: kecil, sedang, dan besar. Nilai memiliki arti urutan yang (yang sesuai dengan ukuran minuman). Contoh atribut ordinal lainnya adalah pangkat dan jabatan profesi. Atribut ordinal berguna untuk melakukan penilaian subjektif terhadap kualitas sesuatu objek yang tidak dapat diukur secara obyektif; atribut ordinal sering digunakan dalam survei untuk peringkat. Dalam satu survei, para peserta diminta untuk menilai tingkat kepuasan mereka sebagai pelanggan.Kepuasan pelanggan memiliki kategori ordinal berikut ini: 0: sangat tidak puas,1: agak tidak puas, 2: netral, 3: puas, dan 4: sangat puas. Atribut ordinal juga dapat diperoleh dari iskritisasi nilai atribut numerik dengan membagi rentang nilai menjadi urutan kategoria Atribut Numerik Atribut numerik bersifat kuantitatif; Artinya, ini adalah kuantitas yang terukur, yang dinyatakan dengan bilangan bulat atau nilai riel. Atribut numerik dapat Atribut Skala Interval(interval-scaled) atau skala ration (ratio-scaled) Atribut skala interval diukur pada dengan skala unit ukuran yang sama. Nilai - nilai Interval berskala memiliki urutan dan bisa positif, 0, atau negatif. Jadi, selain untuk memberikan peringkat nilai, atribut semacam itu memungkinkan kita untuk membandingkan dan mengukur perbedaan antar nilai Contoh : Atribut suhu adalah Skala interval. Misalkan kita memiliki nilai suhu di luar ruangan untuk beberapa hari yang berbeda dari suatu objek. Dengan mengurutkan nilai, kita mendapatkan peringkat objek yang berkenaan dengan suhu. Selain itu, kita bisa mengukur perbedaan antara nilai.Misalnya, a suhu 20o C adalah lima derajat lebih tinggi dari suhu 15oC. Contoh lain kalender tahun adalah. Misalnya, tahun 2002 dan 2010 terpisah delapan tahun. Karena atribut skala interval adalah numerik, kita dapat menghitung nilai ratarata, ukuran median dan modus dari kecenderungan terpusat Atribut Skala Ratio Atribut skala rasio adalah atribut numerik dengan melekat titik nol pada nilai atribut tersebut. Artinya, jika pengukuran adalah berskala rasio, kita dapat dapat mengatakan berapa kali dari nilai yang lain atau rasio dari nilai yang lain. Selain itu, nilai yang dipesan, dan kita juga bisa menghitung selisih antara nilai, serta mean, median, dan modus Contoh Atribut tentang pengukuran berat badan, tinggi badan, jumlah kata dalam dokumen","title":"Macam- macam atribut"},{"location":"memahami/#data-tidak-terstruktur","text":"Data tidak terstruktur adalah data yang tidak mudah dimasukkan ke dalam model data karena isi/kontennya spesifik atau bervariasi. Salah satu contoh data tidak terstruktur adalah data email. Meskipun email berisi elemen terstruktur seperti pengirim, judul, dan isi teks, terlalu banyak variasi dari isi yang terkandung dalamnya diantaranya dialek bahasa yang dipakai dan sebagainya. Email juga salah satu contoh data bahasa alami Gambar 2.2 Contoh Data email","title":"Data Tidak Terstruktur"},{"location":"memahami/#bahasa-alami","text":"Dalam neuropsikologi , linguistik , dan filsafat bahasa , bahasa alami atau bahasa biasa adalah bahasa yang telah berevolusi secara alami pada manusia melalui penggunaan dan pengulangan tanpa perencanaan. Bahasa alami berbeda dengan bahasa yang dibangun untuk memprogramna komputer atau membangun logika nalar. Bahasa alami dikenal sebagai bahasa manusia misal bahasa indonesia, bahasa inggris dan lain lain. Didalam pemrosesan bahasa alami diperluangan pengetahuan ilmu linguistics, semantics, statistics and machine learning.Dengan pemrosesan bahasa alami membantu komputer untuk memahami bahasa yang telah diucapkan oleh manusia","title":"Bahasa Alami"},{"location":"memahami/#data-yang-dibangkitkan-oleh-mesin","text":"Data yang dibangkitkan oleh mesin secara otomatis tanpa intervensi manusia. Data ini terus menerus dibangkitkan selama proses tertentu sedang berjalan. Misalkan data weblog dari mesin server yang dihasilkan dari hasil transaksi user dengan sistem web. Contoh lain adalah data yang dihasilkan dari implementasi internet of things misal perekaman suhu udara dan kelembaban udara dari daerah tertentu yang terhubung dengan pusat penyimpanan data tersebut.","title":"Data yang dibangkitkan oleh Mesin"},{"location":"memahami/#data-jaringan-atau-data-berbasis-graph","text":"Data graph adalah data yang dinyatakan dengan graph yang dalam matematika mengacu pada konsep teori graph. Data ini menunjukkan keterhubungan antara objek objek atau relasi antar objek objek dengan menggunakan struktur node, edge, dan karakteristik/sifat keterhubungan antar objek tersebut. Salah satu data graph adalah data keterhubungan orang dalam media sosial. Dengan memanfaatkan data graph media sosial kita dapat mengukur ukuran ukuran tertentu berdasarkan struktur yang dibentuknya. Misalkan menentukan pengaruh orang dalam struktur jaringan tersebut, apakah termasuk orang penting/berpengaruh atau bukan. Gambar berikut menunjukkan contoh data graph Gambar 2.3 .Pertemanan dalam media sosial yang dinyataka dengan data graph Database graph dapat digunakan untuk menyimpan data berbasis graph dan mengunakan query tertentu yaitu SPARQL","title":"Data jaringan atau data berbasis Graph"},{"location":"memahami/#data-audiovidio-dan-citra","text":"Dengan perkembangan teknologi implementasi multimedia yang sangat pesat saat,data audio,video dan citra cukup besar dihasilkan dari transaksi bisnis. Dengan besarnya data yang dihasilkan membutuhkan proses pengolahan spesifik dari data tersebut untuk dimanfaatkan terutama dalam analisa data sain. Diantara pemanfaatan data mulitimedia tersebut adalah pengenalan objek, pengenala suara, segmentasi citra satelit dan banyak analisa lain yang dihasilkan dari data multimeda tersebut.","title":"Data Audio,Vidio dan Citra"},{"location":"memahami/#data-streamming","text":"Data Streaming adalah data yang dihasilkan secara terus-menerus oleh ribuan sumber data, yang biasanya mengirimkan catatan data secara bersamaan, dan dalam ukuran kecil (urutan Kilobyte). Data streaming mencakup berbagai macam data seperti logfile yang dihasilkan oleh pelanggan aplikasi seluler atau website Anda, transaksi e-commerce, informasi dari jejaring sosial, data geospasial, dan perangkat sensor yang terhubung atau instrumentasi di pusat data. Data ini perlu diproses secara berurutan dan bertahap secara record-by-record digunakan untuk berbagai macam analisis misalkan korelasi, agregasi, penyaringan, dan pengambilan sampel. Informasi yang diperoleh dari analisis tersebut memberikan petunjuk terhadap pelanggan mereka seperti penggunaan layanan mereka, aktivitas server, klik website, dan lain lain. Misalnya, dalam bisnis kita dapat melacak perubahan sentimen publik pada merek dan produk mereka dengan menganalisis aliran data media sosial, sehingga dapat merespons secara tepat baik waktu dan tindakan yang harus dilakukan","title":"Data streamming"},{"location":"memahami/#distribusi-data","text":"Karakteristik utama dari data adalah distribusi probabilitasnya. Distribusi data yang paling dikenal adalah distribusi normal atau Gaussian. Distribusi ini ditemukan pada sistem fisik dimana data dibangkitkan secara acak. Fungsi dinyatakan dalam bentuk fungsi padat probabilitas(probability density function) $$ f ( x ) = \\frac { 1 } { ( \\sigma \\sqrt { 2 } \\pi ) } \\frac { e ^ { - ( x - \\mu ) ^ { 2 } } } { ( 2 \\sigma ^ { 2 } ) } $$ Dimana \\sigma \\sigma adalah standar deviasi dan \\mu \\mu adalah mean. Persamaan ini menyatakan peluang variable acak dari suatu data x x . Kita menyatakan standar deviasi sebagai lebar kurva lonceng dan rata rata sebagai pusat. Kadangkala istilah variance digunakan dan ini adalah kuadrat dari standar deviasi. Standar deviasi pada dasarnya mengukur bagaimana sebaran data. Untuk memahami lebih jelasnya bagaimana fungsi tersebut digambarkan, berikut implementasinya data dengan distribusi normal yang memiliki rata-rata 1 dan variansinya 0.5 Gambar 2.4. Distribusi Data mu = 1 # rata-rata sigma = np . sqrt ( 0.5 ) # standar deviasi (akar dari varians) s = np . random . normal ( mu , sigma , 1000 ) # membangkitkan 1000 bilangan acak dgn distribusi norma import matplotlib.pyplot as plt plt . plot ( bins , 1 / ( sigma * np . sqrt ( 2 * np . pi )) * np . exp ( - ( bins - mu ) ** 2 / ( 2 * sigma ** 2 ) ), linewidth = 2 , color = 'blue' ) plt . show ()","title":"Distribusi Data"},{"location":"memahami/#ekplorasi-data-tipe-numerik","text":"Pada bagian ini kita membahas metode statistik dasar untuk analisis ekploarasi data atribut numerik. Kita membahas ukuran kecenderungan pusat (central tendency), ukuran dispersi atau sebaran, dan ukuran ketergantungan linier atau hubungan antara atribut. Kita menekankan hubungan antara probabilistik dan geometris dan aljabar dari sudut pandang data matriks","title":"Ekplorasi data tipe Numerik"},{"location":"memahami/#analisa-univariat","text":"Analisis univariat dilakukan pada atribut tunggal ( X X ); dengan demikian matriks data D bisa dianggap sebagai matriks n \u00d7 1 n \u00d7 1 , atau sebagai vektor kolom, yang dianyatakan dengan kkk X=\\begin {pmatrix} \\begin{array} { c } { X } \\\\ \\hline x _ { 1 } \\\\ { x _ { 2 } } \\\\ { \\vdots } \\\\ { x _ { n } } \\end{array} \\end {pmatrix} X=\\begin {pmatrix} \\begin{array} { c } { X } \\\\ \\hline x _ { 1 } \\\\ { x _ { 2 } } \\\\ { \\vdots } \\\\ { x _ { n } } \\end{array} \\end {pmatrix} dimana X X adalah atribut numerik yang dimaksudkan, dengan x_i \\in \\mathbb R x_i \\in \\mathbb R . X X diasumsikan adalah variabel random, dengan setiap titik x_i(1\\leq i \\leq n) x_i(1\\leq i \\leq n) , merupakan variabel acak. Kita asumsikan bawa data pengamatan adalah. Kami berasumsi bahwa data yang diamati adalah sampel acak yang diambil dari X X , artinya, setiap variabel x_i x_i adalah saling bebas dan berdistribus sama (iid). Dalam sudut pandang vektor, kami memperlakukan sampel sebagai vektor n-dimensi, dan menulis X \\in \\mathbb R^n X \\in \\mathbb R^n Secara umum, fungsi padat probabilitas atau fungsi mass f(x) f(x) dan fungsi distribusi kumulatif F(x) F(x) untuk atribut X X keduanya tidak diketahui. Akan tetapi, kita dapat mengestimasi distribusi ini langsung dar data sample, juga juga memungkinkan kita untuk menghitung beberapa parameter penting populasi. Secara umum, fungsi padat probabilitas atau fungsi mass f(x) f(x) dan fungsi distribusi kumulatif F(x) F(x) untuk F ^ { - 1 } ( q ) = \\operatorname { min } { x | \\hat { F } ( x ) \\geq q } \\quad \\text { for } q \\in [ 0,1 ] F ^ { - 1 } ( q ) = \\operatorname { min } { x | \\hat { F } ( x ) \\geq q } \\quad \\text { for } q \\in [ 0,1 ] atribut X X keduanya tidak diketahui. Akan tetapi, kita dapat mengestimasi distribusi ini langsung dar data sample, juga juga memungkinkan kita untuk menghitung beberapa parameter penting populasi.","title":"Analisa univariat"},{"location":"memahami/#fungsi-distribusi-kumulatif-empiris","text":"Fungsi distribusi kumulatif empiris (CDF ) dari X X dinyatakan dengan $$ \\hat { F } ( x ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i } \\leq x ) $$ dimana I(x_i\\le x)=\\Biggl\\{\\begin{array}={} 1 & {\\text {if }x_i\\le x }\\\\ 0 & {\\text {if }x_i > x}\\end{array} I(x_i\\le x)=\\Biggl\\{\\begin{array}={} 1 & {\\text {if }x_i\\le x }\\\\ 0 & {\\text {if }x_i > x}\\end{array} adalah variabel indikator biner yang menyatakan variabel indikator biner yang menunjukkan apakah kondisi yang diberikan terpenuhi atau tidak.","title":"Fungsi distribusi  Kumulatif Empiris"},{"location":"memahami/#fungsi-distribusi-kumulatif-invers","text":"Definisi fungsi distribusi kumulatif invers atau fungsi quantile untuk variabel acak X X sebagai berikut : $$ F ^ { - 1 } ( q ) = \\operatorname { min } { x | \\hat { F } ( x ) \\geq q } \\quad \\text { for } q \\in [ 0,1 ] $$ Fungsi distribusi kumulatif Invers empiris dapat diperoleh dari persamaan (2)","title":"Fungsi distribusi kumulatif Invers"},{"location":"memahami/#fungsi-massa-probabilitas-empiris","text":"Fungsi massa probabilitas empiris dari X X dinyatakan dengan $$ \\hat { f } ( x ) = P ( X = x ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i } = x ) $$ dimana I(x_i\\le x)=\\Biggl\\{\\begin{array}={} 1 & {\\text {if }x_i= x }\\\\ 0 & {\\text {if }x_i \\neq x}\\end{array} I(x_i\\le x)=\\Biggl\\{\\begin{array}={} 1 & {\\text {if }x_i= x }\\\\ 0 & {\\text {if }x_i \\neq x}\\end{array} Fungsi massa probabilitas empiris juga menempatkan massa probabitas \\frac {1}{n} \\frac {1}{n} pada setipa titik x_i x_i","title":"Fungsi massa Probabilitas Empiris"},{"location":"memahami/#mengukur-kecenduran-terpusat","text":"Ukuran ini memberikan indikasi tentang konsentrasi massa probabilitas , nilai tengah dan lainnya.","title":"Mengukur kecenduran terpusat"},{"location":"memahami/#mean","text":"Mean juga disebut dengan nilai harapan dari variabel acak X X adalah rata rata aritmetika dari nilai X X . Itu merupakan salah satu dari kecenderungan terpusat dari X X . Mean atau nilai harapan dari variabel acak X X didefinisikan dengan $$ \\mu = E [ X ] = \\sum _ { x } x f ( x ) $$ diman f(x) f(x) adalah fungsi massa probabilitas dari X X . Nilai harapan dari variabel acak kontinu X X dinyakan dengan $$ \\mu = E [ X ] = \\int _ { - \\infty } ^ { \\infty } x f ( x ) d x $$ dimana f(x) f(x) adalah fungsi padat probabilitas dari X X . Sample Mean . Sample mean adalah statistik, yaitu fungsi $ \\hat { \\mu } : { x _ { 1 } , x _ { 2 } , \\ldots , x _ { n } } \\rightarrow \\mathbb R$, didefinisikan sebagai nilai rata-rata dari x_i x_i : $$ \\hat { \\mu } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } $$ nilai adalah sebagai pengestimasi nilai mean yang tidak diketahui dari X X . Nilai tersebut diperoleh dengan memasukkan dalam fungsi massa probabilitas empiris dalam persamaan (7) $$ \\hat { \\mu } = \\sum _ { x } x \\hat { f } ( x ) = \\sum _ { x } x ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i } = x ) ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } $$ Sample mean adalah tidak bias . Estimator \\hat { \\theta } \\hat { \\theta } disebut dengan unbiased estimatore (stimator tidak bias) untuk parameter \\theta \\theta jika E[\\hat \\theta] = \\theta E[\\hat \\theta] = \\theta untuk setiap kemungkinan nilai dari \\theta \\theta . Sample mean \\hat \\mu \\hat \\mu adalah unbiased estimator untuk mean populasi \\mu \\mu sehingga $$ E [ \\hat { \\mu } ] = E [ \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } ] = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } E [ x _ { i } ] = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } \\mu = \\mu $$ dimana kita gunakan fakta bahwa variabel acak x_i x_i adalah IID sesuai dengan X X , yang berarti bahwa mereka memiliki rata-rata \\mu \\mu yang sama seperti X X , yaitu,$ E [x_i] =\\mu$ untuk semua x_i x_i . Kita juga menggunakan fakta bahwa fungsi ekpektasi E E adalah linier operator yaitu untuk suatu dua bilangan acak X X dan Y Y dan bilangan real a a dan b b , kita memiliki E [ a X + b Y ] = a E [ X ] + b E [ Y ] E [ a X + b Y ] = a E [ X ] + b E [ Y ] Robustnes Kita mengatakan bahwa statistik adalah robust jika tidak dipengaruhi oleh suatu nilai ekstrim ( misal outlier/pencilan) dalam data. Rata-rata sampel sayangnya tidak kuat karena ada satu nilai besar (outlier) dapat mejadikan rata-rata yang tidak sebenarnya. Ukuran yang lebih robust adalah trimmed mean yang didapatkan setalah mengabaikan sebagian kecil dari nilai nilai ekstrim pada salah satu ujungnya.","title":"Mean"},{"location":"memahami/#median","text":"Median dari suatu variabel acak didefinisikan dengan nilai m m sehingga $$ P ( X \\leq m ) \\geq \\frac { 1 } { 2 } \\text { and } P ( X \\geq m ) \\geq \\frac { 1 } { 2 } $$ Degan kata lain, median m m adalah nilai paling tengan (middle-most). Dalam istliah (invers) cumulatif distribution function , median m m dinyatakan dengan $$ F ( m ) = 0.5 \\text { or } m = F ^ { - 1 } ( 0.5 ) $$ Sample median dapat diperoleh dari Fungsi distribusi kumulatif invers atau fungsi distribusi kumulatif invers empiris dengan dihitung $$ \\hat { F } ( m ) = 0.5 \\text { atau } m = \\hat { F } ^ { - 1 } ( 0.5 ) $$ Pendekatan paling sederhan untuk menghitung sample median adalah pertama kai dari mengurutkan semua nilai x_i x_i (i \\in [1,n]) (i \\in [1,n]) dengan urutan naik. Jika n n adalah ganjil , media adalah nilai pada posisi \\frac {n+1}{2} \\frac {n+1}{2} . Jika n n adalah genap, nilai padan posisi \\frac {n}{2} \\frac {n}{2} dan \\frac {n}{2}+1 \\frac {n}{2}+1 adalah keduanaya median. Tidak seperti mean, media adalah robust, sehingga ia tidak dipengaruhi oleh banyak nilai extrim. Juga nilai tersebut terjadi dalam sample dan nilai yang bisa diasumsikan oleh variabel acak.","title":"Median"},{"location":"memahami/#mode","text":"Nilai mode dari variabel acak adalah nilai dimana fungsi massa probabilitas atau fungsi padat probabilitas mencapai nilai maximumnya, bergantung pada apakah X X adalah diskrit atau kontinu. Sample mode adalah nila untuk fungsi probabilitas empiris mencapai nilai maksimum, dinyatakan dengan $$ mode(X) =\\arg \\underset{x}{max} {\\hat f(x)} $$ Mode ini mungkin bukan ukuran kecenderungan sentral yang sangat berguna untuk sampel, karena kemungkinan elemen yang tidak representatif menjadi elemen yang paling sering muncul. Selanjutnya, jika semua nilai dalam sampel berbeda, maka masing-masing akan menjadi mode Contoh . (Sample Mean, Median, dan Mode) . Perhatikan atribut sepal length (Xi) (Xi) dalam data iris. Data iris, dimana nilainya seperti yang ditunjukkan dalam tebel 1.2 . Sample mean dinyatakan dengan $$ \\hat { \\mu } = \\frac { 1 } { 150 } ( 5.9 + 6.9 + \\cdots + 7.7 + 5.1 ) = \\frac { 876.5 } { 150 } = 5.843 $$ Gambar 2.1 menunjukkan semua dari 150 nilai sepal length dan sample mean. Gambar 2.2a menunjukkan fungsi distribusi kumulatif empiri dan gambar 2.2b menunjukkan fungsi distribusi kumulatif empiris untuk sepal length Karena n=150 n=150 adalah genap, sample median adalah nilai pada posisi \\frac {n}{2}=75 \\frac {n}{2}=75 dan \\frac {n}{2}+1=76 \\frac {n}{2}+1=76 setelah diurutkan. Untuk sepal length kedua nilainya adalah 5.8, kemudian sample media adalah 5.8 . Dari fungsi distribusi kumulatif invers dalam gambar 2.2b, kita dapat melihat bahwa $$ \\hat { F } ( 5.8 ) = 0.5 \\text { or } 5.8 = \\hat { F } ^ { - 1 } ( 0.5 ) $$ Sample mode untuk sepal length adalah 5. yang dapat dilihat dari frequency dari 5 dalam gambar 2.1. Massa probabilitas empiris pada x=5 x=5 adalah $$ \\hat { f } ( 5 ) = \\frac { 10 } { 150 } = 0.067 $$","title":"Mode"},{"location":"memahami/#mengukur-sebaran-data","text":"Kita sekarang membahas ukuran ukuran untuk menilai dispersi atau penyebaran data numerik. Ukuran-ukuran itu adalah rentang (range), kuantil, kuartil, persentil, dan rentang interkuartil. Semua itu adalah ringkasan lima angka, yang dapat ditunjukkan dengan boxplot, berguna dalam mengidentifikasi pencilan (outlier). Varians dan standar deviasi juga menunjukkan sebaran distribusi data.","title":"Mengukur Sebaran Data"},{"location":"memahami/#rentang-range-quartil-and-rentang-interquartile","text":"Misalkan x_1, x_2, .. x_N x_1, x_2, .. x_N adalah sekumpulan pengamatan untuk atribut numerik, X X . Rentang adalah selisih antara nilai terbesar (maks ()) dan terkecil (min ()). Misalkan data untuk atribut X diurutkan dalam urutan naik.Bagilah data berdasarkan titik titik tertentu sehingga membagi distribusi data ukuran yang sama, seperti pada Gambar dibawah. Titik data ini disebut kuantil. 2-quantile adalah titik data yang membagi bagian bawah dan atas dari distribusi data. Ini sama dengan median. 4-kuantil adalah tiga titik data yang membagi distribusi data menjadi empat bagian yang sama; setiap bagian mewakili seperempat dari distribusi data. Ini lebih sering disebut sebagai kuartil. 100-kuantil lebih sering disebut sebagai persentil; mereka membagi distribusi data menjadi 100 data berukuran sama. Median, kuartil, dan persentil adalah bentuk kuantil yang paling banyak digunakan. Gambar 2.6. Percentile data Kuartil memberikan gambaran pusat distribus, penyebaran, dan bentuk distribusi. Kuartil satu, dilambangkan oleh Q1, adalah persentil ke-25. Nilai ini menunjukan 25% terendah dari data. Kuartil ketiga, dilambangkan oleh Q3, adalah persentil ke-75 - itu memisahkan data 75% dari terendah data (atau 25% dari tertinggi data. Kuartil kedua adalah persentil ke-50 atau median dari distribusi data. Jarak antara kuartil pertama dan ketiga adalah ukuran yang menyatakan rentang yang dicakup oleh bagian tengah data. Jarak ini disebut rentang interkuartil (IQR) dan dinyatakan dengan I Q R = Q _ { 3 } - Q _ { 1 } I Q R = Q _ { 3 } - Q _ { 1 } Dengan ukuran a kuartil Q1 dan Q3, dan median kita dapat mengidentifikasikan ada tidaknya pencilan (outlier) pada suatu data. Data pencilan atau outlier nilai data biasanya ada di setidaknya 1,5 \u00d7 IQR di atas kuartil ketiga atau di bawah kuartil pertama Karena Q1, median, dan Q3 tidak berisi informasi tentang titik akhir (mis., Ekor) data, ringkasan yang lebih lengkap dari bentuk distribusi dapat diperoleh dengan memberikan nilai data terendah dan tertinggi juga. Ini dikenal sebagai ringkasan lima angka. Ringkasan lima nomor distribusi terdiri dari median (Q2), kuartil Q1 dan Q3, dan data terkecil dan terbesar( Minimum, Q1, Median, Q3, Maksimum) Boxplots adalah cara populer untuk memvisualisasikan distribusi. Boxplot menggabungkan ringkasan lima angka sebagai berikut: - Ujung kotak adalah kuartil dan panjang kotak adalah rentang interkuartil. - Median ditandai dengan garis di dalam kotak. - Dua garis (disebut whiskers) di luar kotak memanjang ke pengamatan terkecil (Minimum) dan terbesar (Maksimum) Outlier biasanya ada di dibawah Q_1 \u2013 1.5 \\times IQR Q_1 \u2013 1.5 \\times IQR dan diatas $ Q_3 + 1.5 \\times IQR$ Gambar 2.7. Boxplot","title":"Rentang (Range), Quartil, and Rentang Interquartile"},{"location":"memahami/#variansi-dan-standar-deviasi","text":"Variansi dan standar deviasi adalah ukuran penyebaran data. Nilai-nilai tersebut menunjukkan bagaimana penyebaran distribusi data. Standar Deviasi yang rendah berarti bahwa pengamatan data cenderung sangat dekat dengan rata-rata, sedangkan deviasi standar yang tinggi menunjukkan data tersebar di sejumlah nilai-nilai besar. Varian dari pengamatan N, x_1, x_2, ..., x_N N, x_1, x_2, ..., x_N , untuk atribut numerik X adalah \\sigma ^ { 2 } = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } ( x _ { i } - \\overline { x } ) ^ { 2 } = ( \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } x _ { i } ^ { 2 } ) - \\overline { x } ^ { 2 } \\sigma ^ { 2 } = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } ( x _ { i } - \\overline { x } ) ^ { 2 } = ( \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } x _ { i } ^ { 2 } ) - \\overline { x } ^ { 2 } di mana $ \\overline { x } $ adalah nilai rata-rata dari pengamatan, Standar deviasi,$\\sigma $, dari pengamatan adalah akar kuadrat dari variansi, \\sigma^2 \\sigma^2 Sifat dasar dari standar deviasi, \\sigma \\sigma , sebagai ukuran penyebaran data adalah sebagai berikut: Ukuran \\sigma \\sigma mengeukur sebaran disekitar rata-rata dan harus dipertimbangkan bila rata-rata dipilih sebagai ukuran pusat data \\sigma = 0 \\sigma = 0 hanya jika tidak ada penyebaran data, hanya terjadi ketika semua pengamatan memiliki nilai sama, Jika tidak maka \\sigma > 0 \\sigma > 0","title":"Variansi dan Standar Deviasi"},{"location":"memahami/#skewness","text":"Derajat distorsi dari kurva lonceng simetris atau distribusi normal. Ini mengukur kurangnya simetri dalam distribusi data Untuk menghitung derajat distorisi dapat menggunakan Koefisien Kemencengan Pearson yang diperoleh dengan menggunakan nilai selisih rata-rata dengan modus dibagi simpangan baku. Koefisien Kemencengan Pearson dirumuskan sebagai berikut s k=\\frac{\\overline{X}-M o}{s} s k=\\frac{\\overline{X}-M o}{s} dengan $$ \\overline{X}-M o \\approx 3(\\overline{X}-M e) $$ maka s k \\approx \\frac{3(\\overline{X}-M e)}{s} s k \\approx \\frac{3(\\overline{X}-M e)}{s} Gambar 2.8. Macam macam Kemiringan data (Skewness)","title":"Skewness"},{"location":"memahami/#implementasi","text":"Untuk implementasi silahkan unduh data .csv import pandas as pd from scipy import stats df = pd . read_csv ( \"data.csv\" , usecols = [ 0 ]) print ( \"jumlah data \" , df [ 'NilaiPreTest' ] . count ()) print ( \"rata-rata \" , df [ 'NilaiPreTest' ] . mean ()) print ( \"nila minimal \" , df [ 'NilaiPreTest' ] . min ()) print ( \"Q1 \" , df [ 'NilaiPreTest' ] . quantile ( 0.25 )) print ( \"Q2 \" , df [ 'NilaiPreTest' ] . quantile ( 0.5 )) print ( \"Q3 \" , df [ 'NilaiPreTest' ] . quantile ( 0.75 )) print ( \"Nilai Max \" , df [ 'NilaiPreTest' ] . max ()) print ( \"kemencengan\" , \" {0:.2f} \" . format ( round ( df [ 'NilaiPreTest' ] . skew (), 2 ))) mode = stats . mode ( df ) print ( \"Nilai modus {} dengan jumlah {} \" . format ( mode . mode [ 0 ], mode . count [ 0 ])) print ( \"kemencengan \" , \" {0:.6f} \" . format ( round ( df [ 'NilaiPreTest' ] . skew (), 6 ))) print ( \"Standar Deviasi \" , \" {0:.2f} \" . format ( round ( df [ 'NilaiPreTest' ] . std (), 2 ))) print ( \"Variansi \" , \" {0:.2f} \" . format ( round ( df [ 'NilaiPreTest' ] . var (), 2 )))","title":"Implementasi"},{"location":"memahami/#analisa-bivariate","text":"Dalam analisa bivariate, kita memandang dua atribut pada waktu yang sama. Kita fokus untuk memahami keterkaitan atau kebergantunga antara dua variabel atau atribut tersebut, jika ada. Kita lalu membatasi pada dua variabel X_1 X_1 dan X_2 X_2 , dengan D D dinyatakan sebagai matrik dengan ukuran n\\times2 n\\times2 X=\\begin {pmatrix} \\begin{array}{ c c } { X _ { 1 } } & { X _ { 2 } } \\\\ \\hline x _ { 11 } & { x _ { 12 } } \\\\ { x _ { 21 } } & { x _ { 22 } } \\\\ { \\vdots } & { \\vdots } \\\\ { x _ { n 1 } } & { x _ { n 2 } } \\end{array} \\end {pmatrix} X=\\begin {pmatrix} \\begin{array}{ c c } { X _ { 1 } } & { X _ { 2 } } \\\\ \\hline x _ { 11 } & { x _ { 12 } } \\\\ { x _ { 21 } } & { x _ { 22 } } \\\\ { \\vdots } & { \\vdots } \\\\ { x _ { n 1 } } & { x _ { n 2 } } \\end{array} \\end {pmatrix} Secara geometri, kita dapat memandang D D dalam dua cara. Itu dapat dianggap sebagai n n titik atau vektor dalam 2-ruang dimensi terhadap atribut X_1 X_1 dan X_2 X_2 yaitu x_i =(x_{i1},x_{i2})^T \\in \\mathbb R^2 x_i =(x_{i1},x_{i2})^T \\in \\mathbb R^2 .Selain itu dapat dilihat sebagai 2 titik atau vektor dalam n n -ruang dimensi yang berisi titik, yaitu setiap kolom adalah vektor dalam \\mathbb R^{n} \\mathbb R^{n} sebagai berikut : $$ \\left. \\begin{array} { l } { X _ { 1 } = ( x _ { 11 } , x _ { 21 } , \\ldots , x _ { n 1 } ) ^ { T } } \\ { X _ { 2 } = ( x _ { 12 } , x _ { 22 } , \\ldots , x _ { n 2 } ) ^ { T } } \\end{array} \\right. $$ Dalam sudut pandang probabilistik, vektor kolom X=(X_1,X_2)^T X=(X_1,X_2)^T dianggapa variabel acak bivariate dan titik titik x _ { i } ( 1 \\leq i \\leq n ) x _ { i } ( 1 \\leq i \\leq n ) dinyatakan sebagai sampel acak yang diperoleh dari X X , yaitu x_i x_i dianggap independent and identically distributed (iid) seperti X X .","title":"Analisa Bivariate"},{"location":"memahami/#fungsi-massa-probabilitas-gabungan-empiris","text":"Fungsi Massa Probabilitas Gabungan Empiris untuk X X dinyatakan dengan $$ \\hat { f } ( x ) = P ( X = x ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i } = x ) $$ \\hat { f } ( x _ { 1 } , x _ { 2 } ) = P ( X _ { 1 } = x _ { 1 } , X _ { 2 } = x _ { 2 } ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i 1 } = x _ { 1 } , x _ { i 2 } = x _ { 2 } ) \\hat { f } ( x _ { 1 } , x _ { 2 } ) = P ( X _ { 1 } = x _ { 1 } , X _ { 2 } = x _ { 2 } ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i 1 } = x _ { 1 } , x _ { i 2 } = x _ { 2 } ) dimana I I adalah variabel indikator yang bernilai 1 jika argumen argumennya benar $$ I ( x _ { i } = x ) = \\left{ \\begin{array} { l l } { 1 } & { \\text { jika } x _ { i 1 } = x _ { 1 } \\text { dan } x _ { i 2 } = x _ { 2 } } \\ { 0 } & { \\text { untuk yang lainnya } } \\end{array} \\right. $$ Seperti dalam kasus univariate, fungsi probabilitas menempatkan massa probabilitas \\frac {1}{n} \\frac {1}{n} pada setiap objek dalam data sampel.","title":"Fungsi Massa Probabilitas Gabungan Empiris"},{"location":"memahami/#mengukur-dispersi","text":"","title":"Mengukur Dispersi"},{"location":"memahami/#mean_1","text":"Rata rata bivariate didefinisikan sebagai nilai harapan dari variabel acak vektor X X , didefinisikan sebagai berikut : $$ \\mu = E [ X ] = E \\left[ \\left( \\begin{array} { l } { X _ { 1 } } \\ { X _ { 2 } } \\end{array} \\right) \\right] = \\left( \\begin{array} { l } { E [ X _ { 1 } ] } \\ { E [ X _ { 2 } ] } \\end{array} \\right) = \\left( \\begin{array} { l } { \\mu _ { 1 } } \\ { \\mu _ { 2 } } \\end{array} \\right) $$ Dengan kata lain, rata-rata bivariate adalah nilai harapan dari masing masing atribut. Rata-rata sampel dapat diperoleh dari \\hat f_{x_1} \\hat f_{x_1} dan \\hat f_{x_2} \\hat f_{x_2} , fungsi massa probabilitas empiris dari X_1 X_1 dan X_2 X_2 , menggunakan persamaan (2.5). Dapat juga dihitung dari gabungan fungsi massa probabilitas empiris dalam persamaan (2.17) $$ \\hat { \\mu } = \\sum _ { x } x \\hat { f } ( x ) = \\sum _ { x } x \\left( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( x _ { i } = x )\\right ) = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } $$","title":"Mean"},{"location":"memahami/#variansi","text":"Kita dapat menghitung variansi masing masing atribut, yaitu \\sigma_1^2 \\sigma_1^2 untuk X_1 X_1 dan \\sigma_2^2 \\sigma_2^2 untuk X_2 X_2 mengggunkan persamaan (2.8). Variansi secara keseluruhan (1.4) dinyatakan dengan $$ var(D)=\\sigma_1^2 +\\sigma_2^2 $$ Variansi sampel \\hat \\sigma_1^2 + \\hat \\sigma_2^2 \\hat \\sigma_1^2 + \\hat \\sigma_2^2 dapat diestimasi dengan menggunakanpersamaan (2.10) dan jumlah variansi sample adalah \\sigma_1^2 +\\sigma_2^2 \\sigma_1^2 +\\sigma_2^2","title":"Variansi"},{"location":"memahami/#mengukur-keterkaitan","text":"","title":"Mengukur keterkaitan"},{"location":"memahami/#covarian","text":"Kovarian antara dua atribut X_1 X_1 dan X_2 X_2 mengukur keterkaitan antara kebergantungan linier diantaranya dan didefinisikan dengan $$ \\sigma _ { 12 } = E [ ( X _ { 1 } - \\mu _ { 1 } ) ( X _ { 2 } - \\mu _ { 2 } ) ] $$ Dengan linieraritas dari harapan, kita miliki $$ \\left. \\begin{array}{l}{ \\sigma _ { 12 } = E [ ( X _ { 1 } - \\mu _ { 1 } ) ( X _ { 2 } - \\mu _ { 2 } ) ] }\\{ = E [ X _ { 1 } X _ { 2 } - X _ { 1 } \\mu _ { 2 } - X _ { 2 } \\mu _ { 1 } + \\mu _ { 1 } \\mu _ { 2 } ] }\\{ = E [ X _ { 1 } X _ { 2 } ] - \\mu _ { 2 } E [ X _ { 1 } ] - \\mu _ { 1 } E [ X _ { 2 } ] + \\mu _ { 1 } \\mu _ { 2 } }\\{ = E [ X _ { 1 } X _ { 2 } ] - \\mu _ { 1 } \\mu _ { 2 } }\\{ = E [ X _ { 1 } X _ { 2 } ] - E [ X _ { 1 } ] E [ X _ { 2 } ] }\\end{array} \\right. $$ Persamaan (2.21) dapat dianggap sebagai generalisasi dari variansi univariate persamaan (2.9) pada kasus bivariate. Jika X_1 X_1 dan X_2 X_2 adalah variabel acak saling bebas, maka kita dapat simpulkan bahwa covariannya adalah nol. Ini karena jika X_1 X_1 dan X_2 X_2 adalah saling bebas, maka kita memiliki $$ E [ X _ { 1 } X _ { 2 } ] = E [ X _ { 1 } ] \\cdot E [ X _ { 2 } ] $$ yang pada akhirnya menyiratkan bahwa $$ \\sigma{12}= 0 $$ Namaun sebaliknya tidak benar. Yaitu jika \\sigma_{12}=0 \\sigma_{12}=0 , kita tidak dapat mengklaim bahwa $X_1 $ dan X_2 X_2 adalah saling bebas. Semuanya kita katakan bahwa tidak adalah kebergantung linier antara keduanya. Kovarian sampel antra X1 X1 dan X_2 X_2 dinyatakan dengan $$ \\hat { \\sigma } _ { 12 } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i 1 } - \\hat { \\mu } _ { 1 } ) ( x _ { i 2 } - \\hat { \\mu } _ { 2 } ) $$","title":"Covarian"},{"location":"memahami/#korelasi","text":"Korelasi antara variabel X_1 X_1 dan X_2 X_2 adalah standarisasi kovarian, yang didapatkan dengan menormalisasi kovarian dengan standar deviasi masing masing variabel dinyatakan dengan \\rho _ { 12 } = \\frac { \\sigma _ { 12 } } { \\sigma _ { 1 } \\sigma _ { 2 } } = \\frac { \\sigma _ { 12 } } { \\sqrt { \\sigma _ { 1 } ^ { 2 } \\sigma _ { 2 } ^ { 2 } } } $$ Korelasi sample untuk atribut $X_1$ dan $X_2$ dinyatakan dengan $$ \\hat { \\rho } _ { 12 } = \\frac { \\hat { \\sigma } _ { 12 } } { \\hat { \\sigma } _ { 1 } \\hat { \\sigma } _ { 2 } } = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i 1 } - \\hat { \\mu } _ { 1 } ) ( x _ { i 2 } - \\hat { \\mu } _ { 2 } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i 1 } - \\hat { \\mu } _ { 1 } ) ^ { 2 } \\sum _ { i = 1 } ^ { m } ( x _ { i 2 } - \\hat { \\mu } _ { 2 } ) ^ { 2 } } } \\rho _ { 12 } = \\frac { \\sigma _ { 12 } } { \\sigma _ { 1 } \\sigma _ { 2 } } = \\frac { \\sigma _ { 12 } } { \\sqrt { \\sigma _ { 1 } ^ { 2 } \\sigma _ { 2 } ^ { 2 } } } $$ Korelasi sample untuk atribut $X_1$ dan $X_2$ dinyatakan dengan $$ \\hat { \\rho } _ { 12 } = \\frac { \\hat { \\sigma } _ { 12 } } { \\hat { \\sigma } _ { 1 } \\hat { \\sigma } _ { 2 } } = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i 1 } - \\hat { \\mu } _ { 1 } ) ( x _ { i 2 } - \\hat { \\mu } _ { 2 } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i 1 } - \\hat { \\mu } _ { 1 } ) ^ { 2 } \\sum _ { i = 1 } ^ { m } ( x _ { i 2 } - \\hat { \\mu } _ { 2 } ) ^ { 2 } } }","title":"Korelasi"},{"location":"memahami/#matrik-kovarian","text":"Variansi dari untuk dua atribut X_1 X_1 dan X_2 X_2 dapat diringkas dalam matrik covarianse bujursangkar denga ukuran $2 \\times 2 $ dinyatakan dengan $$ \\left. \\begin{array}{l}{ \\Sigma = E [ ( X - \\mu ) ( X - \\mu ) ^ { T } ] }\\{ = E \\left[ \\left( \\begin{array} { c } { X _ { 1 } - \\mu _ { 1 } } \\ { X _ { 2 } - \\mu _ { 2 } } \\end{array} \\right) ( X _ { 1 } - \\mu _ { 1 } \\quad X _ { 2 } - \\mu _ { 2 } ) \\right ] }\\{ = \\left( \\begin{array} { c c } { E [ ( X _ { 1 } - \\mu _ { 1 } ) ( X _ { 1 } - \\mu _ { 1 } ) ] } & { E [ ( X _ { 1 } - \\mu _ { 1 } ) ( X _ { 2 } - \\mu _ { 2 } ) ] } \\ { E [ ( X _ { 2 } - \\mu _ { 2 } ) ( X _ { 1 } - \\mu _ { 1 } ) ] } & { E [ ( X _ { 2 } - \\mu _ { 2 } ) ( X _ { 2 } - \\mu _ { 2 } ) ] } \\end{array} \\right) }\\{ = \\left( \\begin{array} { c c } { \\sigma _ { 1 } ^ { 2 } } & { \\sigma _ { 12 } } \\ { \\sigma _ { 21 } } & { \\sigma _ { 2 } ^ { 2 } } \\end{array} \\right) }\\end{array} \\right. $$ Karena \\sigma_{12}=\\sigma_{21} \\sigma_{12}=\\sigma_{21} , $\\Sigma $ adalah matrik simetris. Matrik vovarian merekam variansi tertentu atribut pada diagonal utamanya, dan informasi covarian pada elemen element bukan diagonal. Total variance dari dua atribut dinyatakan sebagai jumlah elemen elemen diagonal dari $ \\Sigma $ , yang juga disebut trace dari $ \\Sigma $ dinyatakan dengan $$ \\operatorname { var } ( D ) = \\operatorname { tr } ( \\Sigma ) = \\sigma _ { 1 } ^ { 2 } + \\sigma _ { 2 } ^ { 2 } $$ Kita segera memiliki $ tr(\\Sigma)\\geq 0$ Secara umum covarian adalah non-negatif, karena $$ | \\Sigma | = \\operatorname { det } ( \\Sigma ) = \\sigma _ { 1 } ^ { 2 } \\sigma _ { 2 } ^ { 2 } - \\sigma _ { 12 } ^ { 2 } = \\sigma _ { 1 } ^ { 2 } \\sigma _ { 2 } ^ { 2 } - \\rho _ { 12 } ^ { 2 } \\sigma _ { 1 } ^ { 2 } \\sigma _ { 2 } ^ { 2 } = ( 1 - \\rho _ { 12 } ^ { 2 } ) \\sigma _ { 1 } ^ { 2 } \\sigma _ { 2 } ^ { 2 } $$ dimana kitu gunakan persamaan (2.23), yaiut \\rho_{12}\\sigma_1\\sigma_2 \\rho_{12}\\sigma_1\\sigma_2 . dengan |\\Sigma| |\\Sigma| adalah determinan dari matrik kovarian. Perhatikan bahwa |\\rho_{12}|\\leq 1 |\\rho_{12}|\\leq 1 menyebabkan \\rho_{12}^2 \\leq 1 \\rho_{12}^2 \\leq 1 sehingga det (\\Sigma) \\geq 1 (\\Sigma) \\geq 1 furthermore determinannya adalah non-negative. Matrik kovarian sampel dinyatakan dengan $$ \\hat { \\Sigma } = \\left( \\begin{array} { l l } { \\hat { \\sigma } _ { 1 } ^ { 2 } } & { \\hat { \\sigma } _ { 12 } } \\ { \\hat { \\sigma } _ { 12 } } & { \\hat { \\sigma } _ { 2 } ^ { 2 } } \\end{array} \\right) $$ Matrik kovarian sampe $ \\hat \\Sigma$ memilki karakteristik sama seperti \\Sigma \\Sigma , yaitu simetris dan |\\hat \\Sigma| \\geq 0 |\\hat \\Sigma| \\geq 0 dan itu dapat digunakan untum memudahkan mendapatkan total sampel dan variansi secara umum Contoh (Rata rata Sampel dan Covarian) Perhatikan atribut sepal length dan sepal width untuk data iris, seperti yang diplot dalam gambar 2.4. Ada n=150 data dalam d=2 d=2 ruang dimensi. Rata rata sampel adalah $$ \\hat { \\mu } = \\left( \\begin{array} { l } { 5.843 } \\ { 3.054 } \\end{array} \\right) $$ Matrik covarian dinyatakan dengan $$ \\hat { \\Sigma } = \\left( \\begin{array} { r r } { 0.681 } & { - 0.039 } \\ { - 0.039 } & { 0.187 } \\end{array} \\right) $$ Variansi untuk sepal length adalah \\hat \\sigma_1^2=0.681 \\hat \\sigma_1^2=0.681 dan sepal width adalah \\hat \\sigma_2^2=0.187 \\hat \\sigma_2^2=0.187 . Covarian antara dua atribut adalah \\hat \\sigma_{12}=-0.039 \\hat \\sigma_{12}=-0.039 dan korelasi antara dua atribut tersebut adalah $$ \\hat { \\rho } _ { 12 } = \\frac { - 0.039 } { \\sqrt { 0.681 \\cdot 0.187 } } = - 0.109 $$ Lalu, ada korelasi yang sangat lemah antara dua atribut tersebut Total variansi sampel dinyatakan dengan $$ \\operatorname { tr } ( \\hat { \\Sigma } ) = 0.681 + 0.187 = 0.868 $$ dan variansi secara umum dinyatakan dengan $$ \\hat { \\Sigma } | = \\operatorname { det } ( \\hat { \\Sigma } ) = 0.681 \\cdot 0.187 - ( - 0.039 ) ^ { 2 } = 0.126 $$","title":"Matrik  Kovarian"},{"location":"memahami/#analisa-multivariate","text":"Dalam analisa multivariate, kita melihat atribut numerik dengan d d dimensi X_1,X_2,...X_d X_1,X_2,...X_d . Data dinyatakan degan matrik n\\times d n\\times d seperti berikut $$ D = \\left( \\begin{array} { c c c c } { X _ { 1 } } & { X _ { 2 } } & { \\cdots } & { X _ { d } } \\ \\hline x _ { 11 } & { x _ { 12 } } & { \\cdots } & { x _ { 1 d } } \\ { x _ { 21 } } & { x _ { 22 } } & { \\cdots } & { x _ { 2 d } } \\ { \\vdots } & { \\vdots } & { \\ddots } & { \\vdots } \\ { x _ { n 1 } } & { x _ { n 2 } } & { \\cdots } & { x _ { n d } } \\end{array} \\right) $$ Jika dilihat dari baris data memiliki n n objek atatu vektor dalam d d ruang dimensi atribut $$ x _ { i } = ( x _ { i 1 } , x _ { i 2 } , \\ldots , x _ { i d } ) ^ { T } \\in \\mathbb R ^ { d } $$ Jika dilihat dari sudut pandang kolom, data diangga sebagai d d objek atau vektor dalam n n dimensi ruang dengan titik-titik data $$ X _ { j } = ( x _ { 1 j } , x _ { 2 j } , \\ldots , x _ { n j } ) ^ { T } \\in R ^ { n } $$ Jika dilihat dari sudut pandang probabilitas, d d atribut dimodelkan dengan variabel acak vektor X=(X_1,X_2,...X_d)^T X=(X_1,X_2,...X_d)^T dan titik titik x_i x_i dianggap sebagai sampel acak yang diperoleh dari X X , atribut atribut tersebut independent and identfically distributed dari X X (i.i.d X X )","title":"Analisa Multivariate"},{"location":"memahami/#mean_2","text":"Generalisasi persamaan (2.18) rata-rata vektor multivariate diperoleh dari masing-masing atribut yang dinyatakan dengan $$ \\mu = E [ X ] = \\left( \\begin{array} { c } { E [ X _ { 1 } ] } \\ { E [ X _ { 2 } ] } \\ { \\vdots } \\ { E [ X _ { d } ] } \\end{array} \\right) = \\left( \\begin{array} { c } { \\mu _ { 1 } } \\ { \\mu _ { 2 } } \\ { \\vdots } \\ { \\mu _ { d } } \\end{array} \\right) $$ Generalisasi persamaan (2.19) rata-rata sampel dinyatakan dengan $$ \\hat { \\mu } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } $$","title":"Mean"},{"location":"memahami/#matrik-kovarian_1","text":"Generalisasi persamaan (2.26) untuk d d dimensi, kovarian multicovariate di dinyatakan dengan matrik kovarian simetris $ d\\times d $yang menyatakan kovarian untuk setiap pasangan atribut $$ \\Sigma = E [ ( X - \\mu ) ( X - \\mu ) ^ { T } ] = \\left( \\begin{array} { c c c c } { \\sigma _ { 1 } ^ { 2 } } & { \\sigma _ { 12 } } & { \\cdots } & { \\sigma _ { 1 d } } \\ { \\sigma _ { 21 } } & { \\sigma _ { 2 } ^ { 2 } } & { \\cdots } & { \\sigma _ { 2 d } } \\ { \\cdots } & { \\cdots } & { \\cdots } & { \\cdots } \\ { \\sigma _ { d 1 } } & { \\sigma _ { d 2 } } & { \\cdots } & { \\sigma _ { d } ^ { 2 } } \\end{array} \\right) $$ Elemen diagonal $\\sigma_i^2 $ menyatakan variansi atribut X_i X_i , dimana elemen-elemen bukan diagonal \\sigma_{ij} = \\sigma_{ji} \\sigma_{ij} = \\sigma_{ji} menyatakan kovarian antara atribut pasangan X_i X_i dan X_j X_j . Matrik kovarian adalah positif semidefinite Contoh Rata-rata sample dan matrik covarian. Perhatikan semua atribut numerik untuk data iris, namanya sepal length, petal length, dan petal width. Rata rata multivarean dinyatakan dengan \\hat { \\mu } = ( 5.843 \\quad 3.054 \\quad 3.759 \\quad 1.199 ) ^ { T } $$ dan matrik covarian nya adalah $$ \\hat { \\Sigma } = \\left( \\begin{array} { r r r r } { 0.681 } & { - 0.039 } & { 1.265 } & { 0.513 } \\\\ { - 0.039 } & { 0.187 } & { - 0.320 } & { - 0.117 } \\\\ { 1.265 } & { - 0.320 } & { 3.092 } & { 1.288 } \\\\ { 0.513 } & { - 0.117 } & { 1.288 } & { 0.579 } \\end{array} \\right) $$ Jumlah variansi adalah $$ \\operatorname { var } ( D ) = \\operatorname { tr } ( \\hat { \\Sigma } ) = 0.681 + 0.187 + 3.092 + 0.579 = 4.539 \\hat { \\mu } = ( 5.843 \\quad 3.054 \\quad 3.759 \\quad 1.199 ) ^ { T } $$ dan matrik covarian nya adalah $$ \\hat { \\Sigma } = \\left( \\begin{array} { r r r r } { 0.681 } & { - 0.039 } & { 1.265 } & { 0.513 } \\\\ { - 0.039 } & { 0.187 } & { - 0.320 } & { - 0.117 } \\\\ { 1.265 } & { - 0.320 } & { 3.092 } & { 1.288 } \\\\ { 0.513 } & { - 0.117 } & { 1.288 } & { 0.579 } \\end{array} \\right) $$ Jumlah variansi adalah $$ \\operatorname { var } ( D ) = \\operatorname { tr } ( \\hat { \\Sigma } ) = 0.681 + 0.187 + 3.092 + 0.579 = 4.539 Contoh Perkalian dalam dan perkalian luar . Untuk mengdeskripsikan komputasi perkalian dalam dan perkalian luar dari matrik covarian, perhatikan data 2-dimensi $$ D = \\left( \\begin{array} { l l } { A _ { 1 } } & { A _ { 2 } } \\ \\hline 1 & { 0.8 } \\ { 5 } & { 2.4 } \\ { 9 } & { 5.5 } \\end{array} \\right) $$ Rata-rata vektor adalah sebagai berikut $$ \\hat { \\mu } = \\left( \\begin{array} { l } { \\hat { \\mu } _ { 1 } } \\ { \\hat { \\mu } _ { 2 } } \\end{array} \\right) = \\left( \\begin{array} { l } { 15 / 3 } \\ { 8.7 / 3 } \\end{array} \\right) = \\left( \\begin{array} { c } { 5 } \\ { 2.9 } \\end{array} \\right) $$ dan matrik data terpusat dinyatakan $$ Z = D - 1 \\cdot \\mu ^ { T } = \\left( \\begin{array} { l l } { 1 } & { 0.8 } \\ { 5 } & { 2.4 } \\ { 9 } & { 5.5 } \\end{array} \\right) - \\left( \\begin{array} { l } { 1 } \\ { 1 } \\ { 1 } \\end{array} \\right) \\left( \\begin{array} { l l } { 5 } & { 2.9 } \\end{array} \\right) = \\left( \\begin{array} { r r } { - 4 } & { - 2.1 } \\ { 0 } & { - 0.5 } \\ { 4 } & { 2.6 } \\end{array} \\right) $$ Pendekatan perkalian dalam [pers. 2.30] untuk menghitung matrik kovarian adalah $$ \\left. \\begin{array}{l}{ \\hat { \\Sigma } = \\frac { 1 } { n } Z ^ { T } Z = \\frac { 1 } { 3 } \\left( \\begin{array} { c c c } { - 4 } & { 0 } & { 4 } \\ { - 2.1 } & { - 0.5 } & { 2.6 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { - 4 } & { - 2.1 } \\ { 0 } & { - 0.5 } \\ { 4 } & { 2.6 } \\end{array} \\right) }\\{ = \\frac { 1 } { 3 } \\left( \\begin{array} { c c } { 32 } & { 18.8 } \\ { 18.8 } & { 11.42 } \\end{array} \\right) = \\left( \\begin{array} { c c } { 10.67 } & { 6.27 } \\ { 6.27 } & { 3.81 } \\end{array} \\right) }\\end{array} \\right. $$ Pendekatan lain yaitu dengan perkalian luar [pers. 2.31] dibyatakan dengan $$ \\hat { \\Sigma } = \\frac { 1 } { n } \\sum _ { j = 1 } ^ { n } z _ { i } \\cdot z _ { i } ^ { T } $$ = \\frac { 1 } { 3 } \\left [ \\left( \\begin{array} { c } { - 4 } \\\\ { - 2.1 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { - 4 } & { - 2.1 } \\end{array} \\right) + \\left( \\begin{array} { r r } { 0 } \\\\ { - 0.5 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { 0 } & { - 0.5 } \\end{array} \\right) + \\left( \\begin{array} { c } { 4 } \\\\ { 2.6 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { 4 } & { 2.6 } \\end{array} \\right)\\right ] = \\frac { 1 } { 3 } \\left [ \\left( \\begin{array} { c } { - 4 } \\\\ { - 2.1 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { - 4 } & { - 2.1 } \\end{array} \\right) + \\left( \\begin{array} { r r } { 0 } \\\\ { - 0.5 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { 0 } & { - 0.5 } \\end{array} \\right) + \\left( \\begin{array} { c } { 4 } \\\\ { 2.6 } \\end{array} \\right) \\cdot \\left( \\begin{array} { c c } { 4 } & { 2.6 } \\end{array} \\right)\\right ] \\left. \\begin{array} { l } { = \\frac { 1 } { 3 } [ \\left( \\begin{array} { c c } { 16.0 } & { 8.4 } \\\\ { 8.4 } & { 4.41 } \\end{array} \\right) + \\left( \\begin{array} { c c } { 0.0 } & { 0.0 } \\\\ { 0.0 } & { 0.25 } \\end{array} \\right) + \\left( \\begin{array} { c c } { 16.0 } & { 10.4 } \\\\ { 10.4 } & { 6.76 } \\end{array} \\right) ] } \\\\ { = \\frac { 1 } { 3 } \\left( \\begin{array} { c c } { 32.0 } & { 18.8 } \\\\ { 18.8 } & { 11.42 } \\end{array} \\right) = \\left( \\begin{array} { c c } { 10.67 } & { 6.27 } \\\\ { 6.27 } & { 3.81 } \\end{array} \\right) } \\end{array} \\right. \\left. \\begin{array} { l } { = \\frac { 1 } { 3 } [ \\left( \\begin{array} { c c } { 16.0 } & { 8.4 } \\\\ { 8.4 } & { 4.41 } \\end{array} \\right) + \\left( \\begin{array} { c c } { 0.0 } & { 0.0 } \\\\ { 0.0 } & { 0.25 } \\end{array} \\right) + \\left( \\begin{array} { c c } { 16.0 } & { 10.4 } \\\\ { 10.4 } & { 6.76 } \\end{array} \\right) ] } \\\\ { = \\frac { 1 } { 3 } \\left( \\begin{array} { c c } { 32.0 } & { 18.8 } \\\\ { 18.8 } & { 11.42 } \\end{array} \\right) = \\left( \\begin{array} { c c } { 10.67 } & { 6.27 } \\\\ { 6.27 } & { 3.81 } \\end{array} \\right) } \\end{array} \\right. dimana data terpusat z_i z_i adalah baris dari Z Z","title":"Matrik Kovarian"},{"location":"memahami/#atribut-kategorikal","text":"Kita asumsikan bahwa data terdiri dari satu atribut X X . Domain dari X X terdiri dari m m nilai simbolis dom(X)={a_1,a_2,...a_m} dom(X)={a_1,a_2,...a_m} . Data D D adalah n\\times 1 n\\times 1 matrik data simbolis yang dinyatakan dengan $$ D = \\left( \\begin{array} { c } { X } \\ { x _ { 1 } } \\ { x _ { 2 } } \\ { \\vdots } \\ { x _ { n } } \\end{array} \\right) $$ dimana setiap nilai x_i \\in dom(X) x_i \\in dom(X)","title":"Atribut Kategorikal"},{"location":"memahami/#variabel-bernouli","text":"Marilah kita lihat kasus ketika atribut kategorikal X X memililik domain $ {a_1,a_2}$ dengan m=2 m=2 . Kita dapat memodelkan X X sebagai variabel acak Bernouli, yang didasarkan pada dua nilai berbeda yaitu 1 dan 0, sesuai dengan pemetaan $$ X ( v ) = \\left{ \\begin{array} { l l } { 1 } & { \\text { if } v = a _ { 1 } } \\ { 0 } & { \\text { if } v = a _ { 2 } } \\end{array} \\right. $$ Fungsi massa probabilitas (PMF) dari X X dinyatakan dengan $$ P ( X = x ) = f ( x ) = \\left{ \\begin{array} { l l } { p _ { 1 } } & { \\text { if } x = 1 } \\ { p _ { 0 } } & { \\text { if } x = 0 } \\end{array} \\right. $$ dimana p_1 p_1 dan p_0 p_0 adalah parameter distribusi, yang harus memenuhi kondisi $$ p_1+p_0=1 $$ Karena hanya ada satu parameter bebas, biasanya menotasikan p_1=p p_1=p maka p_0=1-p p_0=1-p . Fungsi Massa Probabilitas dari variabel acak Bernouli X X dapat kemudian ditulis dengan $$ P ( X = x ) = f ( x ) = p ^ { x } ( 1 - p ) ^ { 1 - x } $$ Kita dapat melihat bahwa P ( X = 1 ) = p ^ { 1 } ( 1 - p ) ^ { 0 } = p \\text { and } P ( X = 0 ) = p ^ { 0 } ( 1 - p ) ^ { 1 } = 1 - p P ( X = 1 ) = p ^ { 1 } ( 1 - p ) ^ { 0 } = p \\text { and } P ( X = 0 ) = p ^ { 0 } ( 1 - p ) ^ { 1 } = 1 - p seperti yand diharapkan Mean dan Variansi Nilai harapan dari X X dinyatakan dengan $$ \\mu = E [ X ] = 1 \\cdot p + 0 \\cdot ( 1 - p ) = p $$ dan variansi dari X X dinyatakan dengan $$ \\left. \\begin{array}{l}{ \\sigma ^ { 2 } = \\operatorname { var } ( X ) = E [ X ^ { 2 } ] - ( E [ X ] ) ^ { 2 } }\\ \\hspace{7mm}= ( 1 ^ { 2 } \\cdot p + 0 ^ { 2 } \\cdot ( 1 - p ) ) - p ^ { 2 } = p - p ^ { 2 } = p ( 1 - p ) \\\\end{array} \\right. $$ Rata-rata sampel dan Variansi Untuk mengestimasi parameter dari variabel Bernouli X X , kita asumsikan bahwa setiap simbol dipetakan ke nilai biner. Sehingga, sekumpulan nilai {x_1,x_2,...x_n} {x_1,x_2,...x_n} diasumsikan menjadi sampel acak yang diperoleh dari X X (yaitu setiap $ x_i$ adalah IID dengan X X . Rata-rata sampel dinyatakan dengan $$ \\hat { \\mu } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } = \\frac { n _ { 1 } } { n } = \\hat { p } $$ dimana n_1 n_1 adalah banyaknya titik dengan x_1=1 x_1=1 dalam sampel acak (sama dengan banyak kejadian dari simbol a_1 a_1 ) Misal n_0=n-n_1 n_0=n-n_1 menyatakan banyak titik dengan x_i=0 x_i=0 dalam sampel acak. Variansi sample dinyatakan dengan $$ \\left. \\begin{array}{l}{ \\hat { \\sigma } ^ { 2 } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\hat { \\mu } ) ^ { 2 } }\\ \\hspace{7mm}{ = \\frac { n _ { 1 } } { n } ( 1 - \\hat { p } ) ^ { 2 } + \\frac { n - n _ { 1 } } { n } ( - \\hat { p } ) ^ { 2 } }\\\\hspace{7mm}{ = \\hat { p } ( 1 - \\hat { p } ) ^ { 2 } + ( 1 - \\hat { p } ) \\hat { p } ^ { 2 } }\\\\hspace{7mm}{ = \\hat { p } ( 1 - \\hat { p } ) ( 1 - \\hat { p } + \\hat { p } ) }\\\\hspace{7mm}{ = \\hat { p } ( 1 - \\hat { p } ) }\\end{array} \\right. $$ Variansi sampel dapat juga diperoleh langsung dari persamaan(3.1) dengan mensubsitusikan \\hat p \\hat p untuk p p . Contoh Perhatikan atribut sepal length ( X X ) untuk dataset iris dalam tabel 1.1. Marilah kita definisikan bunga iris dengan Long jika bunga itu sepal length dalam range [7, \\infty ] [7, \\infty ] , dan short jika sepal length dalam range [-\\infty,7] [-\\infty,7] . Kemudian X_1 X_1 dapat dinyatakan dengan atribut kategorikan dengan domain {Long,Short}. Dari sampel yang diamati ukuran n=150 n=150 , kita menemukan 13 iris long. Rata-rata sampel dari X_1 X_1 adalah $$ \\hat { \\mu } = \\hat { p } = 13 / 150 = 0.087 $$ dan variansinya adalah $$ \\hat { \\sigma } ^ { 2 } = \\hat { p } ( 1 - \\hat { p } ) = 0.087 ( 1 - 0.087 ) = 0.087 \\cdot 0.913 = 0.079 $$","title":"Variabel Bernouli"},{"location":"memahami/#ditribusi-binomial-banyaknya-kejadian","text":"Diberikan variabel Bernoulli X X , misal \\{x_1,x_2,...x_n\\} \\{x_1,x_2,...x_n\\} menyatakan sampel acak dari ukuran n n yang diperoleh dari X X . Misal N N adalah variabel acak yang menyatakan numlah kejadi dari simbol a_1 a_1 (nilai X=1 X=1 ) dalam sampe. N adalah distribusi binomial yang dinyatakan dengan $$ f ( N = n _ { 1 } | n , p ) = \\left( \\begin{array} { l } { n } \\ { n _ { 1 } } \\end{array} \\right) p ^ { n _ { 1 } } ( 1 - p ) ^ { n - n _ { 1 } } $$ Dalam kenyataannya, N N adalah jumlah dari n n variabel acak Bernoulli x_i x_i yang saling bebas dan (IID) dengan X X yaitu N=\\sum_{i=1}^n x_i N=\\sum_{i=1}^n x_i . Dengan liniearitas dari ekpektasi, mean atau jumlah harapan dari kejadian simbol a_i a_i dinyatakan dengan $$ \\mu _ { N } = E [ N ] = E \\left[ \\sum _ { i = 1 } ^ { n } x _ { i } \\right] = \\sum _ { i = 1 } ^ { n } E [ x _ { i } ] = \\sum _ { i = 1 } ^ { n } p = n p $$ Karena x_i x_i adalah semuanya saling bebas, variansi dari N N dinyatakan dengan $$ \\sigma _ { N } ^ { 2 } = \\operatorname { var } ( N ) = \\sum _ { i = 1 } ^ { n } \\operatorname { var } ( x _ { i } ) = \\sum _ { i = 1 } ^ { n } p ( 1 - p ) = n p ( 1 - p ) $$ Contoh 3.2. Dengan meneruskan contoh 3.1, kita dapat menggunakan parameter yang telah diestimasi \\hat p=0.087 \\hat p=0.087 untuk menghitung banyaknya kejadian yang diharapkan N long dari sepal length. distribusi binomial Iris $$ E [ N ] = n \\hat { p } = 150 \\cdot 0.087 = 13 $$ Dalam kasus ini, karena p p dihitung dari sample melalui \\hat p \\hat p , tidak mengherankan bahwa jumlah kejadian diharapkan dari Long Iris sama dengan kejadian yang sebenarnya. Akan tetapi yang lebih menarik adalah kita dapat menghitung variansi jumlah kejadian $$ \\operatorname { var } ( N ) = n \\hat { p } ( 1 - \\hat { p } ) = 150 \\cdot 0.079 = 11.9 $$ Meningkatnya ukuran sample, distribusi binomial seperti yang diberikan dapalam persamaan 3.3 cenderung ke distribusi normal dengan \\mu=13 \\mu=13 dan \\sigma=\\sqrt{11.9}=3.45 \\sigma=\\sqrt{11.9}=3.45 . Sehingga dengan kepercaan lebih besar dari 95%, kita dapat mengklam bahwa jumlah kejadian dari a_i a_i akan terletak dalam rentang \\mu \\pm 2 \\sigma = [ 9.55,16.45 ] \\mu \\pm 2 \\sigma = [ 9.55,16.45 ] yang mengikuti dari fakta bahwa untuk distribusi normal 95,45% dari massa probabilitas terletak dalam dua standar deviasi dari rata-rata.","title":"Ditribusi binomial : banyaknya kejadian"},{"location":"memahami/#variable-multivariate-bernoulli","text":"Sekarang kita memandang kasus umum ketika X X adalah atribut kategorical dengan domain \\{a_1,a_2,...a_m\\} \\{a_1,a_2,...a_m\\} . Kita dapat memodelkan X X sebagai variabel acak Bernoulli m m -dimensi X = ( A _ { 1 } , A _ { 2 } , \\ldots , A _ { m } ) ^ { T } X = ( A _ { 1 } , A _ { 2 } , \\ldots , A _ { m } ) ^ { T } dimana setiap A_i A_i adalah variabel Bernoulli dengan parameter p_i p_i yang menotasikan probabilitas dari pengamatan simbol a_i a_i . Akan tetapi karena X X dapat mengasumsikan hanya satu dari nilai simbolik pada suatu waktum jika X=a_i X=a_i maka A_i=1 A_i=1 dan A_j=0 A_j=0 untuk semua j \\neq i j \\neq i . Variabel acak X \\in {0,1}^m X \\in {0,1}^m , dan jika X=a_i X=a_i , maka X=e_i X=e_i , dimana e_i e_i adalah standar vektor basis ke i, e_i\\in\\mathbb R^m e_i\\in\\mathbb R^m yang dinyatakan dengan $$ e _ { i } = ( \\overbrace { 0 , \\ldots , 0 } ^ { i - 1 } , 1 , \\overbrace { 0 , \\ldots , 0 } ^ { m - i } ) ^ { T } $$ Pada e_i e_i hanya elemen ke i adalah 1 ( e_{ii}=1 e_{ii}=1 ) , sedangkan semua elemen yang lain adalah nol, ( e_{ij}=0, \\forall j \\neq i e_{ij}=0, \\forall j \\neq i ). Disini, definis yang lebih tepat dari variabel Bernoulli multivariate , yaitu generalisasi dari variabel Bernoullii dari dua hasil ke m m hasil. Kita kemudian memodelkan atribut kategorical X X sebagai variabel Bernoulli multivariate X X didefinisikan dengan $$ X ( v ) = e _ { i } \\text { if } v = a _ { i } $$ Rentang dari X X terdiri dari m m nilai vektor berbeda \\{e_1,e_2,...e_m\\} \\{e_1,e_2,...e_m\\} dengan fungsi massa probabilitas dari X X dinyatakan dengan $$ P ( X = e _ { i } ) = f ( e _ { i } ) = p _ { i } $$ dimana p_i p_i adalah probabilitas dari nilai pengamatan a_i a_i . Parameter ini harus memenuhi kondisi $$ \\sum _ { i = 1 } ^ { m } p _ { i } = 1 $$ Fungsi massa prababilitas dapat ditulis secara utuh sebagai berikut $$ P ( X = e _ { i } ) = f ( e _ { i } ) = \\prod _ { j = 1 } ^ { m } p _ { j } ^ { e _ { i j } }Ka $$ Kareana e_ii=1 e_ii=1 dan e_ij=0 e_ij=0 funtuk $ j\\neq i$, kita dapat melihat bahwa, seperti yang diharapkan, kita miliki $$ f ( e _ { i } ) = \\prod _ { j=1 } ^ { m } p _ { j } ^ { e _ { i j } } = p _ { 1 } ^ { e _ { i 0 } } \\times \\cdots p _ { i } ^ { e _ { i i } } \\cdots \\times p _ { m } ^ { e _ { i m } } = p _ { 1 } ^ { 0 } \\times \\cdots p _ { i } ^ { 1 } \\cdots \\times p _ { m } ^ { 0 } = p _ { i } $$ \\left. \\begin{array} { | l | l | l | } \\hline \\text { Bins } & { { \\text { Domain } } } & { { \\text { Counts } } } \\\\ \\hline [ 4.3,5.2 ] & { \\text { Very Short } ( a _ { 1 } ) } & { n _ { 1 } = 45 } \\\\ { ( 5.2,6.1 ] } & { \\text { Short } ( a _ { 2 } ) } & { n _ { 2 } = 50 } \\\\ { ( 6.1,7.0 ] } & { \\text { Long } ( a _ { 3 } ) } & { n _ { 3 } = 43 } \\\\ { ( 7.0,7.9 ] } & { \\text { Very Long } ( a _ { 4 } ) } & { n _ { 4 } = 12 } \\\\ \\hline \\end{array} \\right. \\left. \\begin{array} { | l | l | l | } \\hline \\text { Bins } & { { \\text { Domain } } } & { { \\text { Counts } } } \\\\ \\hline [ 4.3,5.2 ] & { \\text { Very Short } ( a _ { 1 } ) } & { n _ { 1 } = 45 } \\\\ { ( 5.2,6.1 ] } & { \\text { Short } ( a _ { 2 } ) } & { n _ { 2 } = 50 } \\\\ { ( 6.1,7.0 ] } & { \\text { Long } ( a _ { 3 } ) } & { n _ { 3 } = 43 } \\\\ { ( 7.0,7.9 ] } & { \\text { Very Long } ( a _ { 4 } ) } & { n _ { 4 } = 12 } \\\\ \\hline \\end{array} \\right. Contoh : Marilah kita lihat atribut sepal length ( X_1 X_1 ) untuk data Iris seperti yang ditunjukkan dalam tabel 1.2. Kita membagi sepal length kedalam empat interval yang sama, dan memberikan nama untuk setiap interval seperti yang diunjukkan dalam tabel 3.1. Kita lihat X_1 X_1 sebagai atribut kategorical dengan domain $$ {a _ { 2 } = \\text { VeryShort, } a _ { 2 } = \\text { Short, } a _ { 3 } = \\operatorname { Long } , a _ { 4 } = \\operatorname{Very Long}} $$ Kita memodelkan atribut kategorical X_1 X_1 sebagai variabel X X Bernoulli multivariate, didefinisikan dengan $$ X ( v ) = \\left{ \\begin{array} { l l } { e _ { 1 } = ( 1,0,0,0 ) } & { \\text { jika } v = a _ { 1 } } \\ { e _ { 2 } = ( 0,1,0,0 ) } & { \\text { jika } v = a _ { 2 } } \\ { e _ { 3 } = ( 0,0,1,0 ) } & { \\text { jika } v = a _ { 3 } } \\ { e _ { 4 } = ( 0,0,0,1 ) } & { \\text { jika } v = a _ { 4 } } \\end{array} \\right. $$ Misalkan, simbol x_1=Short=a_2 x_1=Short=a_2 dinyatakan dengan (0,1,0,0)^T=e_2 (0,1,0,0)^T=e_2 Mean Mean atau nilai harapan dari X X dapat diperoleh dengan $$ \\mu = E [ X ] = \\sum _ { i = 1 } ^ { m } e _ { i } f ( e _ { i } ) = \\sum _ { i = 1 } ^ { m } e _ { i } p _ { i } = \\left( \\begin{array} { l } { 1 } \\ { 0 } \\ { \\vdots } \\ { 0 } \\end{array} \\right) p _ { 1 } + \\cdots + \\left( \\begin{array} { l } { 0 } \\ { 0 } \\ { \\vdots } \\ { 1 } \\end{array} \\right) p _ { m } = \\left( \\begin{array} { c } { p _ { 1 } } \\ { p _ { 2 } } \\ { \\vdots } \\ { p _ { m } } \\end{array} \\right) = p $$","title":"Variable multivariate Bernoulli"},{"location":"memahami/#mengukur-jarak-data","text":"","title":"Mengukur Jarak Data"},{"location":"memahami/#mengukur-jarak-tipe-numerik","text":"Salah satu tantangan dalam era ini dengan datatabase yang memiliki banyak tipe data. Mngukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan Sebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaiut v_1, v_2 v_1, v_2 menyatakandua vektor yang menyatakan v_1 = {x_1, x_2, . . ., x_n}, v_2 ={y_1, y_2, . . ., y_n}, v_1 = {x_1, x_2, . . ., x_n}, v_2 ={y_1, y_2, . . ., y_n}, dimana x_i, y_i x_i, y_i disebut attribut. Ada beberapa ukuran similaritas datau ukuran jarak, diantaranya 2","title":"Mengukur Jarak  Tipe Numerik"},{"location":"memahami/#minkowski-distance","text":"Kelompk Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski distance. Minkowski distance dinyatakan dengan d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 diman m m adalah bilangan riel positif dan x_i x_i dan $ y_i$ adalah dua vektor dalam runang dimensi n n Implementasi ukuran jarak Minkowski pada model clustering data atribut dilakukan normalisasi untuk menghindari dominasi dari atribut yang memiliki skala data besar.","title":"Minkowski Distance"},{"location":"memahami/#manhattan-distance","text":"Manhattan distance adalah kasus khsusu dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. BIla ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right|","title":"Manhattan distance"},{"location":"memahami/#euclidean-distance","text":"Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini.","title":"Euclidean distance"},{"location":"memahami/#average-distance","text":"Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adala versi modikfikasid ari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x,y x,y dalam ruang dimensi n n , rata-rata jarak didefinisikan dengan d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } } d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } }","title":"Average Distance"},{"location":"memahami/#weighted-euclidean-distance","text":"Jika berdasarkan tingkatan penting dari masing masing atribut ditentukan, maka Weighted Euclidean distance adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. Ukuran ini dirumuskan dengan $$ d _ { w e } = \\left ( \\sum _ { i = 1 } ^ { n } w _ { i } ( x _ { i } - y _ { i } \\right) ^ { 2 } ) ^ { \\frac { 1 } { 2 } } $$ dimana w_i w_i adalah bobot yang diberikan pada atribut ke i.","title":"Weighted euclidean distance"},{"location":"memahami/#chord-distance","text":"Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan d _ { \\text {chord} } = \\left ( 2 - 2 \\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { \\| x \\| _ { 2 } \\| y \\| _ { 2 } } \\right) ^ { \\frac { 1 } { 2 } } d _ { \\text {chord} } = \\left ( 2 - 2 \\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { \\| x \\| _ { 2 } \\| y \\| _ { 2 } } \\right) ^ { \\frac { 1 } { 2 } } dimana \\| x \\|_ {2} \\| x \\|_ {2} adalah L^{2} \\text {-norm} \\| x \\|_{2} = \\sqrt { \\sum_{ i = 1 }^{ n }x_{i}^{2}} L^{2} \\text {-norm} \\| x \\|_{2} = \\sqrt { \\sum_{ i = 1 }^{ n }x_{i}^{2}}","title":"Chord distance"},{"location":"memahami/#mahalanobis-distance","text":"Mahalanobis distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antra data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. Mahalanobis distance dinyatakan dengan d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } diman S S adalah matrik covariance data.","title":"Mahalanobis distance"},{"location":"memahami/#cosine-measure","text":"Ukuran Cosine similarity lebih banyak digunakan dalam similaritas dokumen dan dinyatakan dengan Cosine(x,y)=\\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { \\| x \\| _ { 2 } \\| y \\| _ { 2 } } Cosine(x,y)=\\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { \\| x \\| _ { 2 } \\| y \\| _ { 2 } } dimana \\|y\\|_{2} \\|y\\|_{2} adalah Euclidean norm dari vektor y=(y_{1} , y_{2} , \\dots , y_{n} ) y=(y_{1} , y_{2} , \\dots , y_{n} ) didefinisikan dengan \\|y\\|_{2}=\\sqrt{ y _ { 1 } ^ { 2 } + y _ { 2 } ^ { 2 } + \\ldots + y _ { n } ^ { 2 } } \\|y\\|_{2}=\\sqrt{ y _ { 1 } ^ { 2 } + y _ { 2 } ^ { 2 } + \\ldots + y _ { n } ^ { 2 } }","title":"Cosine measure"},{"location":"memahami/#pearson-correlation","text":"Pearson correlation banyak digunakan dalam data expresi gen. Ukuran similaritas ini menghitung similaritas antara duan bentuk pola expresi gen. Pearson correlation didefinisikan dengan Pearson ( x , y ) = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\mu _ { x } ) ( y _ { i } - \\mu _ { y } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } } Pearson ( x , y ) = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\mu _ { x } ) ( y _ { i } - \\mu _ { y } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } } The Pearson correlation kelemahannya adalah sensitif terhadap outlier","title":"Pearson correlation"},{"location":"memahami/#mengukur-jarak-atribut-binary","text":"Mari kita lihat similaritas dan desimilirity untuk objek yang dijelaskan oleh atribut biner simetris atau asimetris. Aatribut biner hanya memiliki dua status: 0 dan 1 Contoh atribut perokok menggambarkan seorang pasien, misalnya, 1 menunjukkan bahwa pasien merokok, sedangkan 0 menunjukkan pasien tidak merokok. Memperlakukan atribut biner sebagai atribut numerik tidak diperkenankan. Oleh karena itu, metode khusus untuk data biner diperlukan untuk membedakan komputasi. Jadi, bagaimana kita bisa menghitung ketidaksamaan antara dua atribut biner? \u201dSatu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2 \\times 2 2 \\times 2 di mana q q adalah jumlah atribut yang sama dengan 1 untuk kedua objek i i dan j j , r r adalah jumlah atribut yang sama dengan 1 untuk objek i i tetapi 0 untuk objek j j , s s adalah jumlah atribut yang sama dengan 0 untuk objek i i tetapi 1 untuk objek j j , dan t t adalah jumlah atribut yang sama dengan 0 untuk kedua objek i i dan j j . Jumlah total atribut adalah p p , di mana p=q+r+s+t p=q+r+s+t Ingatlah bahwa untuk atribut biner simetris, masing-masing nilai bobot yang sama.Dissimilarity yang didasarkan pada atribut aymmetric binary disebut symmetric binary dissimilarity. Jika objek i dan j dinyatakan sebagai atribut biner simetris, maka dissimilarity antar i i dan j j adalah d ( i , j ) = \\frac { r + s } { q + r + s + t } d ( i , j ) = \\frac { r + s } { q + r + s + t } Untuk atribut biner asimetris, kedua kondisi tersebut tidak sama pentingnya, seperti hasil positif (1) dan negatif (0) dari tes penyakit. Diberikan dua atribut biner asimetris, pencocokan keduanya 1 (kecocokan positif) kemudian dianggap lebih signifikan daripada kecocokan negatif. Ketidaksamaan berdasarkan atribut-atribut ini disebut asimetris biner dissimilarity, di mana jumlah kecocokan negatif, t, dianggap tidak penting dan dengan demikian diabaikan. Berikut perhitungannya d ( i , j ) = \\frac { r + s } { q + r + s } d ( i , j ) = \\frac { r + s } { q + r + s } Kita dapat mengukur perbedaan antara dua atribut biner berdasarkan pada disimilarity. Misalnya, biner asimetris kesamaan antara objek i i dan j j dapat dihitung dengan \\operatorname { sim } ( i , j ) = \\frac { q } { q + r + s } = 1 - d ( i , j ) \\operatorname { sim } ( i , j ) = \\frac { q } { q + r + s } = 1 - d ( i , j ) Persamaan similarity ini disebut dengan Jaccard coefficient","title":"Mengukur Jarak Atribut Binary"},{"location":"memahami/#mengukur-jarak-tipe-categorical","text":"Ada beberapa macam pengukuran untuk tipe data categorical 3","title":"Mengukur Jarak Tipe categorical"},{"location":"memahami/#overlay-metric","text":"Ketika semua atribut adalah bertipe nominal, ukuran jarak yang paling sederhana adalah dengan Ovelay Metric (OM) yang dinyatakan dengan d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\delta ( a _ { i } ( x ) , a _ { i } ( y ) ) d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\delta ( a _ { i } ( x ) , a _ { i } ( y ) ) dimana n n adalah banyaknya atribut, a_i(x) a_i(x) dan a_i(y) a_i(y) adalah nilai atribut ke i i yaitu A_i A_i dari masing masing objek x x dan y y , \\delta \\ ( a_{ i } ( x ) , a_{ i } ( y ) ) \\delta \\ ( a_{ i } ( x ) , a_{ i } ( y ) ) adalah 0 jika a _ { i } ( x ) = a _ { i } ( y ) a _ { i } ( x ) = a _ { i } ( y ) dan 1 jika sebaliknya. OM banyak digunakan oleh instance-based learning dan locally weighted learning. Jelas sekali , ini sedikit beruk untuk mengukur jarak antara masing-masing pasangan sample, karena gagal memanfaatkan tambahan informasi yang diberikan oleh nilai atribut nominal yang bisa membantu dalam generalisasi.","title":"Overlay Metric"},{"location":"memahami/#value-difference-metric-vdm","text":"VDM dikenalkan oleh Standfill and Waltz, versi sederhana dari VDM tanpa skema pembobotan didefinsisikan dengan d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\sum _ { c = 1 } ^ { C } \\left| P ( c | a _ { i } ( x ) ) - P ( c | a _ { i } ( y ) ) \\right | d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\sum _ { c = 1 } ^ { C } \\left| P ( c | a _ { i } ( x ) ) - P ( c | a _ { i } ( y ) ) \\right | dimana C C adalah banyaknya kelas, P(c|a_i(x)) P(c|a_i(x)) adalah probabilitas bersyarat dimana kelas x x adalah c c dari atribut A_i A_i , yang memiliki nilai a_i(x) a_i(x) , P(c|a_i(y)) P(c|a_i(y)) adalah probabilitas bersyarat dimana kelas y y adalah c c dengan atribut A_i A_i memiliki nilai a_i(y) a_i(y) VDM mengasumsikan bahwa dua nilai dari atribut adalah lebih dekat jika memiliki klasifikasi sama. Pendekatan lain berbasi probabilitas adalah SFM (Short and Fukunaga Metric) yang kemudian dikembangkan oleh Myles dan Hand dan didefinisikan dengan d ( x , y ) = \\sum _ { c = 1 } ^ { C } \\left | P ( c | x ) - P ( c | y ) \\right| d ( x , y ) = \\sum _ { c = 1 } ^ { C } \\left | P ( c | x ) - P ( c | y ) \\right| diman probabilitas keanggotaan kelas diestimasi dengan P(c|x) P(c|x) dan P(c|y) P(c|y) didekati dengan Naive Bayes,","title":"Value Difference Metric (VDM)"},{"location":"memahami/#minimum-risk-metric-mrm","text":"Ukuran ini dipresentasikan oleh Blanzieri and Ricci, berbeda dari SFM yaitu meminimumkan selisih antara kesalahan berhingga dan kesalahan asymtotic. MRM meminimumkan risk of misclassification yang didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } P ( c | x ) ( 1 - P ( c | y ) ) $$","title":"Minimum Risk Metric (MRM)"},{"location":"memahami/#mengukur-jarak-tipe-ordinal","text":"Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. Kategori-kategori ini disusun dalam peringkat. Yaitu, rentang atribut numerik dapat dipetakan ke atribut ordinal f f yang memiliki M_f M_f state. Misalnya, kisaran suhu atribut skala-skala (dalam Celcius)dapat diatur ke dalam status berikut: \u221230 hingga \u221210, \u221210 hingga 10, 10 hingga 30, masing-masing mewakili kategori suhu dingin, suhu sedang, dan suhu hangat. M M adalah jumlah keadaan yang dapat dilakukan oleh atribut ordinalmemiliki. State ini menentukan peringkat 1, ..., M_f 1, ..., M_f Perlakuan untuk atribut ordinal adalah cukup sama dengan atribut numerik ketika menghitung disimilarity antara objek 4 . Misalkan f f adalah atribut-atribut dari atribut ordinal dari n n objek. Menghitung disimilarity terhadap f fitur sebagai berikut: Nilai f f untuk objek ke- i i adalah x_{if} x_{if} , dan f f memiliki M_f M_f status urutan , mewakili peringkat 1, .., M_f 1, .., M_f Ganti setiap x_{if} x_{if} dengan peringkatnya, r_{if} \\in \\{1...M_f\\} r_{if} \\in \\{1...M_f\\} Karena setiap atribut ordinal dapat memiliki jumlah state yang berbeda, diperlukan untuk memetakan rentang setiap atribut ke [0,0, 1.0] sehingga setiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data dengan mengganti peringkat r_{if} r_{if} dengan $$ z _ { i f } = \\frac { r _ { i f } - 1 } { M _ { f } - 1 } $$ Dissimilarity kemudian dihitung dengan menggunakan ukuran jarak seperti atribut numerik dengan data yang baru setelah ditransformasi $ z _ { i f }$","title":"Mengukur Jarak Tipe Ordinal"},{"location":"memahami/#menghitung-jarak-tipe-campuran","text":"Menghitung ketidaksamaan antara objek dengan atribut campuran yang berupa nominal, biner simetris, biner asimetris, numerik, atau ordinal yang ada pada kebanyakan databasae dapat dinyatakan dengan memproses semua tipe atribut secara bersamaan 5 . Salah satu teknik tersebut menggabungkan atribut yang berbeda ke dalam matriks ketidaksamaan tunggal dan menyatakannya dengan skala interval antar [0,0, 1.0] [0,0, 1.0] . Misalkan data berisi atribut p p tipe campuran. Ketidaksamaan (disimilarity ) antara objek i i dan j j dinyatakan dengan d ( i , j ) = \\frac { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } d _ { i j } ^ { ( f ) } } { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } } d ( i , j ) = \\frac { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } d _ { i j } ^ { ( f ) } } { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } } dimana \\delta_{ij}^{f}=0 \\delta_{ij}^{f}=0 - jika x_{if} x_{if} atau x_{jf} x_{jf} adalah hilang (i.e., tidak ada pengukuran dari atribut f untuk objek i i atau objek j j ) jika x_{if}=x_{jf}=0 x_{if}=x_{jf}=0 dan atribut f f adalah binary asymmetric, selain itu \\delta_{ij}^{f}=1 \\delta_{ij}^{f}=1 Kontribusi dari atribut f f untuk dissimilarity antara i dan j (yaitu. d_{ij}^{f} d_{ij}^{f} ) dihitung bergantung pada tipenya, Jika f f adalah numerik, d_{ij}^{f}=\\frac{ \\|x _{if}-x_{jf}\\|}{max_hx_{hf}-min_hx{hf}} d_{ij}^{f}=\\frac{ \\|x _{if}-x_{jf}\\|}{max_hx_{hf}-min_hx{hf}} , di mana h menjalankan semua nilai objek yang tidak hilang untuk atribut f Jika f f adalah nominal atau binary,$d_{ij}^{f}=0 $jika x_{if}=x_{jf} x_{if}=x_{jf} , sebaliknya d_{ij}^{f}=1 d_{ij}^{f}=1 Jika f f adalah ordinal maka hitung rangking r_{if} r_{if} dan \\mathcal z_{if}=\\frac {r_{if}-1}{M_f-1} \\mathcal z_{if}=\\frac {r_{if}-1}{M_f-1} , dan perlakukan z_{if} z_{if} sebagai numerik.","title":"Menghitung Jarak Tipe Campuran"},{"location":"memahami/#referensi","text":"Cielen, Davy, Arno Meysman, and Mohamed Ali. Introducing data science: big data, machine learning, and more, using Python tools . Manning Publications Co., 2016. \u21a9 Shirkhorshidi, Ali Seyed, Saeed Aghabozorgi, and Teh Ying Wah. \"A comparison study on similarity and dissimilarity measures in clustering continuous data.\" PloS one 10.12 (2015): e0144059. \u21a9 Li, Chaoqun, and Hongwei Li. \"A Survey of Distance Metrics for Nominal Attributes.\" JSW 5.11 (2010): 1262-1269. \u21a9 Han, Jiawei, Jian Pei, and Micheline Kamber. Data mining: concepts and techniques . Elsevier, 2011. \u21a9 Wilson, D. Randall, and Tony R. Martinez. \"Improved heterogeneous distance functions.\" Journal of artificial intelligence research 6 (1997): 1-34. \u21a9","title":"Referensi"},{"location":"release-notes/","text":"Release notes \u00b6 Upgrading \u00b6 To upgrade Material to the latest version, use pip : pip install --upgrade mkdocs-material To inspect the currently installed version, use the following command: pip show mkdocs-material Material 3.x to 4.x \u00b6 Material for MkDocs 4.x finally fixes incorrect layout on Chinese systems. The fix includes a mandatory change of the base font-size from 10px to 20px which means all rem values needed to be updated. Within the theme, px to rem calculation is now encapsulated in a new function called px2rem which is part of the SASS code base. If you use Material with custom CSS that is based on rem values, note that those values must now be divided by 2. Now, 1.0rem doesn't map to 10px , but 20px . To learn more about the problem and implications, please refer to the issue in which the problem was discovered and fixed. Material 2.x to 3.x \u00b6 Material for MkDocs 3.x requires MkDocs 1.0 because the way paths are resolved internally changed significantly. Furthermore, pages was renamed to nav , so remember to adjust your mkdocs.yml file. All extended templates should continue to work but in order to make them future-proof the url filter should be introduced on all paths. Please see the official release notes for further guidance. Material 1.x to 2.x \u00b6 Material for MkDocs 2.x requires MkDocs 0.17.1, as this version introduced changes to the way themes can define options. The following variables inside your project's mkdocs.yml need to be renamed: extra.feature becomes theme.feature extra.palette becomes theme.palette extra.font becomes theme.font extra.logo becomes theme.logo Favicon support has been dropped by MkDocs, it must now be defined in theme.favicon (previously site_favicon ). Localization is now separated into theme language and search language. While there can only be a single language on theme-level, the search supports multiple languages which can be separated by commas. See the getting started guide for more guidance. The search tokenizer can now be set through extra.search.tokenizer . Changelog \u00b6 4.4.0 _ June 15, 2019 \u00b6 Added Slovenian translations Reverted template minification in favor of mkdocs-minify-plugin Fixed #1114 : Tabs don't reappear when default font-size is smaller than 16 4.3.1 _ May 23, 2019 \u00b6 Fixed spelling error in Danish translations 4.3.0 _ May 17, 2019 \u00b6 Added support for changing header through metadata title property Added font-display: swap to Google Font loading logic Removed whitespace from templates, saving 4kb ( .7kb gzipped) per request Fixed alignment of repository icons on tablet and desktop 4.2.0 _ April 28, 2019 \u00b6 Added Norwegian (Nynorsk) translations Fixed loss of focus in non-form input elements due to search hotkeys Fixed #1067 : Search hotkeys not working for mobile/tablet screensize Fixed #1068 : Search not correctly aligned for tablet screensize 4.1.2 _ April 16, 2019 \u00b6 Fixed #1072 : HTML tags appearing in navigation link titles 4.1.1 _ March 28, 2019 \u00b6 Fixed minor CSS errors detected during validation 4.1.0 _ March 22, 2019 \u00b6 Fixed #1023 : Search for Asian languages broken after Lunr.js update Fixed #1026 : contenteditable elements loose focus on hotkeys 4.0.2 _ March 1, 2019 \u00b6 Fixed #1012 : HTML character entities appear in search result titles 4.0.1 _ February 13, 2019 \u00b6 Fixed #762 , #816 : Glitch in sidebar when collapsing items Fixed #869 : Automatically expand details before printing 4.0.0 _ February 13, 2019 \u00b6 Added background on hover for table rows Removed Google Tag Manager and reverted to Google Analytics Removed blocks in partials - Jinja doesn't support them Fixed #911 : Chrome breaks layout if system language is Chinese [BREAKING] Fixed #976 : Removed FastClick 3.3.0 _ January 29, 2019 \u00b6 Moved Google Analytics integration into head using Google Tag Manager Fixed #972 : Unicode slugifier breaks table of contents blur on scroll Fixed #974 : Additional links in table of contents break blur on scroll 3.2.0 _ December 28, 2018 \u00b6 Added support for redirects using metadata refresh Fixed #921 : Load Google Analytics snippet asynchronously 3.1.0 _ November 17, 2018 \u00b6 Added support for Progressive Web App Manifest Fixed #915 : Search bug in Safari (upgraded Lunr.js) 3.0.6 _ October 26, 2018 \u00b6 Added Taiwanese translations Fixed #906 : JavaScript code blocks evaluated in search results 3.0.5 _ October 23, 2018 \u00b6 Added Croatian and Indonesian translations Fixed #899 : Skip-to-content link invalid from 2 nd level on Fixed #902 : Missing URL filter in footer for FontAwesome link 3.0.4 _ September 3, 2018 \u00b6 Updated Dutch translations Fixed #856 : Removed preconnect meta tag if Google Fonts are disabled 3.0.3 _ August 7, 2018 \u00b6 Fixed #841 : Additional path levels for extra CSS and JS 3.0.2 _ August 6, 2018 \u00b6 Fixed #839 : Lunr.js stemmer imports incorrect 3.0.1 _ August 5, 2018 \u00b6 Fixed #838 : Search result links incorrect 3.0.0 _ August 5, 2018 \u00b6 Upgraded MkDocs to 1.0 [BREAKING] Upgraded Python in official Docker image to 3.6 Added Serbian and Serbo-Croatian translations 2.9.4 _ July 29, 2018 \u00b6 Fixed build error after MkDocs upgrade 2.9.3 _ July 29, 2018 \u00b6 Added link to home for logo in drawer Fixed dependency problems between MkDocs and Tornado 2.9.2 _ June 29, 2018 \u00b6 Added Hindi and Czech translations 2.9.1 _ June 18, 2018 \u00b6 Added support for different spellings for theme color Fixed #799 : Added support for web font minification in production Fixed #800 : Added .highlighttable as an alias for .codehilitetable 2.9.0 _ June 13, 2018 \u00b6 Added support for theme color on Android Fixed #796 : Rendering of nested tabbed code blocks 2.8.0 _ June 10, 2018 \u00b6 Added support for grouping code blocks with tabs Added Material and FontAwesome icon fonts to distribution files (GDPR) Added note on compliance with GDPR Added Slovak translations Fixed #790 : Prefixed id attributes with __ to avoid name clashes 2.7.3 _ April 26, 2018 \u00b6 Added Finnish translations 2.7.2 _ April 9, 2018 \u00b6 Fixed rendering issue for details on Edge 2.7.1 _ March 21, 2018 \u00b6 Added Galician translations Fixed #730 : Scroll chasing error on home page if Disqus is enabled Fixed #736 : Reset drawer and search upon back button invocation 2.7.0 _ March 6, 2018 \u00b6 Added ability to set absolute URL for logo Added Hebrew translations 2.6.6 _ February 22, 2018 \u00b6 Added preconnect for Google Fonts for faster loading Fixed #710 : With tabs sidebar disappears if JavaScript is not available 2.6.5 _ February 22, 2018 \u00b6 Reverted --dev-addr flag removal from Dockerfile 2.6.4 _ February 21, 2018 \u00b6 Added Catalan translations Fixed incorrect margins for buttons in Firefox and Safari Replaced package manager yarn with npm 5.6 Reverted GitHub stars rounding method Removed --dev-addr flag from Dockerfile for Windows compatibility 2.6.3 _ February 18, 2018 \u00b6 Added Vietnamese translations 2.6.2 _ February 12, 2018 \u00b6 Added Arabic translations Fixed incorrect rounding of amount of GitHub stars Fixed double-layered borders for tables 2.6.1 _ February 11, 2018 \u00b6 Added ability to override Disqus integration using metadata Fixed #690 : Duplicate slashes in source file URLs Fixed #696 : Active page highlight not working with default palette Adjusted German translations 2.6.0 _ February 2, 2018 \u00b6 Moved default search configuration to default translation (English) Added support to automatically set text direction from translation Added support to disable search stop word filter in translation Added support to disable search trimmer in translation Added Persian translations Fixed support for Polish search Fixed disappearing GitHub, GitLab and Bitbucket repository icons 2.5.5 _ January 31, 2018 \u00b6 Added Hungarian translations 2.5.4 _ January 29, 2018 \u00b6 Fixed #683 : gh-deploy fails inside Docker 2.5.3 _ January 25, 2018 \u00b6 Added Ukrainian translations 2.5.2 _ January 22, 2018 \u00b6 Added default search language mappings for all localizations Fixed #673 : Error loading non-existent search language Fixed #675 : Uncaught reference error when search plugin disabled 2.5.1 _ January 20, 2018 \u00b6 Fixed permalink for main headline Improved missing translation handling with English as a fallback Improved accessibility with skip-to-content link 2.5.0 _ January 13, 2018 \u00b6 Added support for right-to-left languages 2.4.0 _ January 11, 2018 \u00b6 Added focus state for clipboard buttons Fixed #400 : Search bar steals tab focus Fixed search not closing on Enter when result is selected Fixed search not closing when losing focus due to Tab Fixed collapsed navigation links getting focus Fixed outline being cut off on Tab focus of navigation links Fixed bug with first search result navigation being ignored Removed search result navigation via Tab (use Up and Down ) Removed outline resets for links Improved general tabbing behavior on desktop 2.3.0 _ January 9, 2018 \u00b6 Added example (synonym: snippet ) style for Admonition Added synonym abstract for summary style for Admonition 2.2.6 _ December 27, 2017 \u00b6 Added Turkish translations Fixed unclickable area below header in case JavaScript is not available 2.2.5 _ December 18, 2017 \u00b6 Fixed #639 : Broken default favicon 2.2.4 _ December 18, 2017 \u00b6 Fixed #638 : Build breaks with Jinja < 2.9 2.2.3 _ December 13, 2017 \u00b6 Fixed #630 : Admonition sets padding on any last child Adjusted Chinese (Traditional) translations 2.2.2 _ December 8, 2017 \u00b6 Added Dutch translations Adjusted targeted link and footnote offsets Simplified Admonition styles and fixed padding bug 2.2.1 _ December 2, 2017 \u00b6 Fixed #616 : Minor styling error with title-only admonition blocks Removed border for table of contents and improved spacing 2.2.0 _ November 22, 2017 \u00b6 Added support for hero teaser Added Portuguese translations Fixed #586 : Footnote backref target offset regression Fixed #605 : Search stemmers not correctly loaded 2.1.1 _ November 21, 2017 \u00b6 Replaced deprecated babel-preset-es2015 with babel-preset-env Refactored Gulp build pipeline with Webpack Removed right border on sidebars Fixed broken color transition on header 2.1.0 _ November 19, 2017 \u00b6 Added support for white as a primary color Added support for sliding site name and title Fixed redundant clipboard button when using line numbers on code blocks Improved header appearance by making it taller Improved tabs appearance Improved CSS customizability by leveraging inheritance Removed scroll shadows via background-attachment 2.0.4 _ November 5, 2017 \u00b6 Fixed details not opening with footnote reference 2.0.3 _ November 5, 2017 \u00b6 Added Japanese translations Fixed #540 : Jumping to anchor inside details doesn't open it Fixed active link colors in footer 2.0.2 _ November 1, 2017 \u00b6 Added Russian translations Fixed #542 : Horizontal scrollbar between 1220px and 1234px Fixed #553 : Metadata values only rendering first character Fixed #558 : Flash of unstyled content Fixed favicon regression caused by deprecation upstream 2.0.1 _ October 31, 2017 \u00b6 Fixed error when initializing search Fixed styles for link to edit the current page Fixed styles on nested admonition in details 2.0.0 _ October 31, 2017 \u00b6 Added support for MkDocs 0.17.1 theme configuration options Added support for easier configuration of search tokenizer Added support to disable search Added Korean translations Removed support for MkDocs 0.16.x [BREAKING] 1.12.2 _ October 26, 2017 \u00b6 Added Italian, Norwegian, French and Chinese translations 1.12.1 _ October 22, 2017 \u00b6 Added Polish, Swedish and Spanish translations Improved downward compatibility with custom partials Temporarily pinned MkDocs version within Docker image to 0.16.3 Fixed #519 : Missing theme configuration file 1.12.0 _ October 20, 2017 \u00b6 Added support for setting language(s) via mkdocs.yml Added support for default localization Added German and Danish translations Fixed #374 : Search bar misalignment on big screens 1.11.0 _ October 19, 2017 \u00b6 Added localization to clipboard Refactored localization logic 1.10.4 _ October 18, 2017 \u00b6 Improved print styles of code blocks Improved search UX (don't close on enter if no selection) Fixed #495 : Vertical scrollbar on short pages 1.10.3 _ October 11, 2017 \u00b6 Fixed #484 : Vertical scrollbar on some MathJax formulas Fixed #483 : Footnote backref target offset regression 1.10.2 _ October 6, 2017 \u00b6 Fixed #468 : Sidebar shows scrollbar if content is shorter (in Safari) 1.10.1 _ September 14, 2017 \u00b6 Fixed #455 : Bold code blocks rendered with normal font weight 1.10.0 _ September 1, 2017 \u00b6 Added support to make logo default icon configurable Fixed uninitialized overflow scrolling on main pane for iOS Fixed error in mobile navigation in case JavaScript is not available Fixed incorrect color transition for nested panes in mobile navigation Improved checkbox styles for Tasklist from PyMdown Extension package 1.9.0 _ August 29, 2017 \u00b6 Added info (synonym: todo ) style for Admonition Added question (synonym: help , faq ) style for Admonition Added support for Details from PyMdown Extensions package Improved Admonition styles to match Details Improved styles for social links in footer Replaced ligatures with Unicode code points to avoid broken layout Upgraded PyMdown Extensions package dependency to >= 3.4 1.8.1 _ August 7, 2017 \u00b6 Fixed #421 : Missing pagination for GitHub API 1.8.0 _ August 2, 2017 \u00b6 Added support for lazy-loading of search results for better performance Added support for customization of search tokenizer/separator Fixed #424 : Search doesn't handle capital letters anymore Fixed #419 : Search doesn't work on whole words 1.7.5 _ July 25, 2017 \u00b6 Fixed #398 : Forms broken due to search shortcuts Improved search overall user experience Improved search matching and highlighting Improved search accessibility 1.7.4 _ June 21, 2017 \u00b6 Fixed functional link colors in table of contents for active palette Fixed #368 : Compatibility issues with IE11 1.7.3 _ June 7, 2017 \u00b6 Fixed error when setting language to Japanese for site search 1.7.2 _ June 6, 2017 \u00b6 Fixed offset of search box when repo_url is not set Fixed non-disappearing tooltip 1.7.1 _ June 1, 2017 \u00b6 Fixed wrong z-index order of header, overlay and drawer Fixed wrong offset of targeted footnote back references 1.7.0 _ June 1, 2017 \u00b6 Added \"copy to clipboard\" buttons to code blocks Added support for multilingual site search Fixed search term highlighting for non-latin languages 1.6.4 _ May 24, 2017 \u00b6 Fixed #337 : JavaScript error for GitHub organization URLs 1.6.3 _ May 16, 2017 \u00b6 Fixed #329 : Broken source stats for private or unknown GitHub repos 1.6.2 _ May 15, 2017 \u00b6 Fixed #316 : Fatal error for git clone on Windows Fixed #320 : Chrome 58 creates double underline for abbr tags Fixed #323 : Ligatures rendered inside code blocks Fixed miscalculated sidebar height due to missing margin collapse Changed deprecated MathJax CDN to Cloudflare 1.6.1 _ April 23, 2017 \u00b6 Fixed following of active/focused element if search input is focused Fixed layer order of search component elements 1.6.0 _ April 22, 2017 \u00b6 Added build test for Docker image on Travis Added search overlay for better user experience (focus) Added language from localizations to html tag Fixed #270 : source links broken for absolute URLs Fixed missing top spacing for first targeted element in content Fixed too small footnote divider when using larger font sizes 1.5.5 _ April 20, 2017 \u00b6 Fixed #282 : Browser search ( Meta + F ) is hijacked 1.5.4 _ April 8, 2017 \u00b6 Fixed broken highlighting for two or more search terms Fixed missing search results when only a h1 is present Fixed unresponsive overlay on Android 1.5.3 _ April 7, 2017 \u00b6 Fixed deprecated calls for template variables Fixed wrong palette color for focused search result Fixed JavaScript errors on 404 page Fixed missing top spacing on 404 page Fixed missing right spacing on overflow of source container 1.5.2 _ April 5, 2017 \u00b6 Added requirements as explicit dependencies in setup.py Fixed non-synchronized transitions in search form 1.5.1 _ March 30, 2017 \u00b6 Fixed rendering and offset of targetted footnotes Fixed #238 : Link on logo is not set to site_url 1.5.0 _ March 24, 2017 \u00b6 Added support for localization of search placeholder Added keyboard events for quick access of search Added keyboard events for search control Added opacity on hover for search buttons Added git hook to skip CI build on non-src changes Fixed non-resetting search placeholder when input is cleared Fixed error for unescaped parentheses in search term Fixed #229 : Button to clear search missing Fixed #231 : Escape key doesn't exit search Removed old-style figures from font feature settings 1.4.1 _ March 16, 2017 \u00b6 Fixed invalid destructuring attempt on NodeList (in Safari, Edge, IE) 1.4.0 _ March 16, 2017 \u00b6 Added support for grouping searched sections by documents Added support for highlighting of search terms Added support for localization of search results Fixed #216 : table of contents icon doesn't show if h1 is not present Reworked style and layout of search results for better usability 1.3.0 _ March 11, 2017 \u00b6 Added support for page-specific title and description using metadata Added support for linking source files to documentation Fixed jitter and offset of sidebar when zooming browser Fixed incorrectly initialized tablet sidebar height Fixed regression for #1 : GitHub stars break if repo_url ends with a / Fixed undesired white line below copyright footer due to base font scaling Fixed issue with whitespace in path for scripts Fixed #205 : support non-fixed (static) header Refactored footnote references for better visibility Reduced repaints to a minimum for non-tabs configuration Reduced contrast of edit button (slightly) 1.2.0 _ March 3, 2017 \u00b6 Added quote (synonym: cite ) style for Admonition Added help message to build pipeline Fixed wrong navigation link colors when applying palette Fixed #197 : Link missing in tabs navigation on deeply nested items Removed unnecessary dev dependencies 1.1.1 _ February 26, 2017 \u00b6 Fixed incorrectly displayed nested lists when using tabs 1.1.0 _ February 26, 2017 \u00b6 Added tabs navigation feature (optional) Added Disqus integration (optional) Added a high resolution Favicon with the new logo Added static type checking using Facebook's Flow Fixed #173 : Dictionary elements have no bottom spacing Fixed #175 : Tables cannot be set to 100% width Fixed race conditions in build related to asset revisioning Fixed accidentally re-introduced Permalink on top-level headline Fixed alignment of logo in drawer on IE11 Refactored styles related to tables Refactored and automated Docker build and PyPI release Refactored build scripts 1.0.5 _ February 18, 2017 \u00b6 Fixed #153 : Sidebar flows out of constrained area in Chrome 56 Fixed #159 : Footer jitter due to JavaScript if content is short 1.0.4 _ February 16, 2017 \u00b6 Fixed #142 : Documentation build errors if h1 is defined as raw HTML Fixed #164 : PyPI release does not build and install Fixed offsets of targeted headlines Increased sidebar font size by 0.12rem 1.0.3 _ January 22, 2017 \u00b6 Fixed #117 : Table of contents items don't blur on fast scrolling Refactored sidebar positioning logic Further reduction of repaints 1.0.2 _ January 15, 2017 \u00b6 Fixed #108 : Horizontal scrollbar in content area 1.0.1 _ January 14, 2017 \u00b6 Fixed massive repaints happening when scrolling Fixed footer back reference positions in case of overflow Fixed header logo from showing when the menu icon is rendered Changed scrollbar behavior to only show when content overflows 1.0.0 _ January 13, 2017 \u00b6 Introduced Webpack for more sophisticated JavaScript bundling Introduced ESLint and Stylelint for code style checks Introduced more accurate Material Design colors and shadows Introduced modular scales for harmonic font sizing Introduced git-hooks for better development workflow Rewrite of CSS using the BEM methodology and SassDoc guidelines Rewrite of JavaScript using ES6 and Babel as a transpiler Rewrite of Admonition, Permalinks and CodeHilite integration Rewrite of the complete typographical system Rewrite of Gulp asset pipeline in ES6 and separation of tasks Removed Bower as a dependency in favor of NPM Removed custom icon build in favor of the Material Design iconset Removed _blank targets on links due to vulnerability: http://bit.ly/1Mk2Rtw Removed unversioned assets from build directory Restructured templates into base templates and partials Added build and watch scripts in package.json Added support for Metadata and Footnotes Markdown extensions Added support for PyMdown Extensions package Added support for collapsible sections in navigation Added support for separate table of contents Added support for better accessibility through REM-based layout Added icons for GitHub, GitLab and BitBucket integrations Added more detailed documentation on specimen, extensions etc. Added a 404.html error page for deployment on GitHub Pages Fixed live reload chain in watch mode when saving a template Fixed variable references to work with MkDocs 0.16 0.2.4 _ June 26, 2016 \u00b6 Fixed improperly set default favicon Fixed #33 : Protocol relative URL for webfonts doesn't work with file:// Fixed #34 : IE11 on Windows 7 doesn't honor max-width on main tag Fixed #35 : Add styling for blockquotes 0.2.3 _ May 16, 2016 \u00b6 Fixed #25 : Highlight inline fenced blocks Fixed #26 : Better highlighting for keystrokes Fixed #30 : Suboptimal syntax highlighting for PHP 0.2.2 _ March 20, 2016 \u00b6 Fixed #15 : Document Pygments dependency for CodeHilite Fixed #16 : Favicon could not be set through mkdocs.yml Fixed #17 : Put version into own container for styling Fixed #20 : Fix rounded borders for tables 0.2.1 _ March 12, 2016 \u00b6 Fixed #10 : Invisible header after closing search bar with ESC key Fixed #13 : Table cells don't wrap Fixed empty list in table of contents when no headline is defined Corrected wrong path for static asset monitoring in Gulpfile.js Set up tracking of site search for Google Analytics 0.2.0 _ February 24, 2016 \u00b6 Fixed #6 : Include multiple color palettes via mkdocs.yml Fixed #7 : Better colors for links inside admonition notes and warnings Fixed #9 : Text for prev/next footer navigation should be customizable Refactored templates (replaced if / else with modifiers where possible) 0.1.3 _ February 21, 2016 \u00b6 Fixed #3 : Ordered lists within an unordered list have ::before content Fixed #4 : Click on Logo/Title without Github-Repository: \"None\" Fixed #5 : Page without headlines renders empty list in table of contents Moved Modernizr to top to ensure basic usability in IE8 0.1.2 _ February 16, 2016 \u00b6 Fixed styles for deep navigational hierarchies Fixed webfont delivery problem when hosted in subdirectories Fixed print styles in mobile/tablet configuration Added option to configure fonts in mkdocs.yml with fallbacks Changed styles for admonition notes and warnings Set download link to latest version if available Set up tracking of outgoing links and actions for Google Analytics 0.1.1 _ February 11, 2016 \u00b6 Fixed #1 : GitHub stars don't work if the repo_url ends with a / Updated NPM and Bower dependencies to most recent versions Changed footer/copyright link to Material theme to GitHub pages Made MkDocs building/serving in build process optional Set up continuous integration with Travis 0.1.0 _ February 9, 2016 \u00b6 Initial release","title":"Release notes"},{"location":"release-notes/#release-notes","text":"","title":"Release notes"},{"location":"release-notes/#upgrading","text":"To upgrade Material to the latest version, use pip : pip install --upgrade mkdocs-material To inspect the currently installed version, use the following command: pip show mkdocs-material","title":"Upgrading"},{"location":"release-notes/#material-3x-to-4x","text":"Material for MkDocs 4.x finally fixes incorrect layout on Chinese systems. The fix includes a mandatory change of the base font-size from 10px to 20px which means all rem values needed to be updated. Within the theme, px to rem calculation is now encapsulated in a new function called px2rem which is part of the SASS code base. If you use Material with custom CSS that is based on rem values, note that those values must now be divided by 2. Now, 1.0rem doesn't map to 10px , but 20px . To learn more about the problem and implications, please refer to the issue in which the problem was discovered and fixed.","title":"Material 3.x to 4.x"},{"location":"release-notes/#material-2x-to-3x","text":"Material for MkDocs 3.x requires MkDocs 1.0 because the way paths are resolved internally changed significantly. Furthermore, pages was renamed to nav , so remember to adjust your mkdocs.yml file. All extended templates should continue to work but in order to make them future-proof the url filter should be introduced on all paths. Please see the official release notes for further guidance.","title":"Material 2.x to 3.x"},{"location":"release-notes/#material-1x-to-2x","text":"Material for MkDocs 2.x requires MkDocs 0.17.1, as this version introduced changes to the way themes can define options. The following variables inside your project's mkdocs.yml need to be renamed: extra.feature becomes theme.feature extra.palette becomes theme.palette extra.font becomes theme.font extra.logo becomes theme.logo Favicon support has been dropped by MkDocs, it must now be defined in theme.favicon (previously site_favicon ). Localization is now separated into theme language and search language. While there can only be a single language on theme-level, the search supports multiple languages which can be separated by commas. See the getting started guide for more guidance. The search tokenizer can now be set through extra.search.tokenizer .","title":"Material 1.x to 2.x"},{"location":"release-notes/#changelog","text":"","title":"Changelog"},{"location":"release-notes/#440-_-june-15-2019","text":"Added Slovenian translations Reverted template minification in favor of mkdocs-minify-plugin Fixed #1114 : Tabs don't reappear when default font-size is smaller than 16","title":"4.4.0 _ June 15, 2019"},{"location":"release-notes/#431-_-may-23-2019","text":"Fixed spelling error in Danish translations","title":"4.3.1 _ May 23, 2019"},{"location":"release-notes/#430-_-may-17-2019","text":"Added support for changing header through metadata title property Added font-display: swap to Google Font loading logic Removed whitespace from templates, saving 4kb ( .7kb gzipped) per request Fixed alignment of repository icons on tablet and desktop","title":"4.3.0 _ May 17, 2019"},{"location":"release-notes/#420-_-april-28-2019","text":"Added Norwegian (Nynorsk) translations Fixed loss of focus in non-form input elements due to search hotkeys Fixed #1067 : Search hotkeys not working for mobile/tablet screensize Fixed #1068 : Search not correctly aligned for tablet screensize","title":"4.2.0 _ April 28, 2019"},{"location":"release-notes/#412-_-april-16-2019","text":"Fixed #1072 : HTML tags appearing in navigation link titles","title":"4.1.2 _ April 16, 2019"},{"location":"release-notes/#411-_-march-28-2019","text":"Fixed minor CSS errors detected during validation","title":"4.1.1 _ March 28, 2019"},{"location":"release-notes/#410-_-march-22-2019","text":"Fixed #1023 : Search for Asian languages broken after Lunr.js update Fixed #1026 : contenteditable elements loose focus on hotkeys","title":"4.1.0 _ March 22, 2019"},{"location":"release-notes/#402-_-march-1-2019","text":"Fixed #1012 : HTML character entities appear in search result titles","title":"4.0.2 _ March 1, 2019"},{"location":"release-notes/#401-_-february-13-2019","text":"Fixed #762 , #816 : Glitch in sidebar when collapsing items Fixed #869 : Automatically expand details before printing","title":"4.0.1 _ February 13, 2019"},{"location":"release-notes/#400-_-february-13-2019","text":"Added background on hover for table rows Removed Google Tag Manager and reverted to Google Analytics Removed blocks in partials - Jinja doesn't support them Fixed #911 : Chrome breaks layout if system language is Chinese [BREAKING] Fixed #976 : Removed FastClick","title":"4.0.0 _ February 13, 2019"},{"location":"release-notes/#330-_-january-29-2019","text":"Moved Google Analytics integration into head using Google Tag Manager Fixed #972 : Unicode slugifier breaks table of contents blur on scroll Fixed #974 : Additional links in table of contents break blur on scroll","title":"3.3.0 _ January 29, 2019"},{"location":"release-notes/#320-_-december-28-2018","text":"Added support for redirects using metadata refresh Fixed #921 : Load Google Analytics snippet asynchronously","title":"3.2.0 _ December 28, 2018"},{"location":"release-notes/#310-_-november-17-2018","text":"Added support for Progressive Web App Manifest Fixed #915 : Search bug in Safari (upgraded Lunr.js)","title":"3.1.0 _ November 17, 2018"},{"location":"release-notes/#306-_-october-26-2018","text":"Added Taiwanese translations Fixed #906 : JavaScript code blocks evaluated in search results","title":"3.0.6 _ October 26, 2018"},{"location":"release-notes/#305-_-october-23-2018","text":"Added Croatian and Indonesian translations Fixed #899 : Skip-to-content link invalid from 2 nd level on Fixed #902 : Missing URL filter in footer for FontAwesome link","title":"3.0.5 _ October 23, 2018"},{"location":"release-notes/#304-_-september-3-2018","text":"Updated Dutch translations Fixed #856 : Removed preconnect meta tag if Google Fonts are disabled","title":"3.0.4 _ September 3, 2018"},{"location":"release-notes/#303-_-august-7-2018","text":"Fixed #841 : Additional path levels for extra CSS and JS","title":"3.0.3 _ August 7, 2018"},{"location":"release-notes/#302-_-august-6-2018","text":"Fixed #839 : Lunr.js stemmer imports incorrect","title":"3.0.2 _ August 6, 2018"},{"location":"release-notes/#301-_-august-5-2018","text":"Fixed #838 : Search result links incorrect","title":"3.0.1 _ August 5, 2018"},{"location":"release-notes/#300-_-august-5-2018","text":"Upgraded MkDocs to 1.0 [BREAKING] Upgraded Python in official Docker image to 3.6 Added Serbian and Serbo-Croatian translations","title":"3.0.0 _ August 5, 2018"},{"location":"release-notes/#294-_-july-29-2018","text":"Fixed build error after MkDocs upgrade","title":"2.9.4 _ July 29, 2018"},{"location":"release-notes/#293-_-july-29-2018","text":"Added link to home for logo in drawer Fixed dependency problems between MkDocs and Tornado","title":"2.9.3 _ July 29, 2018"},{"location":"release-notes/#292-_-june-29-2018","text":"Added Hindi and Czech translations","title":"2.9.2 _ June 29, 2018"},{"location":"release-notes/#291-_-june-18-2018","text":"Added support for different spellings for theme color Fixed #799 : Added support for web font minification in production Fixed #800 : Added .highlighttable as an alias for .codehilitetable","title":"2.9.1 _ June 18, 2018"},{"location":"release-notes/#290-_-june-13-2018","text":"Added support for theme color on Android Fixed #796 : Rendering of nested tabbed code blocks","title":"2.9.0 _ June 13, 2018"},{"location":"release-notes/#280-_-june-10-2018","text":"Added support for grouping code blocks with tabs Added Material and FontAwesome icon fonts to distribution files (GDPR) Added note on compliance with GDPR Added Slovak translations Fixed #790 : Prefixed id attributes with __ to avoid name clashes","title":"2.8.0 _ June 10, 2018"},{"location":"release-notes/#273-_-april-26-2018","text":"Added Finnish translations","title":"2.7.3 _ April 26, 2018"},{"location":"release-notes/#272-_-april-9-2018","text":"Fixed rendering issue for details on Edge","title":"2.7.2 _ April 9, 2018"},{"location":"release-notes/#271-_-march-21-2018","text":"Added Galician translations Fixed #730 : Scroll chasing error on home page if Disqus is enabled Fixed #736 : Reset drawer and search upon back button invocation","title":"2.7.1 _ March 21, 2018"},{"location":"release-notes/#270-_-march-6-2018","text":"Added ability to set absolute URL for logo Added Hebrew translations","title":"2.7.0 _ March 6, 2018"},{"location":"release-notes/#266-_-february-22-2018","text":"Added preconnect for Google Fonts for faster loading Fixed #710 : With tabs sidebar disappears if JavaScript is not available","title":"2.6.6 _ February 22, 2018"},{"location":"release-notes/#265-_-february-22-2018","text":"Reverted --dev-addr flag removal from Dockerfile","title":"2.6.5 _ February 22, 2018"},{"location":"release-notes/#264-_-february-21-2018","text":"Added Catalan translations Fixed incorrect margins for buttons in Firefox and Safari Replaced package manager yarn with npm 5.6 Reverted GitHub stars rounding method Removed --dev-addr flag from Dockerfile for Windows compatibility","title":"2.6.4 _ February 21, 2018"},{"location":"release-notes/#263-_-february-18-2018","text":"Added Vietnamese translations","title":"2.6.3 _ February 18, 2018"},{"location":"release-notes/#262-_-february-12-2018","text":"Added Arabic translations Fixed incorrect rounding of amount of GitHub stars Fixed double-layered borders for tables","title":"2.6.2 _ February 12, 2018"},{"location":"release-notes/#261-_-february-11-2018","text":"Added ability to override Disqus integration using metadata Fixed #690 : Duplicate slashes in source file URLs Fixed #696 : Active page highlight not working with default palette Adjusted German translations","title":"2.6.1 _ February 11, 2018"},{"location":"release-notes/#260-_-february-2-2018","text":"Moved default search configuration to default translation (English) Added support to automatically set text direction from translation Added support to disable search stop word filter in translation Added support to disable search trimmer in translation Added Persian translations Fixed support for Polish search Fixed disappearing GitHub, GitLab and Bitbucket repository icons","title":"2.6.0 _ February 2, 2018"},{"location":"release-notes/#255-_-january-31-2018","text":"Added Hungarian translations","title":"2.5.5 _ January 31, 2018"},{"location":"release-notes/#254-_-january-29-2018","text":"Fixed #683 : gh-deploy fails inside Docker","title":"2.5.4 _ January 29, 2018"},{"location":"release-notes/#253-_-january-25-2018","text":"Added Ukrainian translations","title":"2.5.3 _ January 25, 2018"},{"location":"release-notes/#252-_-january-22-2018","text":"Added default search language mappings for all localizations Fixed #673 : Error loading non-existent search language Fixed #675 : Uncaught reference error when search plugin disabled","title":"2.5.2 _ January 22, 2018"},{"location":"release-notes/#251-_-january-20-2018","text":"Fixed permalink for main headline Improved missing translation handling with English as a fallback Improved accessibility with skip-to-content link","title":"2.5.1 _ January 20, 2018"},{"location":"release-notes/#250-_-january-13-2018","text":"Added support for right-to-left languages","title":"2.5.0 _ January 13, 2018"},{"location":"release-notes/#240-_-january-11-2018","text":"Added focus state for clipboard buttons Fixed #400 : Search bar steals tab focus Fixed search not closing on Enter when result is selected Fixed search not closing when losing focus due to Tab Fixed collapsed navigation links getting focus Fixed outline being cut off on Tab focus of navigation links Fixed bug with first search result navigation being ignored Removed search result navigation via Tab (use Up and Down ) Removed outline resets for links Improved general tabbing behavior on desktop","title":"2.4.0 _ January 11, 2018"},{"location":"release-notes/#230-_-january-9-2018","text":"Added example (synonym: snippet ) style for Admonition Added synonym abstract for summary style for Admonition","title":"2.3.0 _ January 9, 2018"},{"location":"release-notes/#226-_-december-27-2017","text":"Added Turkish translations Fixed unclickable area below header in case JavaScript is not available","title":"2.2.6 _ December 27, 2017"},{"location":"release-notes/#225-_-december-18-2017","text":"Fixed #639 : Broken default favicon","title":"2.2.5 _ December 18, 2017"},{"location":"release-notes/#224-_-december-18-2017","text":"Fixed #638 : Build breaks with Jinja < 2.9","title":"2.2.4 _ December 18, 2017"},{"location":"release-notes/#223-_-december-13-2017","text":"Fixed #630 : Admonition sets padding on any last child Adjusted Chinese (Traditional) translations","title":"2.2.3 _ December 13, 2017"},{"location":"release-notes/#222-_-december-8-2017","text":"Added Dutch translations Adjusted targeted link and footnote offsets Simplified Admonition styles and fixed padding bug","title":"2.2.2 _ December 8, 2017"},{"location":"release-notes/#221-_-december-2-2017","text":"Fixed #616 : Minor styling error with title-only admonition blocks Removed border for table of contents and improved spacing","title":"2.2.1 _ December 2, 2017"},{"location":"release-notes/#220-_-november-22-2017","text":"Added support for hero teaser Added Portuguese translations Fixed #586 : Footnote backref target offset regression Fixed #605 : Search stemmers not correctly loaded","title":"2.2.0 _ November 22, 2017"},{"location":"release-notes/#211-_-november-21-2017","text":"Replaced deprecated babel-preset-es2015 with babel-preset-env Refactored Gulp build pipeline with Webpack Removed right border on sidebars Fixed broken color transition on header","title":"2.1.1 _ November 21, 2017"},{"location":"release-notes/#210-_-november-19-2017","text":"Added support for white as a primary color Added support for sliding site name and title Fixed redundant clipboard button when using line numbers on code blocks Improved header appearance by making it taller Improved tabs appearance Improved CSS customizability by leveraging inheritance Removed scroll shadows via background-attachment","title":"2.1.0 _ November 19, 2017"},{"location":"release-notes/#204-_-november-5-2017","text":"Fixed details not opening with footnote reference","title":"2.0.4 _ November 5, 2017"},{"location":"release-notes/#203-_-november-5-2017","text":"Added Japanese translations Fixed #540 : Jumping to anchor inside details doesn't open it Fixed active link colors in footer","title":"2.0.3 _ November 5, 2017"},{"location":"release-notes/#202-_-november-1-2017","text":"Added Russian translations Fixed #542 : Horizontal scrollbar between 1220px and 1234px Fixed #553 : Metadata values only rendering first character Fixed #558 : Flash of unstyled content Fixed favicon regression caused by deprecation upstream","title":"2.0.2 _ November 1, 2017"},{"location":"release-notes/#201-_-october-31-2017","text":"Fixed error when initializing search Fixed styles for link to edit the current page Fixed styles on nested admonition in details","title":"2.0.1 _ October 31, 2017"},{"location":"release-notes/#200-_-october-31-2017","text":"Added support for MkDocs 0.17.1 theme configuration options Added support for easier configuration of search tokenizer Added support to disable search Added Korean translations Removed support for MkDocs 0.16.x [BREAKING]","title":"2.0.0 _ October 31, 2017"},{"location":"release-notes/#1122-_-october-26-2017","text":"Added Italian, Norwegian, French and Chinese translations","title":"1.12.2 _ October 26, 2017"},{"location":"release-notes/#1121-_-october-22-2017","text":"Added Polish, Swedish and Spanish translations Improved downward compatibility with custom partials Temporarily pinned MkDocs version within Docker image to 0.16.3 Fixed #519 : Missing theme configuration file","title":"1.12.1 _ October 22, 2017"},{"location":"release-notes/#1120-_-october-20-2017","text":"Added support for setting language(s) via mkdocs.yml Added support for default localization Added German and Danish translations Fixed #374 : Search bar misalignment on big screens","title":"1.12.0 _ October 20, 2017"},{"location":"release-notes/#1110-_-october-19-2017","text":"Added localization to clipboard Refactored localization logic","title":"1.11.0 _ October 19, 2017"},{"location":"release-notes/#1104-_-october-18-2017","text":"Improved print styles of code blocks Improved search UX (don't close on enter if no selection) Fixed #495 : Vertical scrollbar on short pages","title":"1.10.4 _ October 18, 2017"},{"location":"release-notes/#1103-_-october-11-2017","text":"Fixed #484 : Vertical scrollbar on some MathJax formulas Fixed #483 : Footnote backref target offset regression","title":"1.10.3 _ October 11, 2017"},{"location":"release-notes/#1102-_-october-6-2017","text":"Fixed #468 : Sidebar shows scrollbar if content is shorter (in Safari)","title":"1.10.2 _ October 6, 2017"},{"location":"release-notes/#1101-_-september-14-2017","text":"Fixed #455 : Bold code blocks rendered with normal font weight","title":"1.10.1 _ September 14, 2017"},{"location":"release-notes/#1100-_-september-1-2017","text":"Added support to make logo default icon configurable Fixed uninitialized overflow scrolling on main pane for iOS Fixed error in mobile navigation in case JavaScript is not available Fixed incorrect color transition for nested panes in mobile navigation Improved checkbox styles for Tasklist from PyMdown Extension package","title":"1.10.0 _ September 1, 2017"},{"location":"release-notes/#190-_-august-29-2017","text":"Added info (synonym: todo ) style for Admonition Added question (synonym: help , faq ) style for Admonition Added support for Details from PyMdown Extensions package Improved Admonition styles to match Details Improved styles for social links in footer Replaced ligatures with Unicode code points to avoid broken layout Upgraded PyMdown Extensions package dependency to >= 3.4","title":"1.9.0 _ August 29, 2017"},{"location":"release-notes/#181-_-august-7-2017","text":"Fixed #421 : Missing pagination for GitHub API","title":"1.8.1 _ August 7, 2017"},{"location":"release-notes/#180-_-august-2-2017","text":"Added support for lazy-loading of search results for better performance Added support for customization of search tokenizer/separator Fixed #424 : Search doesn't handle capital letters anymore Fixed #419 : Search doesn't work on whole words","title":"1.8.0 _ August 2, 2017"},{"location":"release-notes/#175-_-july-25-2017","text":"Fixed #398 : Forms broken due to search shortcuts Improved search overall user experience Improved search matching and highlighting Improved search accessibility","title":"1.7.5 _ July 25, 2017"},{"location":"release-notes/#174-_-june-21-2017","text":"Fixed functional link colors in table of contents for active palette Fixed #368 : Compatibility issues with IE11","title":"1.7.4 _ June 21, 2017"},{"location":"release-notes/#173-_-june-7-2017","text":"Fixed error when setting language to Japanese for site search","title":"1.7.3 _ June 7, 2017"},{"location":"release-notes/#172-_-june-6-2017","text":"Fixed offset of search box when repo_url is not set Fixed non-disappearing tooltip","title":"1.7.2 _ June 6, 2017"},{"location":"release-notes/#171-_-june-1-2017","text":"Fixed wrong z-index order of header, overlay and drawer Fixed wrong offset of targeted footnote back references","title":"1.7.1 _ June 1, 2017"},{"location":"release-notes/#170-_-june-1-2017","text":"Added \"copy to clipboard\" buttons to code blocks Added support for multilingual site search Fixed search term highlighting for non-latin languages","title":"1.7.0 _ June 1, 2017"},{"location":"release-notes/#164-_-may-24-2017","text":"Fixed #337 : JavaScript error for GitHub organization URLs","title":"1.6.4 _ May 24, 2017"},{"location":"release-notes/#163-_-may-16-2017","text":"Fixed #329 : Broken source stats for private or unknown GitHub repos","title":"1.6.3 _ May 16, 2017"},{"location":"release-notes/#162-_-may-15-2017","text":"Fixed #316 : Fatal error for git clone on Windows Fixed #320 : Chrome 58 creates double underline for abbr tags Fixed #323 : Ligatures rendered inside code blocks Fixed miscalculated sidebar height due to missing margin collapse Changed deprecated MathJax CDN to Cloudflare","title":"1.6.2 _ May 15, 2017"},{"location":"release-notes/#161-_-april-23-2017","text":"Fixed following of active/focused element if search input is focused Fixed layer order of search component elements","title":"1.6.1 _ April 23, 2017"},{"location":"release-notes/#160-_-april-22-2017","text":"Added build test for Docker image on Travis Added search overlay for better user experience (focus) Added language from localizations to html tag Fixed #270 : source links broken for absolute URLs Fixed missing top spacing for first targeted element in content Fixed too small footnote divider when using larger font sizes","title":"1.6.0 _ April 22, 2017"},{"location":"release-notes/#155-_-april-20-2017","text":"Fixed #282 : Browser search ( Meta + F ) is hijacked","title":"1.5.5 _ April 20, 2017"},{"location":"release-notes/#154-_-april-8-2017","text":"Fixed broken highlighting for two or more search terms Fixed missing search results when only a h1 is present Fixed unresponsive overlay on Android","title":"1.5.4 _ April 8, 2017"},{"location":"release-notes/#153-_-april-7-2017","text":"Fixed deprecated calls for template variables Fixed wrong palette color for focused search result Fixed JavaScript errors on 404 page Fixed missing top spacing on 404 page Fixed missing right spacing on overflow of source container","title":"1.5.3 _ April 7, 2017"},{"location":"release-notes/#152-_-april-5-2017","text":"Added requirements as explicit dependencies in setup.py Fixed non-synchronized transitions in search form","title":"1.5.2 _ April 5, 2017"},{"location":"release-notes/#151-_-march-30-2017","text":"Fixed rendering and offset of targetted footnotes Fixed #238 : Link on logo is not set to site_url","title":"1.5.1 _ March 30, 2017"},{"location":"release-notes/#150-_-march-24-2017","text":"Added support for localization of search placeholder Added keyboard events for quick access of search Added keyboard events for search control Added opacity on hover for search buttons Added git hook to skip CI build on non-src changes Fixed non-resetting search placeholder when input is cleared Fixed error for unescaped parentheses in search term Fixed #229 : Button to clear search missing Fixed #231 : Escape key doesn't exit search Removed old-style figures from font feature settings","title":"1.5.0 _ March 24, 2017"},{"location":"release-notes/#141-_-march-16-2017","text":"Fixed invalid destructuring attempt on NodeList (in Safari, Edge, IE)","title":"1.4.1 _ March 16, 2017"},{"location":"release-notes/#140-_-march-16-2017","text":"Added support for grouping searched sections by documents Added support for highlighting of search terms Added support for localization of search results Fixed #216 : table of contents icon doesn't show if h1 is not present Reworked style and layout of search results for better usability","title":"1.4.0 _ March 16, 2017"},{"location":"release-notes/#130-_-march-11-2017","text":"Added support for page-specific title and description using metadata Added support for linking source files to documentation Fixed jitter and offset of sidebar when zooming browser Fixed incorrectly initialized tablet sidebar height Fixed regression for #1 : GitHub stars break if repo_url ends with a / Fixed undesired white line below copyright footer due to base font scaling Fixed issue with whitespace in path for scripts Fixed #205 : support non-fixed (static) header Refactored footnote references for better visibility Reduced repaints to a minimum for non-tabs configuration Reduced contrast of edit button (slightly)","title":"1.3.0 _ March 11, 2017"},{"location":"release-notes/#120-_-march-3-2017","text":"Added quote (synonym: cite ) style for Admonition Added help message to build pipeline Fixed wrong navigation link colors when applying palette Fixed #197 : Link missing in tabs navigation on deeply nested items Removed unnecessary dev dependencies","title":"1.2.0 _ March 3, 2017"},{"location":"release-notes/#111-_-february-26-2017","text":"Fixed incorrectly displayed nested lists when using tabs","title":"1.1.1 _ February 26, 2017"},{"location":"release-notes/#110-_-february-26-2017","text":"Added tabs navigation feature (optional) Added Disqus integration (optional) Added a high resolution Favicon with the new logo Added static type checking using Facebook's Flow Fixed #173 : Dictionary elements have no bottom spacing Fixed #175 : Tables cannot be set to 100% width Fixed race conditions in build related to asset revisioning Fixed accidentally re-introduced Permalink on top-level headline Fixed alignment of logo in drawer on IE11 Refactored styles related to tables Refactored and automated Docker build and PyPI release Refactored build scripts","title":"1.1.0 _ February 26, 2017"},{"location":"release-notes/#105-_-february-18-2017","text":"Fixed #153 : Sidebar flows out of constrained area in Chrome 56 Fixed #159 : Footer jitter due to JavaScript if content is short","title":"1.0.5 _ February 18, 2017"},{"location":"release-notes/#104-_-february-16-2017","text":"Fixed #142 : Documentation build errors if h1 is defined as raw HTML Fixed #164 : PyPI release does not build and install Fixed offsets of targeted headlines Increased sidebar font size by 0.12rem","title":"1.0.4 _ February 16, 2017"},{"location":"release-notes/#103-_-january-22-2017","text":"Fixed #117 : Table of contents items don't blur on fast scrolling Refactored sidebar positioning logic Further reduction of repaints","title":"1.0.3 _ January 22, 2017"},{"location":"release-notes/#102-_-january-15-2017","text":"Fixed #108 : Horizontal scrollbar in content area","title":"1.0.2 _ January 15, 2017"},{"location":"release-notes/#101-_-january-14-2017","text":"Fixed massive repaints happening when scrolling Fixed footer back reference positions in case of overflow Fixed header logo from showing when the menu icon is rendered Changed scrollbar behavior to only show when content overflows","title":"1.0.1 _ January 14, 2017"},{"location":"release-notes/#100-_-january-13-2017","text":"Introduced Webpack for more sophisticated JavaScript bundling Introduced ESLint and Stylelint for code style checks Introduced more accurate Material Design colors and shadows Introduced modular scales for harmonic font sizing Introduced git-hooks for better development workflow Rewrite of CSS using the BEM methodology and SassDoc guidelines Rewrite of JavaScript using ES6 and Babel as a transpiler Rewrite of Admonition, Permalinks and CodeHilite integration Rewrite of the complete typographical system Rewrite of Gulp asset pipeline in ES6 and separation of tasks Removed Bower as a dependency in favor of NPM Removed custom icon build in favor of the Material Design iconset Removed _blank targets on links due to vulnerability: http://bit.ly/1Mk2Rtw Removed unversioned assets from build directory Restructured templates into base templates and partials Added build and watch scripts in package.json Added support for Metadata and Footnotes Markdown extensions Added support for PyMdown Extensions package Added support for collapsible sections in navigation Added support for separate table of contents Added support for better accessibility through REM-based layout Added icons for GitHub, GitLab and BitBucket integrations Added more detailed documentation on specimen, extensions etc. Added a 404.html error page for deployment on GitHub Pages Fixed live reload chain in watch mode when saving a template Fixed variable references to work with MkDocs 0.16","title":"1.0.0 _ January 13, 2017"},{"location":"release-notes/#024-_-june-26-2016","text":"Fixed improperly set default favicon Fixed #33 : Protocol relative URL for webfonts doesn't work with file:// Fixed #34 : IE11 on Windows 7 doesn't honor max-width on main tag Fixed #35 : Add styling for blockquotes","title":"0.2.4 _ June 26, 2016"},{"location":"release-notes/#023-_-may-16-2016","text":"Fixed #25 : Highlight inline fenced blocks Fixed #26 : Better highlighting for keystrokes Fixed #30 : Suboptimal syntax highlighting for PHP","title":"0.2.3 _ May 16, 2016"},{"location":"release-notes/#022-_-march-20-2016","text":"Fixed #15 : Document Pygments dependency for CodeHilite Fixed #16 : Favicon could not be set through mkdocs.yml Fixed #17 : Put version into own container for styling Fixed #20 : Fix rounded borders for tables","title":"0.2.2 _ March 20, 2016"},{"location":"release-notes/#021-_-march-12-2016","text":"Fixed #10 : Invisible header after closing search bar with ESC key Fixed #13 : Table cells don't wrap Fixed empty list in table of contents when no headline is defined Corrected wrong path for static asset monitoring in Gulpfile.js Set up tracking of site search for Google Analytics","title":"0.2.1 _ March 12, 2016"},{"location":"release-notes/#020-_-february-24-2016","text":"Fixed #6 : Include multiple color palettes via mkdocs.yml Fixed #7 : Better colors for links inside admonition notes and warnings Fixed #9 : Text for prev/next footer navigation should be customizable Refactored templates (replaced if / else with modifiers where possible)","title":"0.2.0 _ February 24, 2016"},{"location":"release-notes/#013-_-february-21-2016","text":"Fixed #3 : Ordered lists within an unordered list have ::before content Fixed #4 : Click on Logo/Title without Github-Repository: \"None\" Fixed #5 : Page without headlines renders empty list in table of contents Moved Modernizr to top to ensure basic usability in IE8","title":"0.1.3 _ February 21, 2016"},{"location":"release-notes/#012-_-february-16-2016","text":"Fixed styles for deep navigational hierarchies Fixed webfont delivery problem when hosted in subdirectories Fixed print styles in mobile/tablet configuration Added option to configure fonts in mkdocs.yml with fallbacks Changed styles for admonition notes and warnings Set download link to latest version if available Set up tracking of outgoing links and actions for Google Analytics","title":"0.1.2 _ February 16, 2016"},{"location":"release-notes/#011-_-february-11-2016","text":"Fixed #1 : GitHub stars don't work if the repo_url ends with a / Updated NPM and Bower dependencies to most recent versions Changed footer/copyright link to Material theme to GitHub pages Made MkDocs building/serving in build process optional Set up continuous integration with Travis","title":"0.1.1 _ February 11, 2016"},{"location":"release-notes/#010-_-february-9-2016","text":"Initial release","title":"0.1.0 _ February 9, 2016"},{"location":"scrapfirst/","text":"Membangun Scraping Pertama kali. \u00b6 Untuk melakukan tahapan penarikan data dari suatu halaman halaman web maka yang perlu dipehatikan adalah **Menganalisa ** strutktur HTML dari laman web Scraping adalah tentang menemukan pola dalam laman- laman web dan mengektraksi isi dari laman tersebut. Sebelum mulai untuk menulis tool scrapyng (scraper), kita perlu untuk memahami struktur HTML dari web page yang akan ditarik datanya dan mengidentifikasi pola didalamnya. Pola dapat terkait dengan penggunaan classes, id, and elemen elemen lain HTML. **Membuat Scrapy parser dengan ** Setelah menganlisa struktur dari halaman web yang akan ditarik datanya, kita lakukan implementasi dengan menulis code untuk menghasilkan scrapy parser . Scrapy parser bertanggung jawab untuk menjelajahi web yang akan ditarik datanya dan mengektrak informasi sesuai dengan aturan yang dibuat Mengumpulkan dan menyimpan informasi Parser dapat menyimpan hasilnya sesuai dengan format yang anda inginkan misalkan csv atau json. Ini adalah hasil akhir dari data yang telah dikumpulkan 1.Analisa Struktur HTML Halaman Website \u00b6 XPath (kepanjangan dari XML Path Language) adalah expression language yang digunakan untuk menetapkanbagian bagian bagian bagian dari dokumen XML . XPath digunakan dalam perangkat lunak dan bahasa yang digunakan untuk manipulasi dokumen XML, seperti XSLT, XQuery atau alat alat web scraping XPath dapat juga digunakan dalam struktur yang sama seperti XML, misalkan html Markup Languages \u00b6 XML dan HTML adalah markup languages . Artinya keduanya menggunakan sekumpulan tags atau rule untuk menyusun dan menyedian informasi yang ada didalamnya. Struktur ini membantu untuk secara otomatis untuk pemrosesan , pengeditan, pemformatan dan penampilan dan pencetakan informasi tersebut. Dokumen XML menyimpan data dalam format plain teks. Ini menyediakan perangkat lunan dan perangkat kerans dengan cara bebas untuk menyimpan, mentransformasi, dan menshare data. Format XML adalah format terbuka. Anda dapat membuka dokumen XML dalam editor teks dan data yang ada didalamnya dapat disajikan. Ini memungkinkan pertukran sistem yang tidak kompatibel dan mudah untuk mengkonversi suatu data. Dokumen XML memilki aturan-aturan dasar sebagai berikut: Dokumen XML disusun menggunakan nodes , yaitu terdiri dari node node elemen, node node atribute dan node node teks Node node elemen XML harus harus dibuka dan ditutup tag, misal. tag pembuka <catfood> dan tag penutup </catfood> Tag XML adalah sensitif e, misal. <catfood> tidak sama dengan <catFood> Elemen XML harus bertingkat dengan benar: <catfood> <manufacturer> Purina </manufacturer> <address> 12 Cat Way, Boise, Idaho, 21341 </address> <date> 2019-10-01 </date> </catfood> Node node teks (data) isinya didalam tag pembuka dan tag penutup Node note atribut XML berisi nilai-nilai yang harus di quoted, misal. <catfood type=\"basic\"></catfood> XPath selalu mengasumsikan data terstruktur ( structured data). \u00b6 Now let\u2019s start using XPath. Menelusuri pohon node HTML menggunakan XPath \u00b6 Cara yang paling umum untuk merepresentasikan struktur dari dokumen XML atau HTML adalah dengan pohon node (node tree): Dalam sebuah dokumen HTML , segala sesuatunya adalah node: Seluruh dokumen adalah node dokumen Setiap elemen elemen HTML adalah node elemen Teks didalam elemen HTML adalah node-node teks Node-node dalam suatu pohon memiliki hubungan hirarki dengan yang lain. Kita menggunakan istilah parent , child dan sibling untuk menjelaskan hubungan ini: Dalam pohon node, node paling atas disebut dengan root (atau root node ) Setiap node memiliki tetap satu parent , kecuali root (yang tidak memiliki parent) Sutau node dapat memilik satu, beberapa atau tidak sama sekali children *Sibling adalah node-node dengan parent sama Serangkaian hubungan antra node ke node disebut dengan path Path-path dalam XPath didefinisikan dengan menggunakan slash ( / ) untuk memisahkan langkah/tahapan dalam rangkaian keterkaitan node, seperti URL-URLS atau direktori Unix Dalam XPath, semua ekspresi didasarkan pada context node . Context node adalah node path dari mana dimulai. Default context adalah node root , yang dinyatakan dengan slash tunggal (/), seperti dalam contoh diatas. Yang paling banyak digunakan untuk ekpresi patha adalah sebagai berikut: Expression Description nodename Memilih semua node dengan nama \u201cnodename\u201d / Garis miring tunggal awal menunjukkan pemilihan dari simpul akar, garis miring berikutnya menunjukkan pemilihan simpul anak dari simpul saat ini // Pilih node anak langsung dan tidak langsung dalam dokumen dari node saat ini - ini memberi kita kemampuan untuk \"skip levels\" . Piiih context node saat ini .. Pilih parent dari context node @ Pilih atribut dari context node [@attribute = 'value'] Pilihan node dengan nilai atribut tertentu text() Pilih isi teks dari node | Rantai ekspresi dan membawa kembali hasil dari ekspresi mana pun Sintaks XPath lanjutan \u00b6 Operators \u00b6 Operator digunakan untuk membandingkan node-node. Terdaoat operator matematika,operator boolean. Operators dapat memberikan nilai boolean (benar /salah) sebagai hasil. Disini beberap yang banyak digunakan : Operator Penjelasan = Membandingkan kesamaan, dapat digunakan untuk nilai numerik maupun teks != Dignuanak untuk membandikanketidak samaan >, >= Lebih besar, lebih besar dari atau sama dengan <, <= Lebih dari, lebih dari atau sama dengan or Boolean atau (or) and Boolean dan (and) not Boolean bukan (not) Contoh \u00b6 Ekspresi Path Hasil Ekspresi html/body/div/h3/ @id =\u2019exercises-2\u2019 Apakah exercise 2 ada? html/body/div/h3/ @id !=\u2019exercises-4\u2019 Apakah exercise 4 tidak ada? //h1/ @id =\u2019references\u2019 or @id =\u2019introduction\u2019 Apakah terdapat terdapat references h1 atau introduction? Predikat \u00b6 Predikat digunakan untuk menemukan node tertentu atau node yang berisi nilai tertentu Predikat selalu diletakkan dalam kurung siku, dimaksudkan untuk memberikan informasi pemfilteran tambahan untuk mengembalikan node. Anda dapat memfilter pada sebuah node dengan menggunakan operator atau fungsi Examples \u00b6 Operator Explanation [1] Pilih node pertama [last()] Pilih node yang terakhir [last()-1] Pilih node terakhir kedua [position()<3] Pilih dua node pertama, perhatikan posisi pertama dimulai dari 1, bukan = [@lang] Pilih node yang memiliki atribut \u2018lang\u2019 [@lang='en'] Pilih semua node yang memiliki atribute dengan nilai atribute \u201cen\u201d [price>15.00] Pilih semua node yang memiliki node price yang lebih besar dari 15.00 Pencarian dalam teks \u00b6 XPath dapat melakukan pencarian dalam teks mengunakan fungsi. Perhatikan: pencarian dalatem teks adalah case-sensitive! Path Expression Result //author[contains(.,\"Matt\")] Cocokkan pada semua node author, dalam node sekarang yang berisi Matt (case-sensitive) //author[starts-with(.,\"G\")] Cocokkan pada semua node author, dalam node sekarang yang dimulai dengan G (case-sensitive) //author[ends-with(.,\"w\")] Cocokkan pada semua dalam node sekarang yang yang berakhiran dengan w (case-sensitive) Menampilkan source dari suatu page \u00b6 Untuk menemukan xpath element halaman website maka kita dapat membuka source page halama tersebut. Yaitu dengan cara klik kanan pada halaman Memetakan suatu webpage denganXPath menggunakan console suatu browser \u00b6 KIta akan menggunakan kode HTML yang ada pada website portal tugas akhir mahasiswa sebagai contoh. Biasanya, setiap browser web dapat menampilkan sumber kode HTML dengan fungsinya tersendiri dari browser tersebut. Contoh diatas adalah menggunakan browser firefox dengan melakukan klik kanan pada pointer akan menampilakn submenu salah satunya View Page Source. Kemudian klik menu tersebut akand dapatkan source sumbernya sebagai berikut. <!DOCTYPE html> < html > < head > < meta charset = \"utf-8\" /> < title > Portal Tugas Akhir Univ. Trunojoyo </ title > < link rel = \"stylesheet\" href = \"https://pta.trunojoyo.ac.id/css/reset.css\" type = \"text/css\" /> < link rel = \"stylesheet\" href = \"https://pta.trunojoyo.ac.id/css/style.css\" type = \"text/css\" /> < link rel = \"stylesheet\" href = \"https://pta.trunojoyo.ac.id/css/960_12_col.css\" type = \"text/css\" /> < link rel = \"stylesheet\" media = \"screen\" href = \"https://pta.trunojoyo.ac.id/css/smoothness/jquery-ui-1.8.18.custom.css\" /> <!--[if lte IE 8]> <link rel=\"stylesheet\" type=\"text/css\" href=\"css/ie.css\" /> <![endif]--> <!-- favicon --> < link rel = \"shortcut icon\" type = \"image/x-icon\" href = \"https://pta.trunojoyo.ac.id/images/favicon.ico\" /> (...) </ body > </ html > Kita dapat melihat dari source code bahwa judul dari halama ini dalam elemen title yang ada dalam elemen head , yang ada didalam elemen html yang berisi seluruh isi dari page Sehingga jika kita ingin memberi tahu web scraper untuk mencari judul dari halaman ini, kita menggunakan informasi ini untuk menentukan path yang diperlukan untuk menjelajahi isi HTML dari page untuk menemukan element title . XPath memungkinkan untuk melakukan hal tersebut. Kita dapat menjalankan query XPath langsung pada browser dengan JavaScript console yang ada didalamnya. Menampilkan console dalam browser \u00b6 Pad Firefox, gunakan menut Tools > Web Developer > Web Console . PadaChrome, gunakan menut View > Developer > JavaScript Console Disini bagaimana console tampak dalam browser Firefox : Untuk saat ini, jangan khawatir terlalu banyak pesan error manakalan anda meliha dalam console ketika anda membukanya. Console akan dengan muncul prompt dengan karakter > (atau >> dalam Firefox) menunggu anda untuk mengetik perintah . Sintaks untuk menjalankan query XPath dalam JavaScript console adalah $x(\"XPATH_QUERY\") , misalkan: $x(\"/html/head/title/text()\") Ini akan menghasilkan sesuatu seperti ini <- Array [ #text \"Portal Tugas Akhir Univ. Trunojoyo\" ] Menggunkan extension firefox xPath Finder \u00b6 Untuk menentukan cpath dari element dari halam website dengan mudah dilakukan menggunakan xPath Finder. Jika belum ada ektension firefox xPath Finder ,silahkan install dulu extension tersebut. Penggunaannya adalah dengan cara klik pada icon extension curson akan berubah menjadi crosshair, kemudian pilih elemen yang diinginkan dengan kursor diarahkan pada elemen tersebut ( elemen akan berubah warna ). Klik elemen tersebut sehingga akan tampil xPath dari elemen tersebut di bawah kiri dari halaman In [ 3 ] : fetch ( 'https://pta.trunojoyo.ac.id/' ) 2020 -09-25 04 :32:42 [ scrapy.core.engine ] DEBUG: Crawled ( 200 ) <GET https://pta.t runojoyo.ac.id/> ( referer: None ) In [ 4 ] : response.xpath ( \"/html/body/div[2]/div[1]/div[2]/ul/li[1]/div[1]/a\" ) Out [ 4 ] : [ <Selector xpath = '/html/body/div[2]/div[1]/div[2]/ul/li[1]/div[1]/a' dat a = '<a class=\"title\" href=\"#\">Analisis Wa...' > ] Dari hasil scrap diatas ddapat diperbaiki ouputnya dengan perintah sebagai berikut In [ 5 ] : response.xpath ( \"//item/title/text()\" ) .extract_first () In [ 6 ] : response.xpath ( \"/html/body/div[2]/div[1]/div[2]/ul/li[1]/div[1]/a/text( ...: )\" ) .extract_first () Out [ 6 ] : 'Analisis Wacana Media Online Detik.com dalam Memberitakan Peristiwa Ker usuhan Mahasiswa Papua di Surabaya' Mari kita bahas query XPath yang digunakan pada contoh diatas dengan XPath /html/head/title/text() . Yang pertama / menyatakan root dari suatu dokumen. Dengan query tersebut , kita memberit tahu browser untuk / Start at the root of the document\u2026 html/ \u2026 mengarahkan ke node html \u2026 head/ \u2026 kemudaian ke node head yang ada didalamnya\u2026 title/ \u2026 kemudaian ke node title yang ada didalamnya\u2026 text() dan pilih node text yang terdapat dalam elemen itu Dengan menggunakan sintaks ini , XPath kemudian memungkinkan kita untuk menentukan path yang tepat pada suatu node. Memilih judul tugas akhir \u00b6 $x ( \"/html/body/div[2]/div[1]/div[2]/ul/li[1]/div[1]/a\" ) Menggunakan Scraper Chrome extension untuk menavigasi XPath \u00b6 Ini adalah alat yang memudahkan untuk mengimplementasikan XPath. Alat ini adalah tambahan dari browser Chrome untuk melakukan scrap data dari suatu website. Jika kita ingin menggunakan tool ini maka perlu ditambahkan pada browser Chrome kita seperti biasa kita menambahkan extension Chrome secara umum. Setelah kita install extension ini misalkan kita sedang menjelajahi pta.trunojoyo.ac.id, pada saat klik kanan pada website tersebut maka akan muncul menu Dan bila kita mengklik menu scraper similar maka akan memunculkan XPath : //div[2]/div[1]/div[2]/ul/li dengan data yang ditarik dari halam website tersebut. Pada view di samping Xpath judul, penulis, dosen pembimbing dan abstrak ditampilkan dalam satu kolom dengan nama kolom text. Untuk memperbaiki hasil tampilan ini, supaya menjadi field field yang sesuai, maka diperlukan perubahan dengan menambahkan nama kolom (field) dan memecah XPath yang sesuai. Berikut perubahan yang telah dilakukan 2.**Membuat Scrapy parser dengan Python ** \u00b6 Persiapan \u00b6 Sebelum saya menjelaskan lebih detail pekerjaan yang harus dilakukan untuk menarik data dari halaman web maka pertama kali yang perlu dipersiapkan adalah instalasi alat alat yaitu Scrapy library (v2.3.0) PyMongo PyMongo (v3.11.0) untuk menyimpan data dalam MongoDB. MongoDB Instalasi a. Install Scrapy : pip install scrapy] b. Install PyMongo : pip install pymongo c. Install Mongo DB. Untuk menginstal MongoDB anda dapat mengunduh file instalasi dari http://downloads.mongodb.org/win32/mongodb-win32-i386-3.2.22-signed.msi . Setelah selesai mengunduh, klik dua kali file instalasi tersebut dan lakukan instalasi dengan klik next... sampai selesai. Desain secara umum scraping website \u00b6 Source Web scraping has become an effective way o Pembuatan Proyek \u00b6 Proyek 1. Menarik data harga dari Shopee a. Membuat proyek bernama shopee scrapy startproject shopee Akan terbentuk struktur folder shopee seperti berikut \\---shopee | scrapy.cfg | \\---shopee | items.py | middlewares.py | pipelines.py | settings.py | __init__.py | \\---spiders __init__.py scrapy genspider aliexpress_tablets https://www.aliexpress.com/category/200216607/tablets.html Created spider 'aliexpress_tablets' using template 'basic' in module: shopee.spiders.aliexpress_tablets Setelah menyelesaikan instalasi alat alat yang diperlukan untuk membuat proyek scrapy. Saatnya kit membuat proyek baru scrapy untuk menarik data dari halaman web dengan perintah scrapy startproject prowebmining Didalam folder prowebmining akan terbentuk file file dan folder sebagai berikut \u251c\u2500\u2500 scrapy.cfg \u2514\u2500\u2500 stack \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 items.py \u251c\u2500\u2500 pipelines.py \u251c\u2500\u2500 settings.py \u2514\u2500\u2500 spiders \u2514\u2500\u2500 __init__.py Scrapy merupakan framewerok aplikasi yang memungkinkan suatu suatu proyek sesuai dengan gaya pemrograman Object Oriented untuk mendefinisikan item-item dan spider untuk keseluruhan aplikasi. Struktur proyek dari scrapy yang telah dibuat diatas dengan masing masing sebagai berikut scrapy.cfg : ini adalah file konfigurasi proyek yang berisi modul pengaturan untuk proyek dan juga dengan informasi pengimplementasiannya. test_project : Ini adalah direktori aplikasi dengan banyak macam file yang benar bena bertanggung jawab untuk menjalankan dan menarik data dari urls-url web. items.py : item-item yang akan ditarik oleh scraper, ini juga bekerja seperti Python dicts . Karena kita dapat menggunakan plain Python dicts dengan Scrapy, Items menyediakan proteksi tambahan terhadap field-field yang tidak dideklarasikan. Hal itu dideklarasikan dengan pembuatan scrapy.Item class dan mendefinisika atribut atributnya sebagai scrapy.Field . pipelines.py : Setelah item di scrap oleh spider, item itu dikirim ke Item Pipeline yang memprosesnya dengan menggunakan beberapa komponen yang dijalankan secara berurutan. Masing masing item pipeline component adalah kelas Python yang harus mengimplementasikan metode yang disebut dengan process_item untuk memproses item yang di- scrap . Proses dilakukan terhadap item dengan memutuskan item harus diteruskan atau didrop dan tidak dinajutkan proses berikutnya. settings.py : Ini memungkinkan kita untuk mengkustom semua karakteristi komponen-komponen Scrapy termasuk core, extensions, pipelines dan spiders itu sediri. spiders : Spiders adalah direktori yang berisi semua spiders/crawlers seperti kelas-kelas Python . Ketika kita menjalankan atau meng-crawler spider maka scrapy mencari didalam di direktori ini dan mencoba untuk menemukan spider dengan namanya yang telah ditetapkan oleh pengguna. Spider mendefinisikan bagaimana site tertentu atau sekumpulan site, termasuk bagaimana melakukan crawl dan bagaimana mengektrak data dari page-pagenya. Artinya, Spiders adalah tempat dimana kita mendefinisikan tiga atribut utam yaitu start_urls yang meberi tahu URLs mana yang akan ditarik, , allowed_domains yaitu mendefinisikan domain mana saja yang boleh parse adalah suatu metode yang dipanggil ketika ada respon yang berasal dari lodged requests . Atribut ini adalah penting karena ini inti dari definisi Spider Setelah kita memiliki proyek seperti diatas, kita perlu membuat spider didalam direktori prowebmining. Spider adalah potongan-potongan kode python yang menentukan bagaiman laman web akan ditarik.Ini adalah komponen utama yang menarik berbagai laman web dan mengektra isi darinya. Dalam kasus kita, kita akan melakukan tugas menjelajahi Amazon dan menarik ulasan-ulasan Amazon. Membuat Spider \u00b6 Membuat file yang disebut dengan stack_spider.py dalam direktori\u201cspiders\u201d . Ini digunakan untuk menemukan data yang akan kita cari, artinya kita akan menetapkan laman laman page yang ingin discrap. Diawali dengan mendefinisikan suatu kelas yang diturunkan dari Spider kemudian menambahkan atribut atribut yang diperlukan : from scrapy import Spider class StackSpider ( Spider ): name = \"stack\" allowed_domains = [ \"\" ] start_urls = [ \"\" , ] dari code diatas dijelaskan sebagai berikut : name adalah nama dari Spider allowed_domains berisi base-URLs domain domain yang dijinkan pada spider untuk di crawl crawl start_urls adalah daftar URLs pada spideruntuk spider untuk memulai melakukan crawlinng dari laman web tersebut. Semua URL berikutnya akan dimulai dari data yang diunduh spider dari URL di `start_urls' scrapy genspider amazon_review your-link-here | scrapy.cfg \\---prowebmining | items.py | middlewares.py | pipelines.py | settings.py | __init__.py | +---spiders | | amazon_review.py | | __init__.py | | | \\---__pycache__ | __init__.cpython-37.pyc | \\---__pycache__ settings.cpython-37.pyc __init__.cpython-37.pyc Pada struktur projek diatas terdapat file middlware.py yang dihasilkan yaitu framework yang terkait dengan mekanisme pemrosesan yang terkait dengan permintaan dan item yang dibangkitkan Spyder. Menetapkan data yang akan dikumpulkan \u00b6 File items.py digunakan untuk mendefinisikan ruang penyimpanan untuk data yang kita rencanakan untuk di scrap Kelas StackItem() diturunkan dari Item ( docs ), yang pada dasarnya memilih beberapa pre-defined objects : import scrapy class StackItem ( scrapy . Item ): # defisikan field-field untuk item anda disini: # name = scrapy.Field() pass Silahkan tambahkan beberapa item-item yang akan ingin dikumpulkan ditarik dari laman web. Untuk setiap pertanyaan client perlu judul dan URL. Sehingga, perubahannya untuk items.py seperti ini: from scrapy.item import Item , Field class StackItem ( Item ): title = Field () url = Field () XPath Selectors \u00b6 Selanjutnya, Scrapy menggunakan XPath selectors untuk mengektrak data dari website. Artinya, kita dapat memilih bagian tertentu dari data HTML didasarkan pada XPath yang ditentukan. Seperti yang telah dijelaskan diatas pada saat analisa struktur halaman website, sedikit banyak dibahas apa itu XPath","title":"Scrapfirst"},{"location":"scrapfirst/#membangun-scraping-pertama-kali","text":"Untuk melakukan tahapan penarikan data dari suatu halaman halaman web maka yang perlu dipehatikan adalah **Menganalisa ** strutktur HTML dari laman web Scraping adalah tentang menemukan pola dalam laman- laman web dan mengektraksi isi dari laman tersebut. Sebelum mulai untuk menulis tool scrapyng (scraper), kita perlu untuk memahami struktur HTML dari web page yang akan ditarik datanya dan mengidentifikasi pola didalamnya. Pola dapat terkait dengan penggunaan classes, id, and elemen elemen lain HTML. **Membuat Scrapy parser dengan ** Setelah menganlisa struktur dari halaman web yang akan ditarik datanya, kita lakukan implementasi dengan menulis code untuk menghasilkan scrapy parser . Scrapy parser bertanggung jawab untuk menjelajahi web yang akan ditarik datanya dan mengektrak informasi sesuai dengan aturan yang dibuat Mengumpulkan dan menyimpan informasi Parser dapat menyimpan hasilnya sesuai dengan format yang anda inginkan misalkan csv atau json. Ini adalah hasil akhir dari data yang telah dikumpulkan","title":"Membangun Scraping Pertama kali."},{"location":"scrapfirst/#1analisa-struktur-html-halaman-website","text":"XPath (kepanjangan dari XML Path Language) adalah expression language yang digunakan untuk menetapkanbagian bagian bagian bagian dari dokumen XML . XPath digunakan dalam perangkat lunak dan bahasa yang digunakan untuk manipulasi dokumen XML, seperti XSLT, XQuery atau alat alat web scraping XPath dapat juga digunakan dalam struktur yang sama seperti XML, misalkan html","title":"1.Analisa Struktur HTML Halaman Website"},{"location":"scrapfirst/#markup-languages","text":"XML dan HTML adalah markup languages . Artinya keduanya menggunakan sekumpulan tags atau rule untuk menyusun dan menyedian informasi yang ada didalamnya. Struktur ini membantu untuk secara otomatis untuk pemrosesan , pengeditan, pemformatan dan penampilan dan pencetakan informasi tersebut. Dokumen XML menyimpan data dalam format plain teks. Ini menyediakan perangkat lunan dan perangkat kerans dengan cara bebas untuk menyimpan, mentransformasi, dan menshare data. Format XML adalah format terbuka. Anda dapat membuka dokumen XML dalam editor teks dan data yang ada didalamnya dapat disajikan. Ini memungkinkan pertukran sistem yang tidak kompatibel dan mudah untuk mengkonversi suatu data. Dokumen XML memilki aturan-aturan dasar sebagai berikut: Dokumen XML disusun menggunakan nodes , yaitu terdiri dari node node elemen, node node atribute dan node node teks Node node elemen XML harus harus dibuka dan ditutup tag, misal. tag pembuka <catfood> dan tag penutup </catfood> Tag XML adalah sensitif e, misal. <catfood> tidak sama dengan <catFood> Elemen XML harus bertingkat dengan benar: <catfood> <manufacturer> Purina </manufacturer> <address> 12 Cat Way, Boise, Idaho, 21341 </address> <date> 2019-10-01 </date> </catfood> Node node teks (data) isinya didalam tag pembuka dan tag penutup Node note atribut XML berisi nilai-nilai yang harus di quoted, misal. <catfood type=\"basic\"></catfood>","title":"Markup Languages"},{"location":"scrapfirst/#xpath-selalu-mengasumsikan-data-terstruktur-structured-data","text":"Now let\u2019s start using XPath.","title":"XPath selalu mengasumsikan data terstruktur (structured data)."},{"location":"scrapfirst/#menelusuri-pohon-node-html-menggunakan-xpath","text":"Cara yang paling umum untuk merepresentasikan struktur dari dokumen XML atau HTML adalah dengan pohon node (node tree): Dalam sebuah dokumen HTML , segala sesuatunya adalah node: Seluruh dokumen adalah node dokumen Setiap elemen elemen HTML adalah node elemen Teks didalam elemen HTML adalah node-node teks Node-node dalam suatu pohon memiliki hubungan hirarki dengan yang lain. Kita menggunakan istilah parent , child dan sibling untuk menjelaskan hubungan ini: Dalam pohon node, node paling atas disebut dengan root (atau root node ) Setiap node memiliki tetap satu parent , kecuali root (yang tidak memiliki parent) Sutau node dapat memilik satu, beberapa atau tidak sama sekali children *Sibling adalah node-node dengan parent sama Serangkaian hubungan antra node ke node disebut dengan path Path-path dalam XPath didefinisikan dengan menggunakan slash ( / ) untuk memisahkan langkah/tahapan dalam rangkaian keterkaitan node, seperti URL-URLS atau direktori Unix Dalam XPath, semua ekspresi didasarkan pada context node . Context node adalah node path dari mana dimulai. Default context adalah node root , yang dinyatakan dengan slash tunggal (/), seperti dalam contoh diatas. Yang paling banyak digunakan untuk ekpresi patha adalah sebagai berikut: Expression Description nodename Memilih semua node dengan nama \u201cnodename\u201d / Garis miring tunggal awal menunjukkan pemilihan dari simpul akar, garis miring berikutnya menunjukkan pemilihan simpul anak dari simpul saat ini // Pilih node anak langsung dan tidak langsung dalam dokumen dari node saat ini - ini memberi kita kemampuan untuk \"skip levels\" . Piiih context node saat ini .. Pilih parent dari context node @ Pilih atribut dari context node [@attribute = 'value'] Pilihan node dengan nilai atribut tertentu text() Pilih isi teks dari node | Rantai ekspresi dan membawa kembali hasil dari ekspresi mana pun","title":"Menelusuri pohon node HTML menggunakan XPath"},{"location":"scrapfirst/#sintaks-xpath-lanjutan","text":"","title":"Sintaks XPath lanjutan"},{"location":"scrapfirst/#operators","text":"Operator digunakan untuk membandingkan node-node. Terdaoat operator matematika,operator boolean. Operators dapat memberikan nilai boolean (benar /salah) sebagai hasil. Disini beberap yang banyak digunakan : Operator Penjelasan = Membandingkan kesamaan, dapat digunakan untuk nilai numerik maupun teks != Dignuanak untuk membandikanketidak samaan >, >= Lebih besar, lebih besar dari atau sama dengan <, <= Lebih dari, lebih dari atau sama dengan or Boolean atau (or) and Boolean dan (and) not Boolean bukan (not)","title":"Operators"},{"location":"scrapfirst/#contoh","text":"Ekspresi Path Hasil Ekspresi html/body/div/h3/ @id =\u2019exercises-2\u2019 Apakah exercise 2 ada? html/body/div/h3/ @id !=\u2019exercises-4\u2019 Apakah exercise 4 tidak ada? //h1/ @id =\u2019references\u2019 or @id =\u2019introduction\u2019 Apakah terdapat terdapat references h1 atau introduction?","title":"Contoh"},{"location":"scrapfirst/#predikat","text":"Predikat digunakan untuk menemukan node tertentu atau node yang berisi nilai tertentu Predikat selalu diletakkan dalam kurung siku, dimaksudkan untuk memberikan informasi pemfilteran tambahan untuk mengembalikan node. Anda dapat memfilter pada sebuah node dengan menggunakan operator atau fungsi","title":"Predikat"},{"location":"scrapfirst/#examples","text":"Operator Explanation [1] Pilih node pertama [last()] Pilih node yang terakhir [last()-1] Pilih node terakhir kedua [position()<3] Pilih dua node pertama, perhatikan posisi pertama dimulai dari 1, bukan = [@lang] Pilih node yang memiliki atribut \u2018lang\u2019 [@lang='en'] Pilih semua node yang memiliki atribute dengan nilai atribute \u201cen\u201d [price>15.00] Pilih semua node yang memiliki node price yang lebih besar dari 15.00","title":"Examples"},{"location":"scrapfirst/#pencarian-dalam-teks","text":"XPath dapat melakukan pencarian dalam teks mengunakan fungsi. Perhatikan: pencarian dalatem teks adalah case-sensitive! Path Expression Result //author[contains(.,\"Matt\")] Cocokkan pada semua node author, dalam node sekarang yang berisi Matt (case-sensitive) //author[starts-with(.,\"G\")] Cocokkan pada semua node author, dalam node sekarang yang dimulai dengan G (case-sensitive) //author[ends-with(.,\"w\")] Cocokkan pada semua dalam node sekarang yang yang berakhiran dengan w (case-sensitive)","title":"Pencarian dalam teks"},{"location":"scrapfirst/#menampilkan-source-dari-suatu-page","text":"Untuk menemukan xpath element halaman website maka kita dapat membuka source page halama tersebut. Yaitu dengan cara klik kanan pada halaman","title":"Menampilkan source dari suatu page"},{"location":"scrapfirst/#memetakan-suatu-webpage-denganxpath-menggunakan-console-suatu-browser","text":"KIta akan menggunakan kode HTML yang ada pada website portal tugas akhir mahasiswa sebagai contoh. Biasanya, setiap browser web dapat menampilkan sumber kode HTML dengan fungsinya tersendiri dari browser tersebut. Contoh diatas adalah menggunakan browser firefox dengan melakukan klik kanan pada pointer akan menampilakn submenu salah satunya View Page Source. Kemudian klik menu tersebut akand dapatkan source sumbernya sebagai berikut. <!DOCTYPE html> < html > < head > < meta charset = \"utf-8\" /> < title > Portal Tugas Akhir Univ. Trunojoyo </ title > < link rel = \"stylesheet\" href = \"https://pta.trunojoyo.ac.id/css/reset.css\" type = \"text/css\" /> < link rel = \"stylesheet\" href = \"https://pta.trunojoyo.ac.id/css/style.css\" type = \"text/css\" /> < link rel = \"stylesheet\" href = \"https://pta.trunojoyo.ac.id/css/960_12_col.css\" type = \"text/css\" /> < link rel = \"stylesheet\" media = \"screen\" href = \"https://pta.trunojoyo.ac.id/css/smoothness/jquery-ui-1.8.18.custom.css\" /> <!--[if lte IE 8]> <link rel=\"stylesheet\" type=\"text/css\" href=\"css/ie.css\" /> <![endif]--> <!-- favicon --> < link rel = \"shortcut icon\" type = \"image/x-icon\" href = \"https://pta.trunojoyo.ac.id/images/favicon.ico\" /> (...) </ body > </ html > Kita dapat melihat dari source code bahwa judul dari halama ini dalam elemen title yang ada dalam elemen head , yang ada didalam elemen html yang berisi seluruh isi dari page Sehingga jika kita ingin memberi tahu web scraper untuk mencari judul dari halaman ini, kita menggunakan informasi ini untuk menentukan path yang diperlukan untuk menjelajahi isi HTML dari page untuk menemukan element title . XPath memungkinkan untuk melakukan hal tersebut. Kita dapat menjalankan query XPath langsung pada browser dengan JavaScript console yang ada didalamnya.","title":"Memetakan suatu webpage denganXPath menggunakan console suatu  browser"},{"location":"scrapfirst/#menampilkan-console-dalam-browser","text":"Pad Firefox, gunakan menut Tools > Web Developer > Web Console . PadaChrome, gunakan menut View > Developer > JavaScript Console Disini bagaimana console tampak dalam browser Firefox : Untuk saat ini, jangan khawatir terlalu banyak pesan error manakalan anda meliha dalam console ketika anda membukanya. Console akan dengan muncul prompt dengan karakter > (atau >> dalam Firefox) menunggu anda untuk mengetik perintah . Sintaks untuk menjalankan query XPath dalam JavaScript console adalah $x(\"XPATH_QUERY\") , misalkan: $x(\"/html/head/title/text()\") Ini akan menghasilkan sesuatu seperti ini <- Array [ #text \"Portal Tugas Akhir Univ. Trunojoyo\" ]","title":"Menampilkan console dalam  browser"},{"location":"scrapfirst/#menggunkan-extension-firefox-xpath-finder","text":"Untuk menentukan cpath dari element dari halam website dengan mudah dilakukan menggunakan xPath Finder. Jika belum ada ektension firefox xPath Finder ,silahkan install dulu extension tersebut. Penggunaannya adalah dengan cara klik pada icon extension curson akan berubah menjadi crosshair, kemudian pilih elemen yang diinginkan dengan kursor diarahkan pada elemen tersebut ( elemen akan berubah warna ). Klik elemen tersebut sehingga akan tampil xPath dari elemen tersebut di bawah kiri dari halaman In [ 3 ] : fetch ( 'https://pta.trunojoyo.ac.id/' ) 2020 -09-25 04 :32:42 [ scrapy.core.engine ] DEBUG: Crawled ( 200 ) <GET https://pta.t runojoyo.ac.id/> ( referer: None ) In [ 4 ] : response.xpath ( \"/html/body/div[2]/div[1]/div[2]/ul/li[1]/div[1]/a\" ) Out [ 4 ] : [ <Selector xpath = '/html/body/div[2]/div[1]/div[2]/ul/li[1]/div[1]/a' dat a = '<a class=\"title\" href=\"#\">Analisis Wa...' > ] Dari hasil scrap diatas ddapat diperbaiki ouputnya dengan perintah sebagai berikut In [ 5 ] : response.xpath ( \"//item/title/text()\" ) .extract_first () In [ 6 ] : response.xpath ( \"/html/body/div[2]/div[1]/div[2]/ul/li[1]/div[1]/a/text( ...: )\" ) .extract_first () Out [ 6 ] : 'Analisis Wacana Media Online Detik.com dalam Memberitakan Peristiwa Ker usuhan Mahasiswa Papua di Surabaya' Mari kita bahas query XPath yang digunakan pada contoh diatas dengan XPath /html/head/title/text() . Yang pertama / menyatakan root dari suatu dokumen. Dengan query tersebut , kita memberit tahu browser untuk / Start at the root of the document\u2026 html/ \u2026 mengarahkan ke node html \u2026 head/ \u2026 kemudaian ke node head yang ada didalamnya\u2026 title/ \u2026 kemudaian ke node title yang ada didalamnya\u2026 text() dan pilih node text yang terdapat dalam elemen itu Dengan menggunakan sintaks ini , XPath kemudian memungkinkan kita untuk menentukan path yang tepat pada suatu node.","title":"Menggunkan extension firefox xPath Finder"},{"location":"scrapfirst/#memilih-judul-tugas-akhir","text":"$x ( \"/html/body/div[2]/div[1]/div[2]/ul/li[1]/div[1]/a\" )","title":"Memilih judul tugas akhir"},{"location":"scrapfirst/#menggunakan-scraper-chrome-extension-untuk-menavigasi-xpath","text":"Ini adalah alat yang memudahkan untuk mengimplementasikan XPath. Alat ini adalah tambahan dari browser Chrome untuk melakukan scrap data dari suatu website. Jika kita ingin menggunakan tool ini maka perlu ditambahkan pada browser Chrome kita seperti biasa kita menambahkan extension Chrome secara umum. Setelah kita install extension ini misalkan kita sedang menjelajahi pta.trunojoyo.ac.id, pada saat klik kanan pada website tersebut maka akan muncul menu Dan bila kita mengklik menu scraper similar maka akan memunculkan XPath : //div[2]/div[1]/div[2]/ul/li dengan data yang ditarik dari halam website tersebut. Pada view di samping Xpath judul, penulis, dosen pembimbing dan abstrak ditampilkan dalam satu kolom dengan nama kolom text. Untuk memperbaiki hasil tampilan ini, supaya menjadi field field yang sesuai, maka diperlukan perubahan dengan menambahkan nama kolom (field) dan memecah XPath yang sesuai. Berikut perubahan yang telah dilakukan","title":"Menggunakan  Scraper Chrome extension untuk menavigasi XPath"},{"location":"scrapfirst/#2membuat-scrapy-parser-dengan-python","text":"","title":"2.**Membuat Scrapy parser dengan Python **"},{"location":"scrapfirst/#persiapan","text":"Sebelum saya menjelaskan lebih detail pekerjaan yang harus dilakukan untuk menarik data dari halaman web maka pertama kali yang perlu dipersiapkan adalah instalasi alat alat yaitu Scrapy library (v2.3.0) PyMongo PyMongo (v3.11.0) untuk menyimpan data dalam MongoDB. MongoDB Instalasi a. Install Scrapy : pip install scrapy] b. Install PyMongo : pip install pymongo c. Install Mongo DB. Untuk menginstal MongoDB anda dapat mengunduh file instalasi dari http://downloads.mongodb.org/win32/mongodb-win32-i386-3.2.22-signed.msi . Setelah selesai mengunduh, klik dua kali file instalasi tersebut dan lakukan instalasi dengan klik next... sampai selesai.","title":"Persiapan"},{"location":"scrapfirst/#desain-secara-umum-scraping-website","text":"Source Web scraping has become an effective way o","title":"Desain secara umum scraping website"},{"location":"scrapfirst/#pembuatan-proyek","text":"Proyek 1. Menarik data harga dari Shopee a. Membuat proyek bernama shopee scrapy startproject shopee Akan terbentuk struktur folder shopee seperti berikut \\---shopee | scrapy.cfg | \\---shopee | items.py | middlewares.py | pipelines.py | settings.py | __init__.py | \\---spiders __init__.py scrapy genspider aliexpress_tablets https://www.aliexpress.com/category/200216607/tablets.html Created spider 'aliexpress_tablets' using template 'basic' in module: shopee.spiders.aliexpress_tablets Setelah menyelesaikan instalasi alat alat yang diperlukan untuk membuat proyek scrapy. Saatnya kit membuat proyek baru scrapy untuk menarik data dari halaman web dengan perintah scrapy startproject prowebmining Didalam folder prowebmining akan terbentuk file file dan folder sebagai berikut \u251c\u2500\u2500 scrapy.cfg \u2514\u2500\u2500 stack \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 items.py \u251c\u2500\u2500 pipelines.py \u251c\u2500\u2500 settings.py \u2514\u2500\u2500 spiders \u2514\u2500\u2500 __init__.py Scrapy merupakan framewerok aplikasi yang memungkinkan suatu suatu proyek sesuai dengan gaya pemrograman Object Oriented untuk mendefinisikan item-item dan spider untuk keseluruhan aplikasi. Struktur proyek dari scrapy yang telah dibuat diatas dengan masing masing sebagai berikut scrapy.cfg : ini adalah file konfigurasi proyek yang berisi modul pengaturan untuk proyek dan juga dengan informasi pengimplementasiannya. test_project : Ini adalah direktori aplikasi dengan banyak macam file yang benar bena bertanggung jawab untuk menjalankan dan menarik data dari urls-url web. items.py : item-item yang akan ditarik oleh scraper, ini juga bekerja seperti Python dicts . Karena kita dapat menggunakan plain Python dicts dengan Scrapy, Items menyediakan proteksi tambahan terhadap field-field yang tidak dideklarasikan. Hal itu dideklarasikan dengan pembuatan scrapy.Item class dan mendefinisika atribut atributnya sebagai scrapy.Field . pipelines.py : Setelah item di scrap oleh spider, item itu dikirim ke Item Pipeline yang memprosesnya dengan menggunakan beberapa komponen yang dijalankan secara berurutan. Masing masing item pipeline component adalah kelas Python yang harus mengimplementasikan metode yang disebut dengan process_item untuk memproses item yang di- scrap . Proses dilakukan terhadap item dengan memutuskan item harus diteruskan atau didrop dan tidak dinajutkan proses berikutnya. settings.py : Ini memungkinkan kita untuk mengkustom semua karakteristi komponen-komponen Scrapy termasuk core, extensions, pipelines dan spiders itu sediri. spiders : Spiders adalah direktori yang berisi semua spiders/crawlers seperti kelas-kelas Python . Ketika kita menjalankan atau meng-crawler spider maka scrapy mencari didalam di direktori ini dan mencoba untuk menemukan spider dengan namanya yang telah ditetapkan oleh pengguna. Spider mendefinisikan bagaimana site tertentu atau sekumpulan site, termasuk bagaimana melakukan crawl dan bagaimana mengektrak data dari page-pagenya. Artinya, Spiders adalah tempat dimana kita mendefinisikan tiga atribut utam yaitu start_urls yang meberi tahu URLs mana yang akan ditarik, , allowed_domains yaitu mendefinisikan domain mana saja yang boleh parse adalah suatu metode yang dipanggil ketika ada respon yang berasal dari lodged requests . Atribut ini adalah penting karena ini inti dari definisi Spider Setelah kita memiliki proyek seperti diatas, kita perlu membuat spider didalam direktori prowebmining. Spider adalah potongan-potongan kode python yang menentukan bagaiman laman web akan ditarik.Ini adalah komponen utama yang menarik berbagai laman web dan mengektra isi darinya. Dalam kasus kita, kita akan melakukan tugas menjelajahi Amazon dan menarik ulasan-ulasan Amazon.","title":"Pembuatan Proyek"},{"location":"scrapfirst/#membuat-spider","text":"Membuat file yang disebut dengan stack_spider.py dalam direktori\u201cspiders\u201d . Ini digunakan untuk menemukan data yang akan kita cari, artinya kita akan menetapkan laman laman page yang ingin discrap. Diawali dengan mendefinisikan suatu kelas yang diturunkan dari Spider kemudian menambahkan atribut atribut yang diperlukan : from scrapy import Spider class StackSpider ( Spider ): name = \"stack\" allowed_domains = [ \"\" ] start_urls = [ \"\" , ] dari code diatas dijelaskan sebagai berikut : name adalah nama dari Spider allowed_domains berisi base-URLs domain domain yang dijinkan pada spider untuk di crawl crawl start_urls adalah daftar URLs pada spideruntuk spider untuk memulai melakukan crawlinng dari laman web tersebut. Semua URL berikutnya akan dimulai dari data yang diunduh spider dari URL di `start_urls' scrapy genspider amazon_review your-link-here | scrapy.cfg \\---prowebmining | items.py | middlewares.py | pipelines.py | settings.py | __init__.py | +---spiders | | amazon_review.py | | __init__.py | | | \\---__pycache__ | __init__.cpython-37.pyc | \\---__pycache__ settings.cpython-37.pyc __init__.cpython-37.pyc Pada struktur projek diatas terdapat file middlware.py yang dihasilkan yaitu framework yang terkait dengan mekanisme pemrosesan yang terkait dengan permintaan dan item yang dibangkitkan Spyder.","title":"Membuat Spider"},{"location":"scrapfirst/#menetapkan-data-yang-akan-dikumpulkan","text":"File items.py digunakan untuk mendefinisikan ruang penyimpanan untuk data yang kita rencanakan untuk di scrap Kelas StackItem() diturunkan dari Item ( docs ), yang pada dasarnya memilih beberapa pre-defined objects : import scrapy class StackItem ( scrapy . Item ): # defisikan field-field untuk item anda disini: # name = scrapy.Field() pass Silahkan tambahkan beberapa item-item yang akan ingin dikumpulkan ditarik dari laman web. Untuk setiap pertanyaan client perlu judul dan URL. Sehingga, perubahannya untuk items.py seperti ini: from scrapy.item import Item , Field class StackItem ( Item ): title = Field () url = Field ()","title":"Menetapkan data yang akan dikumpulkan"},{"location":"scrapfirst/#xpath-selectors","text":"Selanjutnya, Scrapy menggunakan XPath selectors untuk mengektrak data dari website. Artinya, kita dapat memilih bagian tertentu dari data HTML didasarkan pada XPath yang ditentukan. Seperti yang telah dijelaskan diatas pada saat analisa struktur halaman website, sedikit banyak dibahas apa itu XPath","title":"XPath Selectors"},{"location":"scrapwebsite/","text":"Membangun Scraping Website . \u00b6 Untuk melakukan tahapan penarikan data dari suatu halaman halaman web maka yang perlu dipehatikan adalah **Menganalisa ** strutktur HTML dari laman web Scraping adalah tentang menemukan pola dalam laman- laman web dan mengektraksi isi dari laman tersebut. Sebelum mulai untuk menulis tool scrapyng (scraper), kita perlu untuk memahami struktur HTML dari web page yang akan ditarik datanya dan mengidentifikasi pola didalamnya. Pola dapat terkait dengan penggunaan classes, id, and elemen elemen lain HTML. **Membuat Scrapy parser dengan python ** Setelah menganlisa struktur dari halaman web yang akan ditarik datanya, kita lakukan implementasi dengan menulis code untuk menghasilkan scrapy parser . Scrapy parser bertanggung jawab untuk menjelajahi web yang akan ditarik datanya dan mengektrak informasi sesuai dengan aturan yang dibuat Mengumpulkan dan menyimpan informasi Parser dapat menyimpan hasilnya sesuai dengan format yang anda inginkan misalkan csv atau json. Ini adalah hasil akhir dari data yang telah dikumpulkan 1.Analisa Struktur HTML Halaman Website \u00b6 XPath (kepanjangan dari XML Path Language) adalah expression language yang digunakan untuk menetapkanbagian bagian bagian bagian dari dokumen XML . XPath digunakan dalam perangkat lunak dan bahasa yang digunakan untuk manipulasi dokumen XML, seperti XSLT, XQuery atau alat alat web scraping XPath dapat juga digunakan dalam struktur yang sama seperti XML, misalkan html Markup Languages \u00b6 XML dan HTML adalah markup languages . Artinya keduanya menggunakan sekumpulan tags atau rule untuk menyusun dan menyedian informasi yang ada didalamnya. Struktur ini membantu untuk secara otomatis untuk pemrosesan , pengeditan, pemformatan dan penampilan dan pencetakan informasi tersebut. Dokumen XML menyimpan data dalam format plain teks. Ini menyediakan perangkat lunan dan perangkat kerans dengan cara bebas untuk menyimpan, mentransformasi, dan menshare data. Format XML adalah format terbuka. Anda dapat membuka dokumen XML dalam editor teks dan data yang ada didalamnya dapat disajikan. Ini memungkinkan pertukran sistem yang tidak kompatibel dan mudah untuk mengkonversi suatu data. Dokumen XML memilki aturan-aturan dasar sebagai berikut: Dokumen XML disusun menggunakan nodes , yaitu terdiri dari node node elemen, node node atribute dan node node teks Node node elemen XML harus harus dibuka dan ditutup tag, misal. tag pembuka <catfood> dan tag penutup </catfood> Tag XML adalah sensitif e, misal. <catfood> tidak sama dengan <catFood> Elemen XML harus bertingkat dengan benar: <catfood> <manufacturer> Purina </manufacturer> <address> 12 Cat Way, Boise, Idaho, 21341 </address> <date> 2019-10-01 </date> </catfood> Node node teks (data) isinya didalam tag pembuka dan tag penutup Node note atribut XML berisi nilai-nilai yang harus di quoted, misal. <catfood type=\"basic\"></catfood> XPath selalu mengasumsikan data terstruktur ( structured data). \u00b6 Now let\u2019s start using XPath. Menelusuri pohon node HTML menggunakan XPath \u00b6 Cara yang paling umum untuk merepresentasikan struktur dari dokumen XML atau HTML adalah dengan pohon node (node tree): Dalam sebuah dokumen HTML , segala sesuatunya adalah node: Seluruh dokumen adalah node dokumen Setiap elemen elemen HTML adalah node elemen Teks didalam elemen HTML adalah node-node teks Node-node dalam suatu pohon memiliki hubungan hirarki dengan yang lain. Kita menggunakan istilah parent , child dan sibling untuk menjelaskan hubungan ini: Dalam pohon node, node paling atas disebut dengan root (atau root node ) Setiap node memiliki tetap satu parent , kecuali root (yang tidak memiliki parent) Sutau node dapat memilik satu, beberapa atau tidak sama sekali children *Sibling adalah node-node dengan parent sama Serangkaian hubungan antra node ke node disebut dengan path Path-path dalam XPath didefinisikan dengan menggunakan slash ( / ) untuk memisahkan langkah/tahapan dalam rangkaian keterkaitan node, seperti URL-URLS atau direktori Unix Dalam XPath, semua ekspresi didasarkan pada context node . Context node adalah node path dari mana dimulai. Default context adalah node root , yang dinyatakan dengan slash tunggal (/), seperti dalam contoh diatas. Yang paling banyak digunakan untuk ekpresi patha adalah sebagai berikut: Expression Description nodename Memilih semua node dengan nama \u201cnodename\u201d / Garis miring tunggal awal menunjukkan pemilihan dari simpul akar, garis miring berikutnya menunjukkan pemilihan simpul anak dari simpul saat ini // Pilih node anak langsung dan tidak langsung dalam dokumen dari node saat ini - ini memberi kita kemampuan untuk \"skip levels\" . Piiih context node saat ini .. Pilih parent dari context node @ Pilih atribut dari context node [@attribute = 'value'] Pilihan node dengan nilai atribut tertentu text() Pilih isi teks dari node | Rantai ekspresi dan membawa kembali hasil dari ekspresi mana pun Sintaks XPath lanjutan \u00b6 Operators \u00b6 Operator digunakan untuk membandingkan node-node. Terdaoat operator matematika,operator boolean. Operators dapat memberikan nilai boolean (benar /salah) sebagai hasil. Disini beberap yang banyak digunakan : Operator Penjelasan = Membandingkan kesamaan, dapat digunakan untuk nilai numerik maupun teks != Dignuanak untuk membandikanketidak samaan >, >= Lebih besar, lebih besar dari atau sama dengan <, <= Lebih dari, lebih dari atau sama dengan or Boolean atau (or) and Boolean dan (and) not Boolean bukan (not) Contoh \u00b6 Ekspresi Path Hasil Ekspresi html/body/div/h3/ @id =\u2019exercises-2\u2019 Apakah exercise 2 ada? html/body/div/h3/ @id !=\u2019exercises-4\u2019 Apakah exercise 4 tidak ada? //h1/ @id =\u2019references\u2019 or @id =\u2019introduction\u2019 Apakah terdapat terdapat references h1 atau introduction? Predikat \u00b6 Predikat digunakan untuk menemukan node tertentu atau node yang berisi nilai tertentu Predikat selalu diletakkan dalam kurung siku, dimaksudkan untuk memberikan informasi pemfilteran tambahan untuk mengembalikan node. Anda dapat memfilter pada sebuah node dengan menggunakan operator atau fungsi Examples \u00b6 Operator Explanation [1] Pilih node pertama [last()] Pilih node yang terakhir [last()-1] Pilih node terakhir kedua [position()<3] Pilih dua node pertama, perhatikan posisi pertama dimulai dari 1, bukan = [@lang] Pilih node yang memiliki atribut \u2018lang\u2019 [@lang='en'] Pilih semua node yang memiliki atribute dengan nilai atribute \u201cen\u201d [price>15.00] Pilih semua node yang memiliki node price yang lebih besar dari 15.00 Pencarian dalam teks \u00b6 XPath dapat melakukan pencarian dalam teks mengunakan fungsi. Perhatikan: pencarian dalatem teks adalah case-sensitive! Path Expression Result //author[contains(.,\"Matt\")] Cocokkan pada semua node author, dalam node sekarang yang berisi Matt (case-sensitive) //author[starts-with(.,\"G\")] Cocokkan pada semua node author, dalam node sekarang yang dimulai dengan G (case-sensitive) //author[ends-with(.,\"w\")] Cocokkan pada semua dalam node sekarang yang yang berakhiran dengan w (case-sensitive) Menampilkan source dari suatu page \u00b6 Untuk menemukan xpath element halaman website maka kita dapat membuka source page halama tersebut. Yaitu dengan cara klik kanan pada halaman Memetakan suatu webpage denganXPath menggunakan console suatu browser \u00b6 KIta akan menggunakan kode HTML yang ada pada website portal tugas akhir mahasiswa sebagai contoh. Biasanya, setiap browser web dapat menampilkan sumber kode HTML dengan fungsinya tersendiri dari browser tersebut. Contoh diatas adalah menggunakan browser firefox dengan melakukan klik kanan pada pointer akan menampilakn submenu salah satunya View Page Source. Kemudian klik menu tersebut akand dapatkan source sumbernya sebagai berikut. <!DOCTYPE html> < html > < head > < meta charset = \"utf-8\" /> < title > Portal Tugas Akhir Univ. Trunojoyo </ title > < link rel = \"stylesheet\" href = \"https://pta.trunojoyo.ac.id/css/reset.css\" type = \"text/css\" /> < link rel = \"stylesheet\" href = \"https://pta.trunojoyo.ac.id/css/style.css\" type = \"text/css\" /> < link rel = \"stylesheet\" href = \"https://pta.trunojoyo.ac.id/css/960_12_col.css\" type = \"text/css\" /> < link rel = \"stylesheet\" media = \"screen\" href = \"https://pta.trunojoyo.ac.id/css/smoothness/jquery-ui-1.8.18.custom.css\" /> <!--[if lte IE 8]> <link rel=\"stylesheet\" type=\"text/css\" href=\"css/ie.css\" /> <![endif]--> <!-- favicon --> < link rel = \"shortcut icon\" type = \"image/x-icon\" href = \"https://pta.trunojoyo.ac.id/images/favicon.ico\" /> (...) </ body > </ html > Kita dapat melihat dari source code bahwa judul dari halama ini dalam elemen title yang ada dalam elemen head , yang ada didalam elemen html yang berisi seluruh isi dari page Sehingga jika kita ingin memberi tahu web scraper untuk mencari judul dari halaman ini, kita menggunakan informasi ini untuk menentukan path yang diperlukan untuk menjelajahi isi HTML dari page untuk menemukan element title . XPath memungkinkan untuk melakukan hal tersebut. Kita dapat menjalankan query XPath langsung pada browser dengan JavaScript console yang ada didalamnya. Menampilkan console dalam browser \u00b6 Pad Firefox, gunakan menut Tools > Web Developer > Web Console . PadaChrome, gunakan menut View > Developer > JavaScript Console Disini bagaimana console tampak dalam browser Firefox : Untuk saat ini, jangan khawatir terlalu banyak pesan error manakalan anda meliha dalam console ketika anda membukanya. Console akan dengan muncul prompt dengan karakter > (atau >> dalam Firefox) menunggu anda untuk mengetik perintah . Sintaks untuk menjalankan query XPath dalam JavaScript console adalah $x(\"XPATH_QUERY\") , misalkan: $x(\"/html/head/title/text()\") Ini akan menghasilkan sesuatu seperti ini <- Array [ #text \"Portal Tugas Akhir Univ. Trunojoyo\" ] Menggunkan extension firefox xPath Finder \u00b6 Untuk menentukan cpath dari element dari halam website dengan mudah dilakukan menggunakan xPath Finder. Jika belum ada ektension firefox xPath Finder ,silahkan install dulu extension tersebut. Penggunaannya adalah dengan cara klik pada icon extension curson akan berubah menjadi crosshair, kemudian pilih elemen yang diinginkan dengan kursor diarahkan pada elemen tersebut ( elemen akan berubah warna ). Klik elemen tersebut sehingga akan tampil xPath dari elemen tersebut di bawah kiri dari halaman In [ 3 ] : fetch ( 'https://pta.trunojoyo.ac.id/' ) 2020 -09-25 04 :32:42 [ scrapy.core.engine ] DEBUG: Crawled ( 200 ) <GET https://pta.t runojoyo.ac.id/> ( referer: None ) In [ 4 ] : response.xpath ( \"/html/body/div[2]/div[1]/div[2]/ul/li[1]/div[1]/a\" ) Out [ 4 ] : [ <Selector xpath = '/html/body/div[2]/div[1]/div[2]/ul/li[1]/div[1]/a' dat a = '<a class=\"title\" href=\"#\">Analisis Wa...' > ] Dari hasil scrap diatas ddapat diperbaiki ouputnya dengan perintah sebagai berikut In [ 5 ] : response.xpath ( \"//item/title/text()\" ) .extract_first () In [ 6 ] : response.xpath ( \"/html/body/div[2]/div[1]/div[2]/ul/li[1]/div[1]/a/text( ...: )\" ) .extract_first () Out [ 6 ] : 'Analisis Wacana Media Online Detik.com dalam Memberitakan Peristiwa Ker usuhan Mahasiswa Papua di Surabaya' Mari kita bahas query XPath yang digunakan pada contoh diatas dengan XPath /html/head/title/text() . Yang pertama / menyatakan root dari suatu dokumen. Dengan query tersebut , kita memberit tahu browser untuk / Start at the root of the document\u2026 html/ \u2026 mengarahkan ke node html \u2026 head/ \u2026 kemudaian ke node head yang ada didalamnya\u2026 title/ \u2026 kemudaian ke node title yang ada didalamnya\u2026 text() dan pilih node text yang terdapat dalam elemen itu Dengan menggunakan sintaks ini , XPath kemudian memungkinkan kita untuk menentukan path yang tepat pada suatu node. Memilih judul tugas akhir \u00b6 $x ( \"/html/body/div[2]/div[1]/div[2]/ul/li[1]/div[1]/a\" ) Menggunakan Scraper Chrome extension untuk menavigasi XPath \u00b6 Ini adalah alat yang memudahkan untuk mengimplementasikan XPath. Alat ini adalah tambahan dari browser Chrome untuk melakukan scrap data dari suatu website. Jika kita ingin menggunakan tool ini maka perlu ditambahkan pada browser Chrome kita seperti biasa kita menambahkan extension Chrome secara umum. Setelah kita install extension ini misalkan kita sedang menjelajahi pta.trunojoyo.ac.id, pada saat klik kanan pada website tersebut maka akan muncul menu Dan bila kita mengklik menu scraper similar maka akan memunculkan XPath : //div[2]/div[1]/div[2]/ul/li dengan data yang ditarik dari halam website tersebut. Pada view di samping Xpath judul, penulis, dosen pembimbing dan abstrak ditampilkan dalam satu kolom dengan nama kolom text. Untuk memperbaiki hasil tampilan ini, supaya menjadi field field yang sesuai, maka diperlukan perubahan dengan menambahkan nama kolom (field) dan memecah XPath yang sesuai. Berikut perubahan yang telah dilakukan 2.**Membuat Scrapy parser dengan Python ** (TUGAS) \u00b6 Persiapan \u00b6 instalasi alat alat yaitu Scrapy library pip install scrapy Desain secara umum scraping website \u00b6 Source Pembuatan Proyek \u00b6 Proyek 1. Menarik data harga dari website a. Membuat proyek bernama mytugaswebmining scrapy startproject mytugaswebmining Akan terbentuk struktur folder shopee seperti berikut \\---mytugaswebmining | scrapy.cfg | \\---mytugaswebmining | items.py | middlewares.py | pipelines.py | settings.py | __init__.py | \\---spiders __init__.py Scrapy merupakan framewerok aplikasi yang memungkinkan suatu suatu proyek sesuai dengan gaya pemrograman Object Oriented untuk mendefinisikan item-item dan spider untuk keseluruhan aplikasi. Struktur proyek dari scrapy yang telah dibuat diatas dengan masing masing sebagai berikut scrapy.cfg : ini adalah file konfigurasi proyek yang berisi modul pengaturan untuk proyek dan juga dengan informasi pengimplementasiannya. test_project : Ini adalah direktori aplikasi dengan banyak macam file yang benar bena bertanggung jawab untuk menjalankan dan menarik data dari urls-url web. items.py : item-item yang akan ditarik oleh scraper, ini juga bekerja seperti Python dicts . Karena kita dapat menggunakan plain Python dicts dengan Scrapy, Items menyediakan proteksi tambahan terhadap field-field yang tidak dideklarasikan. Hal itu dideklarasikan dengan pembuatan scrapy.Item class dan mendefinisika atribut atributnya sebagai scrapy.Field . pipelines.py : Setelah item di scrap oleh spider, item itu dikirim ke Item Pipeline yang memprosesnya dengan menggunakan beberapa komponen yang dijalankan secara berurutan. Masing masing item pipeline component adalah kelas Python yang harus mengimplementasikan metode yang disebut dengan process_item untuk memproses item yang di- scrap . Proses dilakukan terhadap item dengan memutuskan item harus diteruskan atau didrop dan tidak dinajutkan proses berikutnya. settings.py : Ini memungkinkan kita untuk mengkustom semua karakteristi komponen-komponen Scrapy termasuk core, extensions, pipelines dan spiders itu sediri. spiders : Spiders adalah direktori yang berisi semua spiders/crawlers seperti kelas-kelas Python . Ketika kita menjalankan atau meng-crawler spider maka scrapy mencari didalam di direktori ini dan mencoba untuk menemukan spider dengan namanya yang telah ditetapkan oleh pengguna. Spider mendefinisikan bagaimana site tertentu atau sekumpulan site, termasuk bagaimana melakukan crawl dan bagaimana mengektrak data dari page-pagenya. Artinya, Spiders adalah tempat dimana kita mendefinisikan tiga atribut utam yaitu start_urls yang meberi tahu URLs mana yang akan ditarik, , allowed_domains yaitu mendefinisikan domain mana saja yang boleh parse adalah suatu metode yang dipanggil ketika ada respon yang berasal dari lodged requests . Atribut ini adalah penting karena ini inti dari definisi Spider","title":"Scrapwebsite"},{"location":"scrapwebsite/#membangun-scraping-website","text":"Untuk melakukan tahapan penarikan data dari suatu halaman halaman web maka yang perlu dipehatikan adalah **Menganalisa ** strutktur HTML dari laman web Scraping adalah tentang menemukan pola dalam laman- laman web dan mengektraksi isi dari laman tersebut. Sebelum mulai untuk menulis tool scrapyng (scraper), kita perlu untuk memahami struktur HTML dari web page yang akan ditarik datanya dan mengidentifikasi pola didalamnya. Pola dapat terkait dengan penggunaan classes, id, and elemen elemen lain HTML. **Membuat Scrapy parser dengan python ** Setelah menganlisa struktur dari halaman web yang akan ditarik datanya, kita lakukan implementasi dengan menulis code untuk menghasilkan scrapy parser . Scrapy parser bertanggung jawab untuk menjelajahi web yang akan ditarik datanya dan mengektrak informasi sesuai dengan aturan yang dibuat Mengumpulkan dan menyimpan informasi Parser dapat menyimpan hasilnya sesuai dengan format yang anda inginkan misalkan csv atau json. Ini adalah hasil akhir dari data yang telah dikumpulkan","title":"Membangun Scraping Website ."},{"location":"scrapwebsite/#1analisa-struktur-html-halaman-website","text":"XPath (kepanjangan dari XML Path Language) adalah expression language yang digunakan untuk menetapkanbagian bagian bagian bagian dari dokumen XML . XPath digunakan dalam perangkat lunak dan bahasa yang digunakan untuk manipulasi dokumen XML, seperti XSLT, XQuery atau alat alat web scraping XPath dapat juga digunakan dalam struktur yang sama seperti XML, misalkan html","title":"1.Analisa Struktur HTML Halaman Website"},{"location":"scrapwebsite/#markup-languages","text":"XML dan HTML adalah markup languages . Artinya keduanya menggunakan sekumpulan tags atau rule untuk menyusun dan menyedian informasi yang ada didalamnya. Struktur ini membantu untuk secara otomatis untuk pemrosesan , pengeditan, pemformatan dan penampilan dan pencetakan informasi tersebut. Dokumen XML menyimpan data dalam format plain teks. Ini menyediakan perangkat lunan dan perangkat kerans dengan cara bebas untuk menyimpan, mentransformasi, dan menshare data. Format XML adalah format terbuka. Anda dapat membuka dokumen XML dalam editor teks dan data yang ada didalamnya dapat disajikan. Ini memungkinkan pertukran sistem yang tidak kompatibel dan mudah untuk mengkonversi suatu data. Dokumen XML memilki aturan-aturan dasar sebagai berikut: Dokumen XML disusun menggunakan nodes , yaitu terdiri dari node node elemen, node node atribute dan node node teks Node node elemen XML harus harus dibuka dan ditutup tag, misal. tag pembuka <catfood> dan tag penutup </catfood> Tag XML adalah sensitif e, misal. <catfood> tidak sama dengan <catFood> Elemen XML harus bertingkat dengan benar: <catfood> <manufacturer> Purina </manufacturer> <address> 12 Cat Way, Boise, Idaho, 21341 </address> <date> 2019-10-01 </date> </catfood> Node node teks (data) isinya didalam tag pembuka dan tag penutup Node note atribut XML berisi nilai-nilai yang harus di quoted, misal. <catfood type=\"basic\"></catfood>","title":"Markup Languages"},{"location":"scrapwebsite/#xpath-selalu-mengasumsikan-data-terstruktur-structured-data","text":"Now let\u2019s start using XPath.","title":"XPath selalu mengasumsikan data terstruktur (structured data)."},{"location":"scrapwebsite/#menelusuri-pohon-node-html-menggunakan-xpath","text":"Cara yang paling umum untuk merepresentasikan struktur dari dokumen XML atau HTML adalah dengan pohon node (node tree): Dalam sebuah dokumen HTML , segala sesuatunya adalah node: Seluruh dokumen adalah node dokumen Setiap elemen elemen HTML adalah node elemen Teks didalam elemen HTML adalah node-node teks Node-node dalam suatu pohon memiliki hubungan hirarki dengan yang lain. Kita menggunakan istilah parent , child dan sibling untuk menjelaskan hubungan ini: Dalam pohon node, node paling atas disebut dengan root (atau root node ) Setiap node memiliki tetap satu parent , kecuali root (yang tidak memiliki parent) Sutau node dapat memilik satu, beberapa atau tidak sama sekali children *Sibling adalah node-node dengan parent sama Serangkaian hubungan antra node ke node disebut dengan path Path-path dalam XPath didefinisikan dengan menggunakan slash ( / ) untuk memisahkan langkah/tahapan dalam rangkaian keterkaitan node, seperti URL-URLS atau direktori Unix Dalam XPath, semua ekspresi didasarkan pada context node . Context node adalah node path dari mana dimulai. Default context adalah node root , yang dinyatakan dengan slash tunggal (/), seperti dalam contoh diatas. Yang paling banyak digunakan untuk ekpresi patha adalah sebagai berikut: Expression Description nodename Memilih semua node dengan nama \u201cnodename\u201d / Garis miring tunggal awal menunjukkan pemilihan dari simpul akar, garis miring berikutnya menunjukkan pemilihan simpul anak dari simpul saat ini // Pilih node anak langsung dan tidak langsung dalam dokumen dari node saat ini - ini memberi kita kemampuan untuk \"skip levels\" . Piiih context node saat ini .. Pilih parent dari context node @ Pilih atribut dari context node [@attribute = 'value'] Pilihan node dengan nilai atribut tertentu text() Pilih isi teks dari node | Rantai ekspresi dan membawa kembali hasil dari ekspresi mana pun","title":"Menelusuri pohon node HTML menggunakan XPath"},{"location":"scrapwebsite/#sintaks-xpath-lanjutan","text":"","title":"Sintaks XPath lanjutan"},{"location":"scrapwebsite/#operators","text":"Operator digunakan untuk membandingkan node-node. Terdaoat operator matematika,operator boolean. Operators dapat memberikan nilai boolean (benar /salah) sebagai hasil. Disini beberap yang banyak digunakan : Operator Penjelasan = Membandingkan kesamaan, dapat digunakan untuk nilai numerik maupun teks != Dignuanak untuk membandikanketidak samaan >, >= Lebih besar, lebih besar dari atau sama dengan <, <= Lebih dari, lebih dari atau sama dengan or Boolean atau (or) and Boolean dan (and) not Boolean bukan (not)","title":"Operators"},{"location":"scrapwebsite/#contoh","text":"Ekspresi Path Hasil Ekspresi html/body/div/h3/ @id =\u2019exercises-2\u2019 Apakah exercise 2 ada? html/body/div/h3/ @id !=\u2019exercises-4\u2019 Apakah exercise 4 tidak ada? //h1/ @id =\u2019references\u2019 or @id =\u2019introduction\u2019 Apakah terdapat terdapat references h1 atau introduction?","title":"Contoh"},{"location":"scrapwebsite/#predikat","text":"Predikat digunakan untuk menemukan node tertentu atau node yang berisi nilai tertentu Predikat selalu diletakkan dalam kurung siku, dimaksudkan untuk memberikan informasi pemfilteran tambahan untuk mengembalikan node. Anda dapat memfilter pada sebuah node dengan menggunakan operator atau fungsi","title":"Predikat"},{"location":"scrapwebsite/#examples","text":"Operator Explanation [1] Pilih node pertama [last()] Pilih node yang terakhir [last()-1] Pilih node terakhir kedua [position()<3] Pilih dua node pertama, perhatikan posisi pertama dimulai dari 1, bukan = [@lang] Pilih node yang memiliki atribut \u2018lang\u2019 [@lang='en'] Pilih semua node yang memiliki atribute dengan nilai atribute \u201cen\u201d [price>15.00] Pilih semua node yang memiliki node price yang lebih besar dari 15.00","title":"Examples"},{"location":"scrapwebsite/#pencarian-dalam-teks","text":"XPath dapat melakukan pencarian dalam teks mengunakan fungsi. Perhatikan: pencarian dalatem teks adalah case-sensitive! Path Expression Result //author[contains(.,\"Matt\")] Cocokkan pada semua node author, dalam node sekarang yang berisi Matt (case-sensitive) //author[starts-with(.,\"G\")] Cocokkan pada semua node author, dalam node sekarang yang dimulai dengan G (case-sensitive) //author[ends-with(.,\"w\")] Cocokkan pada semua dalam node sekarang yang yang berakhiran dengan w (case-sensitive)","title":"Pencarian dalam teks"},{"location":"scrapwebsite/#menampilkan-source-dari-suatu-page","text":"Untuk menemukan xpath element halaman website maka kita dapat membuka source page halama tersebut. Yaitu dengan cara klik kanan pada halaman","title":"Menampilkan source dari suatu page"},{"location":"scrapwebsite/#memetakan-suatu-webpage-denganxpath-menggunakan-console-suatu-browser","text":"KIta akan menggunakan kode HTML yang ada pada website portal tugas akhir mahasiswa sebagai contoh. Biasanya, setiap browser web dapat menampilkan sumber kode HTML dengan fungsinya tersendiri dari browser tersebut. Contoh diatas adalah menggunakan browser firefox dengan melakukan klik kanan pada pointer akan menampilakn submenu salah satunya View Page Source. Kemudian klik menu tersebut akand dapatkan source sumbernya sebagai berikut. <!DOCTYPE html> < html > < head > < meta charset = \"utf-8\" /> < title > Portal Tugas Akhir Univ. Trunojoyo </ title > < link rel = \"stylesheet\" href = \"https://pta.trunojoyo.ac.id/css/reset.css\" type = \"text/css\" /> < link rel = \"stylesheet\" href = \"https://pta.trunojoyo.ac.id/css/style.css\" type = \"text/css\" /> < link rel = \"stylesheet\" href = \"https://pta.trunojoyo.ac.id/css/960_12_col.css\" type = \"text/css\" /> < link rel = \"stylesheet\" media = \"screen\" href = \"https://pta.trunojoyo.ac.id/css/smoothness/jquery-ui-1.8.18.custom.css\" /> <!--[if lte IE 8]> <link rel=\"stylesheet\" type=\"text/css\" href=\"css/ie.css\" /> <![endif]--> <!-- favicon --> < link rel = \"shortcut icon\" type = \"image/x-icon\" href = \"https://pta.trunojoyo.ac.id/images/favicon.ico\" /> (...) </ body > </ html > Kita dapat melihat dari source code bahwa judul dari halama ini dalam elemen title yang ada dalam elemen head , yang ada didalam elemen html yang berisi seluruh isi dari page Sehingga jika kita ingin memberi tahu web scraper untuk mencari judul dari halaman ini, kita menggunakan informasi ini untuk menentukan path yang diperlukan untuk menjelajahi isi HTML dari page untuk menemukan element title . XPath memungkinkan untuk melakukan hal tersebut. Kita dapat menjalankan query XPath langsung pada browser dengan JavaScript console yang ada didalamnya.","title":"Memetakan suatu webpage denganXPath menggunakan console suatu  browser"},{"location":"scrapwebsite/#menampilkan-console-dalam-browser","text":"Pad Firefox, gunakan menut Tools > Web Developer > Web Console . PadaChrome, gunakan menut View > Developer > JavaScript Console Disini bagaimana console tampak dalam browser Firefox : Untuk saat ini, jangan khawatir terlalu banyak pesan error manakalan anda meliha dalam console ketika anda membukanya. Console akan dengan muncul prompt dengan karakter > (atau >> dalam Firefox) menunggu anda untuk mengetik perintah . Sintaks untuk menjalankan query XPath dalam JavaScript console adalah $x(\"XPATH_QUERY\") , misalkan: $x(\"/html/head/title/text()\") Ini akan menghasilkan sesuatu seperti ini <- Array [ #text \"Portal Tugas Akhir Univ. Trunojoyo\" ]","title":"Menampilkan console dalam  browser"},{"location":"scrapwebsite/#menggunkan-extension-firefox-xpath-finder","text":"Untuk menentukan cpath dari element dari halam website dengan mudah dilakukan menggunakan xPath Finder. Jika belum ada ektension firefox xPath Finder ,silahkan install dulu extension tersebut. Penggunaannya adalah dengan cara klik pada icon extension curson akan berubah menjadi crosshair, kemudian pilih elemen yang diinginkan dengan kursor diarahkan pada elemen tersebut ( elemen akan berubah warna ). Klik elemen tersebut sehingga akan tampil xPath dari elemen tersebut di bawah kiri dari halaman In [ 3 ] : fetch ( 'https://pta.trunojoyo.ac.id/' ) 2020 -09-25 04 :32:42 [ scrapy.core.engine ] DEBUG: Crawled ( 200 ) <GET https://pta.t runojoyo.ac.id/> ( referer: None ) In [ 4 ] : response.xpath ( \"/html/body/div[2]/div[1]/div[2]/ul/li[1]/div[1]/a\" ) Out [ 4 ] : [ <Selector xpath = '/html/body/div[2]/div[1]/div[2]/ul/li[1]/div[1]/a' dat a = '<a class=\"title\" href=\"#\">Analisis Wa...' > ] Dari hasil scrap diatas ddapat diperbaiki ouputnya dengan perintah sebagai berikut In [ 5 ] : response.xpath ( \"//item/title/text()\" ) .extract_first () In [ 6 ] : response.xpath ( \"/html/body/div[2]/div[1]/div[2]/ul/li[1]/div[1]/a/text( ...: )\" ) .extract_first () Out [ 6 ] : 'Analisis Wacana Media Online Detik.com dalam Memberitakan Peristiwa Ker usuhan Mahasiswa Papua di Surabaya' Mari kita bahas query XPath yang digunakan pada contoh diatas dengan XPath /html/head/title/text() . Yang pertama / menyatakan root dari suatu dokumen. Dengan query tersebut , kita memberit tahu browser untuk / Start at the root of the document\u2026 html/ \u2026 mengarahkan ke node html \u2026 head/ \u2026 kemudaian ke node head yang ada didalamnya\u2026 title/ \u2026 kemudaian ke node title yang ada didalamnya\u2026 text() dan pilih node text yang terdapat dalam elemen itu Dengan menggunakan sintaks ini , XPath kemudian memungkinkan kita untuk menentukan path yang tepat pada suatu node.","title":"Menggunkan extension firefox xPath Finder"},{"location":"scrapwebsite/#memilih-judul-tugas-akhir","text":"$x ( \"/html/body/div[2]/div[1]/div[2]/ul/li[1]/div[1]/a\" )","title":"Memilih judul tugas akhir"},{"location":"scrapwebsite/#menggunakan-scraper-chrome-extension-untuk-menavigasi-xpath","text":"Ini adalah alat yang memudahkan untuk mengimplementasikan XPath. Alat ini adalah tambahan dari browser Chrome untuk melakukan scrap data dari suatu website. Jika kita ingin menggunakan tool ini maka perlu ditambahkan pada browser Chrome kita seperti biasa kita menambahkan extension Chrome secara umum. Setelah kita install extension ini misalkan kita sedang menjelajahi pta.trunojoyo.ac.id, pada saat klik kanan pada website tersebut maka akan muncul menu Dan bila kita mengklik menu scraper similar maka akan memunculkan XPath : //div[2]/div[1]/div[2]/ul/li dengan data yang ditarik dari halam website tersebut. Pada view di samping Xpath judul, penulis, dosen pembimbing dan abstrak ditampilkan dalam satu kolom dengan nama kolom text. Untuk memperbaiki hasil tampilan ini, supaya menjadi field field yang sesuai, maka diperlukan perubahan dengan menambahkan nama kolom (field) dan memecah XPath yang sesuai. Berikut perubahan yang telah dilakukan","title":"Menggunakan  Scraper Chrome extension untuk menavigasi XPath"},{"location":"scrapwebsite/#2membuat-scrapy-parser-dengan-python-tugas","text":"","title":"2.**Membuat Scrapy parser dengan Python ** (TUGAS)"},{"location":"scrapwebsite/#persiapan","text":"instalasi alat alat yaitu Scrapy library pip install scrapy","title":"Persiapan"},{"location":"scrapwebsite/#desain-secara-umum-scraping-website","text":"Source","title":"Desain secara umum scraping website"},{"location":"scrapwebsite/#pembuatan-proyek","text":"Proyek 1. Menarik data harga dari website a. Membuat proyek bernama mytugaswebmining scrapy startproject mytugaswebmining Akan terbentuk struktur folder shopee seperti berikut \\---mytugaswebmining | scrapy.cfg | \\---mytugaswebmining | items.py | middlewares.py | pipelines.py | settings.py | __init__.py | \\---spiders __init__.py Scrapy merupakan framewerok aplikasi yang memungkinkan suatu suatu proyek sesuai dengan gaya pemrograman Object Oriented untuk mendefinisikan item-item dan spider untuk keseluruhan aplikasi. Struktur proyek dari scrapy yang telah dibuat diatas dengan masing masing sebagai berikut scrapy.cfg : ini adalah file konfigurasi proyek yang berisi modul pengaturan untuk proyek dan juga dengan informasi pengimplementasiannya. test_project : Ini adalah direktori aplikasi dengan banyak macam file yang benar bena bertanggung jawab untuk menjalankan dan menarik data dari urls-url web. items.py : item-item yang akan ditarik oleh scraper, ini juga bekerja seperti Python dicts . Karena kita dapat menggunakan plain Python dicts dengan Scrapy, Items menyediakan proteksi tambahan terhadap field-field yang tidak dideklarasikan. Hal itu dideklarasikan dengan pembuatan scrapy.Item class dan mendefinisika atribut atributnya sebagai scrapy.Field . pipelines.py : Setelah item di scrap oleh spider, item itu dikirim ke Item Pipeline yang memprosesnya dengan menggunakan beberapa komponen yang dijalankan secara berurutan. Masing masing item pipeline component adalah kelas Python yang harus mengimplementasikan metode yang disebut dengan process_item untuk memproses item yang di- scrap . Proses dilakukan terhadap item dengan memutuskan item harus diteruskan atau didrop dan tidak dinajutkan proses berikutnya. settings.py : Ini memungkinkan kita untuk mengkustom semua karakteristi komponen-komponen Scrapy termasuk core, extensions, pipelines dan spiders itu sediri. spiders : Spiders adalah direktori yang berisi semua spiders/crawlers seperti kelas-kelas Python . Ketika kita menjalankan atau meng-crawler spider maka scrapy mencari didalam di direktori ini dan mencoba untuk menemukan spider dengan namanya yang telah ditetapkan oleh pengguna. Spider mendefinisikan bagaimana site tertentu atau sekumpulan site, termasuk bagaimana melakukan crawl dan bagaimana mengektrak data dari page-pagenya. Artinya, Spiders adalah tempat dimana kita mendefinisikan tiga atribut utam yaitu start_urls yang meberi tahu URLs mana yang akan ditarik, , allowed_domains yaitu mendefinisikan domain mana saja yang boleh parse adalah suatu metode yang dipanggil ketika ada respon yang berasal dari lodged requests . Atribut ini adalah penting karena ini inti dari definisi Spider","title":"Pembuatan Proyek"},{"location":"specimen/","text":"Specimen \u00b6 Body copy \u00b6 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Cras arcu libero, mollis sed massa vel, ornare viverra ex . Mauris a ullamcorper lacus. Nullam urna elit, malesuada eget finibus ut, ullamcorper ac tortor. Vestibulum sodales pulvinar nisl, pharetra aliquet est. Quisque volutpat erat ac nisi accumsan tempor. Sed suscipit , orci non pretium pretium, quam mi gravida metus, vel venenatis justo est condimentum diam. Maecenas non ornare justo. Nam a ipsum eros. Nulla aliquam orci sit amet nisl posuere malesuada. Proin aliquet nulla velit, quis ultricies orci feugiat et. Ut tincidunt sollicitudin tincidunt. Aenean ullamcorper sit amet nulla at interdum. Headings \u00b6 The 3 rd level \u00b6 The 4 th level \u00b6 The 5 th level \u00b6 The 6 th level \u00b6 Headings with secondary text \u00b6 The 3 rd level with secondary text \u00b6 The 4 th level with secondary text \u00b6 The 5 th level with secondary text \u00b6 The 6 th level with secondary text \u00b6 Blockquotes \u00b6 Morbi eget dapibus felis. Vivamus venenatis porttitor tortor sit amet rutrum. Pellentesque aliquet quam enim, eu volutpat urna rutrum a. Nam vehicula nunc mauris, a ultricies libero efficitur sed. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Sed molestie imperdiet consectetur. Blockquote nesting \u00b6 Sed aliquet , neque at rutrum mollis, neque nisi tincidunt nibh, vitae faucibus lacus nunc at lacus. Nunc scelerisque, quam id cursus sodales, lorem libero fermentum urna, ut efficitur elit ligula et nunc. Mauris dictum mi lacus, sit amet pellentesque urna vehicula fringilla. Ut sit amet placerat ante. Proin sed elementum nulla. Nunc vitae sem odio. Suspendisse ac eros arcu. Vivamus orci erat, volutpat a tempor et, rutrum. eu odio. Suspendisse rutrum facilisis risus , eu posuere neque commodo a. Interdum et malesuada fames ac ante ipsum primis in faucibus. Sed nec leo bibendum, sodales mauris ut, tincidunt massa. Other content blocks \u00b6 Vestibulum vitae orci quis ante viverra ultricies ut eget turpis. Sed eu lectus dapibus, eleifend nulla varius, lobortis turpis. In ac hendrerit nisl, sit amet laoreet nibh. var _extends = function ( target ) { for ( var i = 1 ; i < arguments . length ; i ++ ) { var source = arguments [ i ]; for ( var key in source ) { target [ key ] = source [ key ]; } } return target ; }; Praesent at return target , sodales nibh vel, tempor felis. Fusce vel lacinia lacus. Suspendisse rhoncus nunc non nisi iaculis ultrices. Donec consectetur mauris non neque imperdiet, eget volutpat libero. Lists \u00b6 Unordered lists \u00b6 Sed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus tellus non sem sollicitudin, quis rutrum leo facilisis. Nulla tempor lobortis orci, at elementum urna sodales vitae. In in vehicula nulla, quis ornare libero. Duis mollis est eget nibh volutpat, fermentum aliquet dui mollis. Nam vulputate tincidunt fringilla. Nullam dignissim ultrices urna non auctor. Aliquam metus eros, pretium sed nulla venenatis, faucibus auctor ex. Proin ut eros sed sapien ullamcorper consequat. Nunc ligula ante, fringilla at aliquam ac, aliquet sed mauris. Nulla et rhoncus turpis. Mauris ultricies elementum leo. Duis efficitur accumsan nibh eu mattis. Vivamus tempus velit eros, porttitor placerat nibh lacinia sed. Aenean in finibus diam. Ordered lists \u00b6 Integer vehicula feugiat magna, a mollis tellus. Nam mollis ex ante, quis elementum eros tempor rutrum. Aenean efficitur lobortis lacinia. Nulla consectetur feugiat sodales. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Aliquam ornare feugiat quam et egestas. Nunc id erat et quam pellentesque lacinia eu vel odio. Vivamus venenatis porttitor tortor sit amet rutrum. Pellentesque aliquet quam enim, eu volutpat urna rutrum a. Nam vehicula nunc mauris, a ultricies libero efficitur sed. Mauris dictum mi lacus Ut sit amet placerat ante Suspendisse ac eros arcu Morbi eget dapibus felis. Vivamus venenatis porttitor tortor sit amet rutrum. Pellentesque aliquet quam enim, eu volutpat urna rutrum a. Sed aliquet, neque at rutrum mollis, neque nisi tincidunt nibh. Pellentesque eget var _extends ornare tellus, ut gravida mi. var _extends = function ( target ) { for ( var i = 1 ; i < arguments . length ; i ++ ) { var source = arguments [ i ]; for ( var key in source ) { target [ key ] = source [ key ]; } } return target ; }; Vivamus id mi enim. Integer id turpis sapien. Ut condimentum lobortis sagittis. Aliquam purus tellus, faucibus eget urna at, iaculis venenatis nulla. Vivamus a pharetra leo. Definition lists \u00b6 Lorem ipsum dolor sit amet Sed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus tellus non sem sollicitudin, quis rutrum leo facilisis. Nulla tempor lobortis orci, at elementum urna sodales vitae. In in vehicula nulla. Duis mollis est eget nibh volutpat, fermentum aliquet dui mollis. Nam vulputate tincidunt fringilla. Nullam dignissim ultrices urna non auctor. Cras arcu libero Aliquam metus eros, pretium sed nulla venenatis, faucibus auctor ex. Proin ut eros sed sapien ullamcorper consequat. Nunc ligula ante, fringilla at aliquam ac, aliquet sed mauris. Code blocks \u00b6 Inline \u00b6 Morbi eget dapibus felis . Vivamus venenatis porttitor tortor sit amet rutrum. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Pellentesque aliquet quam enim , eu volutpat urna rutrum a. Nam vehicula nunc return target mauris, a ultricies libero efficitur sed. Sed molestie imperdiet consectetur. Vivamus a pharetra leo. Pellentesque eget ornare tellus, ut gravida mi. Fusce vel lacinia lacus. Listing \u00b6 1 2 3 4 5 6 7 8 9 var _extends = function ( target ) { for ( var i = 1 ; i < arguments . length ; i ++ ) { var source = arguments [ i ]; for ( var key in source ) { target [ key ] = source [ key ]; } } return target ; }; Horizontal rules \u00b6 Aenean in finibus diam. Duis mollis est eget nibh volutpat, fermentum aliquet dui mollis. Nam vulputate tincidunt fringilla. Nullam dignissim ultrices urna non auctor. Integer vehicula feugiat magna, a mollis tellus. Nam mollis ex ante, quis elementum eros tempor rutrum. Aenean efficitur lobortis lacinia. Nulla consectetur feugiat sodales. Data tables \u00b6 Sollicitudo / Pellentesi consectetur adipiscing elit arcu sed Vivamus a pharetra yes yes yes yes yes Ornare viverra ex yes yes yes yes yes Mauris a ullamcorper yes yes partial yes yes Nullam urna elit yes yes yes yes yes Malesuada eget finibus yes yes yes yes yes Ullamcorper yes yes yes yes yes Vestibulum sodales yes - yes - yes Pulvinar nisl yes yes yes - - Pharetra aliquet est yes yes yes yes yes Sed suscipit yes yes yes yes yes Orci non pretium yes partial - - - Sed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus tellus non sem sollicitudin, quis rutrum leo facilisis. Nulla tempor lobortis orci, at elementum urna sodales vitae. In in vehicula nulla, quis ornare libero. Left Center Right Lorem dolor amet ipsum sit Vestibulum vitae orci quis ante viverra ultricies ut eget turpis. Sed eu lectus dapibus, eleifend nulla varius, lobortis turpis. In ac hendrerit nisl, sit amet laoreet nibh. Table with colgroups (Pandoc) Lorem ipsum dolor sit amet. Sed sagittis eleifend rutrum. Donec vitae suscipit est.","title":"Specimen"},{"location":"specimen/#specimen","text":"","title":"Specimen"},{"location":"specimen/#body-copy","text":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Cras arcu libero, mollis sed massa vel, ornare viverra ex . Mauris a ullamcorper lacus. Nullam urna elit, malesuada eget finibus ut, ullamcorper ac tortor. Vestibulum sodales pulvinar nisl, pharetra aliquet est. Quisque volutpat erat ac nisi accumsan tempor. Sed suscipit , orci non pretium pretium, quam mi gravida metus, vel venenatis justo est condimentum diam. Maecenas non ornare justo. Nam a ipsum eros. Nulla aliquam orci sit amet nisl posuere malesuada. Proin aliquet nulla velit, quis ultricies orci feugiat et. Ut tincidunt sollicitudin tincidunt. Aenean ullamcorper sit amet nulla at interdum.","title":"Body copy"},{"location":"specimen/#headings","text":"","title":"Headings"},{"location":"specimen/#the-3rd-level","text":"","title":"The 3rd level"},{"location":"specimen/#the-4th-level","text":"","title":"The 4th level"},{"location":"specimen/#the-5th-level","text":"","title":"The 5th level"},{"location":"specimen/#the-6th-level","text":"","title":"The 6th level"},{"location":"specimen/#headings-with-secondary-text","text":"","title":"Headings with secondary text"},{"location":"specimen/#the-3rd-level-with-secondary-text","text":"","title":"The 3rd level with secondary text"},{"location":"specimen/#the-4th-level-with-secondary-text","text":"","title":"The 4th level with secondary text"},{"location":"specimen/#the-5th-level-with-secondary-text","text":"","title":"The 5th level with secondary text"},{"location":"specimen/#the-6th-level-with-secondary-text","text":"","title":"The 6th level with secondary text"},{"location":"specimen/#blockquotes","text":"Morbi eget dapibus felis. Vivamus venenatis porttitor tortor sit amet rutrum. Pellentesque aliquet quam enim, eu volutpat urna rutrum a. Nam vehicula nunc mauris, a ultricies libero efficitur sed. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Sed molestie imperdiet consectetur.","title":"Blockquotes"},{"location":"specimen/#blockquote-nesting","text":"Sed aliquet , neque at rutrum mollis, neque nisi tincidunt nibh, vitae faucibus lacus nunc at lacus. Nunc scelerisque, quam id cursus sodales, lorem libero fermentum urna, ut efficitur elit ligula et nunc. Mauris dictum mi lacus, sit amet pellentesque urna vehicula fringilla. Ut sit amet placerat ante. Proin sed elementum nulla. Nunc vitae sem odio. Suspendisse ac eros arcu. Vivamus orci erat, volutpat a tempor et, rutrum. eu odio. Suspendisse rutrum facilisis risus , eu posuere neque commodo a. Interdum et malesuada fames ac ante ipsum primis in faucibus. Sed nec leo bibendum, sodales mauris ut, tincidunt massa.","title":"Blockquote nesting"},{"location":"specimen/#other-content-blocks","text":"Vestibulum vitae orci quis ante viverra ultricies ut eget turpis. Sed eu lectus dapibus, eleifend nulla varius, lobortis turpis. In ac hendrerit nisl, sit amet laoreet nibh. var _extends = function ( target ) { for ( var i = 1 ; i < arguments . length ; i ++ ) { var source = arguments [ i ]; for ( var key in source ) { target [ key ] = source [ key ]; } } return target ; }; Praesent at return target , sodales nibh vel, tempor felis. Fusce vel lacinia lacus. Suspendisse rhoncus nunc non nisi iaculis ultrices. Donec consectetur mauris non neque imperdiet, eget volutpat libero.","title":"Other content blocks"},{"location":"specimen/#lists","text":"","title":"Lists"},{"location":"specimen/#unordered-lists","text":"Sed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus tellus non sem sollicitudin, quis rutrum leo facilisis. Nulla tempor lobortis orci, at elementum urna sodales vitae. In in vehicula nulla, quis ornare libero. Duis mollis est eget nibh volutpat, fermentum aliquet dui mollis. Nam vulputate tincidunt fringilla. Nullam dignissim ultrices urna non auctor. Aliquam metus eros, pretium sed nulla venenatis, faucibus auctor ex. Proin ut eros sed sapien ullamcorper consequat. Nunc ligula ante, fringilla at aliquam ac, aliquet sed mauris. Nulla et rhoncus turpis. Mauris ultricies elementum leo. Duis efficitur accumsan nibh eu mattis. Vivamus tempus velit eros, porttitor placerat nibh lacinia sed. Aenean in finibus diam.","title":"Unordered lists"},{"location":"specimen/#ordered-lists","text":"Integer vehicula feugiat magna, a mollis tellus. Nam mollis ex ante, quis elementum eros tempor rutrum. Aenean efficitur lobortis lacinia. Nulla consectetur feugiat sodales. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Aliquam ornare feugiat quam et egestas. Nunc id erat et quam pellentesque lacinia eu vel odio. Vivamus venenatis porttitor tortor sit amet rutrum. Pellentesque aliquet quam enim, eu volutpat urna rutrum a. Nam vehicula nunc mauris, a ultricies libero efficitur sed. Mauris dictum mi lacus Ut sit amet placerat ante Suspendisse ac eros arcu Morbi eget dapibus felis. Vivamus venenatis porttitor tortor sit amet rutrum. Pellentesque aliquet quam enim, eu volutpat urna rutrum a. Sed aliquet, neque at rutrum mollis, neque nisi tincidunt nibh. Pellentesque eget var _extends ornare tellus, ut gravida mi. var _extends = function ( target ) { for ( var i = 1 ; i < arguments . length ; i ++ ) { var source = arguments [ i ]; for ( var key in source ) { target [ key ] = source [ key ]; } } return target ; }; Vivamus id mi enim. Integer id turpis sapien. Ut condimentum lobortis sagittis. Aliquam purus tellus, faucibus eget urna at, iaculis venenatis nulla. Vivamus a pharetra leo.","title":"Ordered lists"},{"location":"specimen/#definition-lists","text":"Lorem ipsum dolor sit amet Sed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus tellus non sem sollicitudin, quis rutrum leo facilisis. Nulla tempor lobortis orci, at elementum urna sodales vitae. In in vehicula nulla. Duis mollis est eget nibh volutpat, fermentum aliquet dui mollis. Nam vulputate tincidunt fringilla. Nullam dignissim ultrices urna non auctor. Cras arcu libero Aliquam metus eros, pretium sed nulla venenatis, faucibus auctor ex. Proin ut eros sed sapien ullamcorper consequat. Nunc ligula ante, fringilla at aliquam ac, aliquet sed mauris.","title":"Definition lists"},{"location":"specimen/#code-blocks","text":"","title":"Code blocks"},{"location":"specimen/#inline","text":"Morbi eget dapibus felis . Vivamus venenatis porttitor tortor sit amet rutrum. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Pellentesque aliquet quam enim , eu volutpat urna rutrum a. Nam vehicula nunc return target mauris, a ultricies libero efficitur sed. Sed molestie imperdiet consectetur. Vivamus a pharetra leo. Pellentesque eget ornare tellus, ut gravida mi. Fusce vel lacinia lacus.","title":"Inline"},{"location":"specimen/#listing","text":"1 2 3 4 5 6 7 8 9 var _extends = function ( target ) { for ( var i = 1 ; i < arguments . length ; i ++ ) { var source = arguments [ i ]; for ( var key in source ) { target [ key ] = source [ key ]; } } return target ; };","title":"Listing"},{"location":"specimen/#horizontal-rules","text":"Aenean in finibus diam. Duis mollis est eget nibh volutpat, fermentum aliquet dui mollis. Nam vulputate tincidunt fringilla. Nullam dignissim ultrices urna non auctor. Integer vehicula feugiat magna, a mollis tellus. Nam mollis ex ante, quis elementum eros tempor rutrum. Aenean efficitur lobortis lacinia. Nulla consectetur feugiat sodales.","title":"Horizontal rules"},{"location":"specimen/#data-tables","text":"Sollicitudo / Pellentesi consectetur adipiscing elit arcu sed Vivamus a pharetra yes yes yes yes yes Ornare viverra ex yes yes yes yes yes Mauris a ullamcorper yes yes partial yes yes Nullam urna elit yes yes yes yes yes Malesuada eget finibus yes yes yes yes yes Ullamcorper yes yes yes yes yes Vestibulum sodales yes - yes - yes Pulvinar nisl yes yes yes - - Pharetra aliquet est yes yes yes yes yes Sed suscipit yes yes yes yes yes Orci non pretium yes partial - - - Sed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus tellus non sem sollicitudin, quis rutrum leo facilisis. Nulla tempor lobortis orci, at elementum urna sodales vitae. In in vehicula nulla, quis ornare libero. Left Center Right Lorem dolor amet ipsum sit Vestibulum vitae orci quis ante viverra ultricies ut eget turpis. Sed eu lectus dapibus, eleifend nulla varius, lobortis turpis. In ac hendrerit nisl, sit amet laoreet nibh. Table with colgroups (Pandoc) Lorem ipsum dolor sit amet. Sed sagittis eleifend rutrum. Donec vitae suscipit est.","title":"Data tables"},{"location":"webscrape/","text":"Dalam materi ini, kita akan membahan topik \u00b6 Pengantar bidang web scraping Menjelaskan tantangan legalitas web scraping Apa itu web scraping \u00b6 Web scraping adalah proses mengektrak data dari website. Selanjutnya data yang tersedia di website dinyatakan dalam bentuk format yang dapat dengan mudah dioleh oleh mesin untuk dianalisa lebih lanjut. Misalkan data dinyatakan dalam bentuk format CSV, disimpan dalam database tertentu dan sebagainya. Data yang ada di website itu bentuknya tidak terstrukur, artinya tidak siap digunakan untuk analisis. Ada beberapa cara scrape data dari website untuk diektrak informasinya untuk digunakan. Bentuk yang paling sederhana, adalah dengan menyalin dan mempast bagian bagian tersebut dari website. TEntunya ini tidak praktis dilakukan jika banyak data yang yang akan diektrak, atau tersebar dibeberapa website. Sehingga diperlukan tool khusus dan teknik khusus yang digunakan untuk melakukan secara otomatis proses ini, dengan menetapkan webiste yang akan dijelajahi informasi apa yang akan dicari dan apakah ektraksi data berhenti diakhir halama yang ditemukan ataukah mengikuti hyperlink dan mengulangi proses secara rekursif. Proses automatis dari web scraping juga memungkinkan untuk dilakukan apakah proses akan dijalankan pada rentang waktu tertentu dan menangkap perubahan yang terjadi dari data. Teknik Webscraping membutuhkan pemahaman teknologi yang digunakan untuk menampilkan inforamasi pada web. Oleh karena itu diperlukan pemahaman tentang HTML dan Document Object Model (DOM) termasuk pemahana sintak XPath untuk memilih elemen pada website untuk Apa web scraping diperlukan \u00b6 Untuk web indexing yang digunakan oleh mesin pencari misal Google untuk menganalisa secara masal web untuk membangun indeknya. Memonitoring perubahan data e-commerse secara online untuk tujuan pemasaran. Seandainya kita telah memiliki toko penjualan sepatu dan ingin untuk melacak terus harga pesaing kita. Kita dapat mengunjungi website pesaing kita setiap hari untuk membandingkan dengan masing masing harga sepatu yang kita miliki. Akan tetapi ini akan butuh waktu banyak dan tidak layak dilakukan jika kita menjual ribuan sepatu atau dibutuhkan untuk mengecek perubahan harga lebih sering. Ini adalah pekerjaan yang tidak efisien dan efektif . Oleh karena itu kita perlu mesin atomatis dengan teknik web scraping untuk menggantikan proses manual tersebut Web scraping juga banyak digunakan oleh is also increasingly being used by scholars to create data sets for text mining projects; these might be collections of journal articles or digitised texts. The practice of data journalism , in particular, relies on the ability of investigative journalists to harvest data that is not always presented or published in a form that allows analysis. Apakah web scraping legal? \u00b6 Jika data yang ditarik (scrap) keperluan sendiri, dalam prakteknya tidak masalah. Akan tetapi jika data akan dipublikasikan lagi maka ada beberapa aspek hukum yang harus dipertimbangkan. Secara singkat bahwa legalitas tidaknya kita melakukan web scraping atau abu abunya tergantung pada tiga hal ini: Bentuk data yang akan discrap Bagaiman anda merencanakan untuk menggunakan data yang di-scrap Bagaiman anda mengektrak data dari website Nomor 1 dan 2 sangat jelas sehingga kita akan memulainya daru sini sebelum membahas nomor 3. Apa saja dari bentuk data yang ilegal untuk di scrap \u00b6 Data seperi e-commerce, personal atau data article , adalah bentuk data yang sangat terkait dengan legalitas. Dua bentuk data yang kita perlu hati-hati atau dikwatirkan legalitasnya adalah Personal Data Copyrighted Data JIka data yang anda tarik adalah bukan bentuk data dari diatas, secara umum adalah legal Bentuk data 1: Personal Data \u00b6 Personal data adalah data yang digunakan secara langsung atau tidak langsung untuk mengidentifksi individu tertentu. Bentuk dari data personal yaitu : Name Email Phone Number Address User Name IP Address Date of Birth Employment Info Bank or Credit Card Info Medical Data Biometric Data Bentuk data #2 : Copyrighted Data \u00b6 Bentuk data yang perlu hati hati untuk ditarik adalah copyrighted data. Copyrighted data data yang dimiliki oleh pribadi atau bisnis untuk mengendalikan terkait reproduksi dan kepemilikan. Seperti gambar dan lagu walaupun data dipublikasi di internet bukan berarti itu legal untuk ditarik tanpa pemberitahuan pemiliknya. Secara umum data tersebut adalah: Articles Videos Pictures Stories Music Databases Mengekstrak data berhak cipta bukanlah tindakan ilegal, jadi tindakan legal dan ilegal benar-benar tergantung pada bagaimana Anda berencana menggunakan data hak cipta tersebut setelah Anda menyalin /men-scrap data tersebut. Juga aspek database data, artinya jika menarik seluruh database dari web dan kemudian merepoduksi ulang untuk tujuan sendiri. Amerika dan EU memiliki regulasi yang berbeda terkait aspek ini. https://www.scraperapi.com/blog/is-web-scraping-legal/","title":"Webscrape"},{"location":"webscrape/#dalam-materi-ini-kita-akan-membahan-topik","text":"Pengantar bidang web scraping Menjelaskan tantangan legalitas web scraping","title":"Dalam materi ini, kita akan membahan topik"},{"location":"webscrape/#apa-itu-web-scraping","text":"Web scraping adalah proses mengektrak data dari website. Selanjutnya data yang tersedia di website dinyatakan dalam bentuk format yang dapat dengan mudah dioleh oleh mesin untuk dianalisa lebih lanjut. Misalkan data dinyatakan dalam bentuk format CSV, disimpan dalam database tertentu dan sebagainya. Data yang ada di website itu bentuknya tidak terstrukur, artinya tidak siap digunakan untuk analisis. Ada beberapa cara scrape data dari website untuk diektrak informasinya untuk digunakan. Bentuk yang paling sederhana, adalah dengan menyalin dan mempast bagian bagian tersebut dari website. TEntunya ini tidak praktis dilakukan jika banyak data yang yang akan diektrak, atau tersebar dibeberapa website. Sehingga diperlukan tool khusus dan teknik khusus yang digunakan untuk melakukan secara otomatis proses ini, dengan menetapkan webiste yang akan dijelajahi informasi apa yang akan dicari dan apakah ektraksi data berhenti diakhir halama yang ditemukan ataukah mengikuti hyperlink dan mengulangi proses secara rekursif. Proses automatis dari web scraping juga memungkinkan untuk dilakukan apakah proses akan dijalankan pada rentang waktu tertentu dan menangkap perubahan yang terjadi dari data. Teknik Webscraping membutuhkan pemahaman teknologi yang digunakan untuk menampilkan inforamasi pada web. Oleh karena itu diperlukan pemahaman tentang HTML dan Document Object Model (DOM) termasuk pemahana sintak XPath untuk memilih elemen pada website","title":"Apa itu web scraping"},{"location":"webscrape/#untuk-apa-web-scraping-diperlukan","text":"Untuk web indexing yang digunakan oleh mesin pencari misal Google untuk menganalisa secara masal web untuk membangun indeknya. Memonitoring perubahan data e-commerse secara online untuk tujuan pemasaran. Seandainya kita telah memiliki toko penjualan sepatu dan ingin untuk melacak terus harga pesaing kita. Kita dapat mengunjungi website pesaing kita setiap hari untuk membandingkan dengan masing masing harga sepatu yang kita miliki. Akan tetapi ini akan butuh waktu banyak dan tidak layak dilakukan jika kita menjual ribuan sepatu atau dibutuhkan untuk mengecek perubahan harga lebih sering. Ini adalah pekerjaan yang tidak efisien dan efektif . Oleh karena itu kita perlu mesin atomatis dengan teknik web scraping untuk menggantikan proses manual tersebut Web scraping juga banyak digunakan oleh is also increasingly being used by scholars to create data sets for text mining projects; these might be collections of journal articles or digitised texts. The practice of data journalism , in particular, relies on the ability of investigative journalists to harvest data that is not always presented or published in a form that allows analysis.","title":"untuk Apa web scraping diperlukan"},{"location":"webscrape/#apakah-web-scraping-legal","text":"Jika data yang ditarik (scrap) keperluan sendiri, dalam prakteknya tidak masalah. Akan tetapi jika data akan dipublikasikan lagi maka ada beberapa aspek hukum yang harus dipertimbangkan. Secara singkat bahwa legalitas tidaknya kita melakukan web scraping atau abu abunya tergantung pada tiga hal ini: Bentuk data yang akan discrap Bagaiman anda merencanakan untuk menggunakan data yang di-scrap Bagaiman anda mengektrak data dari website Nomor 1 dan 2 sangat jelas sehingga kita akan memulainya daru sini sebelum membahas nomor 3.","title":"Apakah web scraping legal?"},{"location":"webscrape/#apa-saja-dari-bentuk-data-yang-ilegal-untuk-di-scrap","text":"Data seperi e-commerce, personal atau data article , adalah bentuk data yang sangat terkait dengan legalitas. Dua bentuk data yang kita perlu hati-hati atau dikwatirkan legalitasnya adalah Personal Data Copyrighted Data JIka data yang anda tarik adalah bukan bentuk data dari diatas, secara umum adalah legal","title":"Apa saja dari bentuk data yang ilegal untuk di scrap"},{"location":"webscrape/#bentuk-data-1-personal-data","text":"Personal data adalah data yang digunakan secara langsung atau tidak langsung untuk mengidentifksi individu tertentu. Bentuk dari data personal yaitu : Name Email Phone Number Address User Name IP Address Date of Birth Employment Info Bank or Credit Card Info Medical Data Biometric Data","title":"Bentuk data 1: Personal Data"},{"location":"webscrape/#bentuk-data-2-copyrighted-data","text":"Bentuk data yang perlu hati hati untuk ditarik adalah copyrighted data. Copyrighted data data yang dimiliki oleh pribadi atau bisnis untuk mengendalikan terkait reproduksi dan kepemilikan. Seperti gambar dan lagu walaupun data dipublikasi di internet bukan berarti itu legal untuk ditarik tanpa pemberitahuan pemiliknya. Secara umum data tersebut adalah: Articles Videos Pictures Stories Music Databases Mengekstrak data berhak cipta bukanlah tindakan ilegal, jadi tindakan legal dan ilegal benar-benar tergantung pada bagaimana Anda berencana menggunakan data hak cipta tersebut setelah Anda menyalin /men-scrap data tersebut. Juga aspek database data, artinya jika menarik seluruh database dari web dan kemudian merepoduksi ulang untuk tujuan sendiri. Amerika dan EU memiliki regulasi yang berbeda terkait aspek ini. https://www.scraperapi.com/blog/is-web-scraping-legal/","title":"Bentuk data  #2: Copyrighted Data"},{"location":"extensions/admonition/","text":"Pengantar Data Science \u00b6 Tujuan Pembelajaran 1. Memahmai dan menggunakan konsep dasar dan teknologi data science 2. Memahami proses data science dengan menggunakan - teknik statistik - matematika, - kecerdasan buatan, - machine learning untuk mengekstraksi dan mengidenti\ufb01kasi informasi yang bermanfaat dan pengetahuan yang terkait dari berbagai database besar","title":"Pengantar Data Science"},{"location":"extensions/admonition/#pengantar-data-science","text":"Tujuan Pembelajaran 1. Memahmai dan menggunakan konsep dasar dan teknologi data science 2. Memahami proses data science dengan menggunakan - teknik statistik - matematika, - kecerdasan buatan, - machine learning untuk mengekstraksi dan mengidenti\ufb01kasi informasi yang bermanfaat dan pengetahuan yang terkait dari berbagai database besar","title":"Pengantar Data Science"},{"location":"extensions/codehilite/","text":"CodeHilite \u00b6 CodeHilite is an extension that adds syntax highlighting to code blocks and is included in the standard Markdown library. The highlighting process is executed during compilation of the Markdown file. Syntax highlighting not working? Please ensure that Pygments is installed. See the next section for further directions on how to set up Pygments or use the official Docker image with all dependencies pre-installed. Installation \u00b6 CodeHilite parses code blocks and wraps them in pre tags. If Pygments is installed, which is a generic syntax highlighter with support for over 300 languages , CodeHilite will also highlight the code block. Pygments can be installed with the following command: pip install pygments To enable CodeHilite, add the following lines to your mkdocs.yml : markdown_extensions : - codehilite Usage \u00b6 Specifying the language \u00b6 The CodeHilite extension uses the same syntax as regular Markdown code blocks, but needs to know the language of the code block. This can be done in three different ways. via Markdown syntax recommended \u00b6 In Markdown, code blocks can be opened and closed by writing three backticks on separate lines. To add code highlighting to those blocks, the easiest way is to specify the language directly after the opening block. Example: ``` python import tensorflow as tf ``` Result: import tensorflow as tf via Shebang \u00b6 Alternatively, if the first line of a code block contains a shebang, the language is derived from the path referenced in the shebang. This will only work for code blocks that are indented using four spaces, not for those encapsulated in three backticks. Example: #!/usr/bin/python import tensorflow as tf Result: #!/usr/bin/python import tensorflow as tf via three colons \u00b6 If the first line starts with three colons followed by a language identifier, the first line is stripped. This will only work for code blocks that are indented using four spaces, not for those encapsulated in three backticks. Example: :::python import tensorflow as tf Result: import tensorflow as tf Adding line numbers \u00b6 Line numbers can be added by enabling the linenums flag in your mkdocs.yml : markdown_extensions : - codehilite : linenums : true Example: ``` python \"\"\" Bubble sort \"\"\" def bubble_sort(items): for i in range(len(items)): for j in range(len(items) - 1 - i): if items[j] > items[j + 1]: items[j], items[j + 1] = items[j + 1], items[j] ``` Result: 1 2 3 4 5 6 \"\"\" Bubble sort \"\"\" def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] Grouping code blocks \u00b6 The SuperFences extension which is part of the PyMdown Extensions package adds support for grouping code blocks with tabs. This is especially useful for documenting projects with multiple language bindings. Example: ``` bash tab=\"Bash\" #!/bin/bash echo \"Hello world!\" ``` ``` c tab=\"C\" #include <stdio.h> int main(void) { printf(\"Hello world!\\n\"); } ``` ``` c++ tab=\"C++\" #include <iostream> int main() { std::cout << \"Hello world!\" << std::endl; return 0; } ``` ``` c# tab=\"C#\" using System; class Program { static void Main(string[] args) { Console.WriteLine(\"Hello world!\"); } } ``` Result: ``` bash tab=\"Bash\" !/bin/bash \u00b6 echo \"Hello world!\" ``` c tab=\"C\" #include <stdio.h> int main(void) { printf(\"Hello world!\\n\"); } ``` c++ tab=\"C++\" include \u00b6 int main() { std::cout << \"Hello world!\" << std::endl; return 0; } ``` c# tab=\"C#\" using System; class Program { static void Main(string[] args) { Console.WriteLine(\"Hello world!\"); } } Highlighting specific lines \u00b6 Specific lines can be highlighted by passing the line numbers to the hl_lines argument placed right after the language identifier. Line counts start at 1. Example: ``` python hl_lines=\"3 4\" \"\"\" Bubble sort \"\"\" def bubble_sort(items): for i in range(len(items)): for j in range(len(items) - 1 - i): if items[j] > items[j + 1]: items[j], items[j + 1] = items[j + 1], items[j] ``` Result: 1 2 3 4 5 6 \"\"\" Bubble sort \"\"\" def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] Supported languages excerpt \u00b6 CodeHilite uses Pygments , a generic syntax highlighter with support for over 300 languages , so the following list of examples is just an excerpt. Bash \u00b6 #!/bin/bash for OPT in \" $@ \" do case \" $OPT \" in '-f' ) canonicalize = 1 ;; '-n' ) switchlf = \"-n\" ;; esac done # readlink -f function __readlink_f { target = \" $1 \" while test -n \" $target \" ; do filepath = \" $target \" cd ` dirname \" $filepath \" ` target = ` readlink \" $filepath \" ` done /bin/echo $switchlf ` pwd -P ` / ` basename \" $filepath \" ` } if [ ! \" $canonicalize \" ] ; then readlink $switchlf \" $@ \" else for file in \" $@ \" do case \" $file \" in -* ) ;; * ) __readlink_f \" $file \" ;; esac done fi exit $? C \u00b6 extern size_t pb_varint_scan ( const uint8_t data [], size_t left ) { assert ( data && left ); left = left > 10 ? 10 : left ; #ifdef __SSE2__ /* Mapping: remaining bytes ==> bitmask */ static const int mask_map [] = { 0x0000 , 0x0001 , 0x0003 , 0x0007 , 0x000F , 0x001F , 0x003F , 0x007F , 0x00FF , 0x01FF , 0x03FF }; /* Load buffer into 128-bit integer and create high-bit mask */ __m128i temp = _mm_loadu_si128 (( const __m128i * ) data ); __m128i high = _mm_set1_epi8 ( 0x80 ); /* Intersect and extract mask with high-bits set */ int mask = _mm_movemask_epi8 ( _mm_and_si128 ( temp , high )); mask = ( mask & mask_map [ left ]) ^ mask_map [ left ]; /* Count trailing zeroes */ return mask ? __builtin_ctz ( mask ) + 1 : 0 ; #else /* Linear scan */ size_t size = 0 ; while ( data [ size ++ ] & 0x80 ) if ( !-- left ) return 0 ; return size ; #endif /* __SSE2__ */ } C++ \u00b6 Extension :: Extension ( const Descriptor * descriptor , const Descriptor * scope ) : descriptor_ ( descriptor ), scope_ ( scope ) { /* Extract full name for signature */ variables_ [ \"signature\" ] = descriptor_ -> full_name (); /* Prepare message symbol */ variables_ [ \"message\" ] = StringReplace ( variables_ [ \"signature\" ], \".\" , \"_\" , true ); LowerString ( & ( variables_ [ \"message\" ])); /* Suffix scope to identifiers, if given */ string suffix ( \"\" ); if ( scope_ ) { suffix = scope_ -> full_name (); /* Check if the base and extension types are in the same package */ if ( ! scope_ -> file () -> package (). compare ( descriptor_ -> file () -> package ())) suffix = StripPrefixString ( suffix , scope_ -> file () -> package () + \".\" ); /* Append to signature */ variables_ [ \"signature\" ] += \".[\" + suffix + \"]\" ; suffix = \"_\" + suffix ; } /* Prepare extension symbol */ variables_ [ \"extension\" ] = StringReplace ( suffix , \".\" , \"_\" , true ); LowerString ( & ( variables_ [ \"extension\" ])); } C# \u00b6 public static void Send ( Socket socket , byte [] buffer , int offset , int size , int timeout ) { int startTickCount = Environment . TickCount ; int sent = 0 ; do { if ( Environment . TickCount > startTickCount + timeout ) throw new Exception ( \"Timeout.\" ); try { sent += socket . Send ( buffer , offset + sent , size - sent , SocketFlags . None ); } catch ( SocketException ex ) { if ( ex . SocketErrorCode == SocketError . WouldBlock || ex . SocketErrorCode == SocketError . IOPending || ex . SocketErrorCode == SocketError . NoBufferSpaceAvailable ) { /* Socket buffer is probably full, wait and try again */ Thread . Sleep ( 30 ); } else { throw ex ; } } } while ( sent < size ); } Clojure \u00b6 ( clojure-version ) ( defn partition-when [ f ] ( fn [ rf ] ( let [ a ( java.util.ArrayList. ) fval ( volatile! false )] ( fn ([] ( rf )) ([ result ] ( let [ result ( if ( .isEmpty a ) result ( let [ v ( vec ( .toArray a ))] ;; Clear first ( .clear a ) ( unreduced ( rf result v ))))] ( rf result ))) ([ result input ] ( if-not ( and ( f input ) @ fval ) ( do ( vreset! fval true ) ( .add a input ) result ) ( let [ v ( vec ( .toArray a ))] ( .clear a ) ( let [ ret ( rf result v )] ( when-not ( reduced? ret ) ( .add a input )) ret )))))))) ( into [] ( partition-when # ( .startsWith % \">>\" )) [ \"1d\" \"33\" \">> 1\" \">> 2\" \"22\" \">> 3\" ]) Diff \u00b6 Index: grunt.js =================================================================== --- grunt.js (revision 31200) +++ grunt.js (working copy) @@ -12,6 +12,7 @@ module.exports = function (grunt) { + console.log('hello world'); // Project configuration. grunt.initConfig({ lint: { @@ -19,10 +20,6 @@ 'packages/services.web/{!(test)/**/,}*.js', 'packages/error/**/*.js' ], - scripts: [ - 'grunt.js', - 'db/**/*.js' - ], browser: [ 'packages/web/server.js', 'packages/web/server/**/*.js', Docker \u00b6 FROM ubuntu # Install vnc, xvfb in order to create a 'fake' display and firefox RUN apt-get update && apt-get install -y x11vnc xvfb firefox RUN mkdir ~/.vnc # Setup a password RUN x11vnc -storepasswd 1234 ~/.vnc/passwd # Autostart firefox (might not be the best way, but it does the trick) RUN bash -c 'echo \"firefox\" >> /.bashrc' EXPOSE 5900 CMD [ \"x11vnc\" , \"-forever\" , \"-usepw\" , \"-create\" ] Elixir \u00b6 require Logger def accept ( port ) do { :ok , socket } = :gen_tcp . listen ( port , [ :binary , packet : :line , active : false , reuseaddr : true ]) Logger . info \"Accepting connections on port #{ port } \" loop_acceptor ( socket ) end defp loop_acceptor ( socket ) do { :ok , client } = :gen_tcp . accept ( socket ) serve ( client ) loop_acceptor ( socket ) end defp serve ( socket ) do socket |> read_line () |> write_line ( socket ) serve ( socket ) end defp read_line ( socket ) do { :ok , data } = :gen_tcp . recv ( socket , 0 ) data end defp write_line ( line , socket ) do :gen_tcp . send ( socket , line ) end Erlang \u00b6 circular ( Defs ) -> [ { { Type , Base }, Fields } || { { Type , Base }, Fields } <- Defs , Type == msg , circular ( Base , Defs ) ]. circular ( Base , Defs ) -> Fields = proplists : get_value ({ msg , Base }, Defs ), circular ( Defs , Fields , [ Base ]). circular (_ Defs , [], _ Path ) -> false ; circular ( Defs , [ Field | Fields ], Path ) -> case Field #field.type of { msg , Type } -> case lists : member ( Type , Path ) of false -> Children = proplists : get_value ({ msg , Type }, Defs ), case circular ( Defs , Children , [ Type | Path ]) of false -> circular ( Defs , Fields , Path ); true -> true end ; true -> Type == lists : last ( Path ) andalso ( length ( Path ) == 1 orelse not is_tree ( Path )) end ; _ -> circular ( Defs , Fields , Path ) end . F# \u00b6 /// Asynchronously download retangles from the server /// and decode the JSON format to F# Rectangle record let [< Js >] getRectangles () : Async < Rectangle [] > = async { let req = XMLHttpRequest () req . Open ( \"POST\" , \"/get\" , true ) let! resp = req . AsyncSend () return JSON . parse ( resp ) } /// Repeatedly update rectangles after 0.5 sec let [< Js >] updateLoop () = async { while true do do ! Async . Sleep ( 500 ) let! rects = getRectangles () cleanRectangles () rects |> Array . iter createRectangle } Go \u00b6 package main import \"fmt\" func counter ( id int , channel chan int , closer bool ) { for i := 0 ; i < 10000000 ; i ++ { fmt . Println ( \"process\" , id , \" send\" , i ) channel <- 1 } if closer { close ( channel ) } } func main () { channel := make ( chan int ) go counter ( 1 , channel , false ) go counter ( 2 , channel , true ) x := 0 // receiving data from channel for i := range channel { fmt . Println ( \"receiving\" ) x += i } fmt . Println ( x ) } HTML \u00b6 <!doctype html> < html class = \"no-js\" lang = \"\" > < head > < meta charset = \"utf-8\" > < meta http-equiv = \"x-ua-compatible\" content = \"ie=edge\" > < title > HTML5 Boilerplate </ title > < meta name = \"description\" content = \"\" > < meta name = \"viewport\" content = \"width=device-width, initial-scale=1\" > < link rel = \"apple-touch-icon\" href = \"apple-touch-icon.png\" > < link rel = \"stylesheet\" href = \"css/normalize.css\" > < link rel = \"stylesheet\" href = \"css/main.css\" > < script src = \"js/vendor/modernizr-2.8.3.min.js\" ></ script > </ head > < body > < p > Hello world! This is HTML5 Boilerplate. </ p > </ body > </ html > Java \u00b6 import java.util.LinkedList ; import java.lang.reflect.Array ; public class UnsortedHashSet < E > { private static final double LOAD_FACTOR_LIMIT = 0.7 ; private int size ; private LinkedList < E >[] con ; public UnsortedHashSet () { con = ( LinkedList < E >[] )( new LinkedList [ 10 ] ); } public boolean add ( E obj ) { int oldSize = size ; int index = Math . abs ( obj . hashCode ()) % con . length ; if ( con [ index ] == null ) con [ index ] = new LinkedList < E > (); if ( ! con [ index ] . contains ( obj )) { con [ index ] . add ( obj ); size ++ ; } if ( 1.0 * size / con . length > LOAD_FACTOR_LIMIT ) resize (); return oldSize != size ; } private void resize () { UnsortedHashSet < E > temp = new UnsortedHashSet < E > (); temp . con = ( LinkedList < E >[] )( new LinkedList [ con . length * 2 + 1 ] ); for ( int i = 0 ; i < con . length ; i ++ ) { if ( con [ i ] != null ) for ( E e : con [ i ] ) temp . add ( e ); } con = temp . con ; } public int size () { return size ; } } JavaScript \u00b6 var Math = require ( 'lib/math' ); var _extends = function ( target ) { for ( var i = 1 ; i < arguments . length ; i ++ ) { var source = arguments [ i ]; for ( var key in source ) { target [ key ] = source [ key ]; } } return target ; }; var e = exports . e = 2.71828182846 ; exports [ 'default' ] = function ( x ) { return Math . exp ( x ); }; module . exports = _extends ( exports [ 'default' ], exports ); JSON \u00b6 { \"name\" : \"mkdocs-material\" , \"version\" : \"0.2.4\" , \"description\" : \"A Material Design theme for MkDocs\" , \"homepage\" : \"http://squidfunk.github.io/mkdocs-material/\" , \"authors\" : [ \"squidfunk <martin.donath@squidfunk.com>\" ], \"license\" : \"MIT\" , \"main\" : \"Gulpfile.js\" , \"scripts\" : { \"start\" : \"./node_modules/.bin/gulp watch --mkdocs\" , \"build\" : \"./node_modules/.bin/gulp build --production\" } ... } Julia \u00b6 using MXNet mlp = @mx . chain mx . Variable ( : data ) => mx . FullyConnected ( name =: fc1 , num_hidden = 128 ) => mx . Activation ( name =: relu1 , act_type =: relu ) => mx . FullyConnected ( name =: fc2 , num_hidden = 64 ) => mx . Activation ( name =: relu2 , act_type =: relu ) => mx . FullyConnected ( name =: fc3 , num_hidden = 10 ) => mx . SoftmaxOutput ( name =: softmax ) # data provider batch_size = 100 include ( Pkg . dir ( \"MXNet\" , \"examples\" , \"mnist\" , \"mnist-data.jl\" )) train_provider , eval_provider = get_mnist_providers ( batch_size ) # setup model model = mx . FeedForward ( mlp , context = mx . cpu ()) # optimization algorithm optimizer = mx . SGD ( lr = 0.1 , momentum = 0.9 ) # fit parameters mx . fit ( model , optimizer , train_provider , n_epoch = 20 , eval_data = eval_provider ) Lua \u00b6 local ffi = require ( \"ffi\" ) ffi . cdef [[ void Sleep(int ms); int poll(struct pollfd *fds, unsigned long nfds, int timeout); ]] local sleep if ffi . os == \"Windows\" then function sleep ( s ) ffi . C . Sleep ( s * 1000 ) end else function sleep ( s ) ffi . C . poll ( nil , 0 , s * 1000 ) end end for i = 1 , 160 do io.write ( \".\" ); io.flush () sleep ( 0.01 ) end io.write ( \" \\n \" ) MySQL \u00b6 SELECT Employees . EmployeeID , Employees . Name , Employees . Salary , Manager . Name AS Manager FROM Employees LEFT JOIN Employees AS Manager ON Employees . ManagerID = Manager . EmployeeID WHERE Employees . EmployeeID = '087652' ; PHP \u00b6 <?php // src/AppBundle/Controller/LuckyController.php namespace AppBundle\\Controller ; use Sensio\\Bundle\\FrameworkExtraBundle\\Configuration\\Route ; use Symfony\\Component\\HttpFoundation\\Response ; class LuckyController { /** * @Route(\"/lucky/number\") */ public function numberAction () { $number = mt_rand ( 0 , 100 ); return new Response ( '<html><body>Lucky number: ' . $number . '</body></html>' ); } } Protocol Buffers \u00b6 syntax = \"proto2\" ; package caffe ; // Specifies the shape (dimensions) of a Blob. message BlobShape { repeated int64 dim = 1 [ packed = true ]; } message BlobProto { optional BlobShape shape = 7 ; repeated float data = 5 [ packed = true ]; repeated float diff = 6 [ packed = true ]; // 4D dimensions -- deprecated. Use \"shape\" instead. optional int32 num = 1 [ default = 0 ]; optional int32 channels = 2 [ default = 0 ]; optional int32 height = 3 [ default = 0 ]; optional int32 width = 4 [ default = 0 ]; } Python \u00b6 \"\"\" A very simple MNIST classifier. See extensive documentation at http://tensorflow.org/tutorials/mnist/beginners/index.md \"\"\" from __future__ import absolute_import from __future__ import division from __future__ import print_function # Import data from tensorflow.examples.tutorials.mnist import input_data import tensorflow as tf flags = tf . app . flags FLAGS = flags . FLAGS flags . DEFINE_string ( 'data_dir' , '/tmp/data/' , 'Directory for storing data' ) mnist = input_data . read_data_sets ( FLAGS . data_dir , one_hot = True ) sess = tf . InteractiveSession () # Create the model x = tf . placeholder ( tf . float32 , [ None , 784 ]) W = tf . Variable ( tf . zeros ([ 784 , 10 ])) b = tf . Variable ( tf . zeros ([ 10 ])) y = tf . nn . softmax ( tf . matmul ( x , W ) + b ) Ruby \u00b6 require 'finity/event' require 'finity/machine' require 'finity/state' require 'finity/transition' require 'finity/version' module Finity class InvalidCallback < StandardError ; end class MissingCallback < StandardError ; end class InvalidState < StandardError ; end # Class methods to be injected into the including class upon inclusion. module ClassMethods # Instantiate a new state machine for the including class by accepting a # block with state and event (and subsequent transition) definitions. def finity options = {}, & block @finity ||= Machine . new self , options , & block end # Return the names of all registered states. def states @finity . states . map { | name , _ | name } end # Return the names of all registered events. def events @finity . events . map { | name , _ | name } end end # Inject methods into the including class upon inclusion. def self . included base base . extend ClassMethods end end XML \u00b6 <?xml version=\"1.0\" encoding=\"UTF-8\"?> <!DOCTYPE mainTag SYSTEM \"some.dtd\" [ENTITY % entity]> <?oxygen RNGSchema=\"some.rng\" type=\"xml\"?> <xs:main-Tag xmlns:xs= \"http://www.w3.org/2001/XMLSchema\" > <!-- This is a sample comment --> <childTag attribute= \"Quoted Value\" another-attribute= 'Single quoted value' a-third-attribute= '123' > <withTextContent> Some text content </withTextContent> <withEntityContent> Some text content with &lt; entities &gt; and mentioning uint8_t and int32_t </withEntityContent> <otherTag attribute= 'Single quoted Value' /> </childTag> <![CDATA[ some CData ]]> </main-Tag>","title":"CodeHilite"},{"location":"extensions/codehilite/#codehilite","text":"CodeHilite is an extension that adds syntax highlighting to code blocks and is included in the standard Markdown library. The highlighting process is executed during compilation of the Markdown file. Syntax highlighting not working? Please ensure that Pygments is installed. See the next section for further directions on how to set up Pygments or use the official Docker image with all dependencies pre-installed.","title":"CodeHilite"},{"location":"extensions/codehilite/#installation","text":"CodeHilite parses code blocks and wraps them in pre tags. If Pygments is installed, which is a generic syntax highlighter with support for over 300 languages , CodeHilite will also highlight the code block. Pygments can be installed with the following command: pip install pygments To enable CodeHilite, add the following lines to your mkdocs.yml : markdown_extensions : - codehilite","title":"Installation"},{"location":"extensions/codehilite/#usage","text":"","title":"Usage"},{"location":"extensions/codehilite/#specifying-the-language","text":"The CodeHilite extension uses the same syntax as regular Markdown code blocks, but needs to know the language of the code block. This can be done in three different ways.","title":"Specifying the language"},{"location":"extensions/codehilite/#via-markdown-syntax-recommended","text":"In Markdown, code blocks can be opened and closed by writing three backticks on separate lines. To add code highlighting to those blocks, the easiest way is to specify the language directly after the opening block. Example: ``` python import tensorflow as tf ``` Result: import tensorflow as tf","title":"via Markdown syntax recommended"},{"location":"extensions/codehilite/#via-shebang","text":"Alternatively, if the first line of a code block contains a shebang, the language is derived from the path referenced in the shebang. This will only work for code blocks that are indented using four spaces, not for those encapsulated in three backticks. Example: #!/usr/bin/python import tensorflow as tf Result: #!/usr/bin/python import tensorflow as tf","title":"via Shebang"},{"location":"extensions/codehilite/#via-three-colons","text":"If the first line starts with three colons followed by a language identifier, the first line is stripped. This will only work for code blocks that are indented using four spaces, not for those encapsulated in three backticks. Example: :::python import tensorflow as tf Result: import tensorflow as tf","title":"via three colons"},{"location":"extensions/codehilite/#adding-line-numbers","text":"Line numbers can be added by enabling the linenums flag in your mkdocs.yml : markdown_extensions : - codehilite : linenums : true Example: ``` python \"\"\" Bubble sort \"\"\" def bubble_sort(items): for i in range(len(items)): for j in range(len(items) - 1 - i): if items[j] > items[j + 1]: items[j], items[j + 1] = items[j + 1], items[j] ``` Result: 1 2 3 4 5 6 \"\"\" Bubble sort \"\"\" def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ]","title":"Adding line numbers"},{"location":"extensions/codehilite/#grouping-code-blocks","text":"The SuperFences extension which is part of the PyMdown Extensions package adds support for grouping code blocks with tabs. This is especially useful for documenting projects with multiple language bindings. Example: ``` bash tab=\"Bash\" #!/bin/bash echo \"Hello world!\" ``` ``` c tab=\"C\" #include <stdio.h> int main(void) { printf(\"Hello world!\\n\"); } ``` ``` c++ tab=\"C++\" #include <iostream> int main() { std::cout << \"Hello world!\" << std::endl; return 0; } ``` ``` c# tab=\"C#\" using System; class Program { static void Main(string[] args) { Console.WriteLine(\"Hello world!\"); } } ``` Result: ``` bash tab=\"Bash\"","title":"Grouping code blocks"},{"location":"extensions/codehilite/#binbash","text":"echo \"Hello world!\" ``` c tab=\"C\" #include <stdio.h> int main(void) { printf(\"Hello world!\\n\"); } ``` c++ tab=\"C++\"","title":"!/bin/bash"},{"location":"extensions/codehilite/#include","text":"int main() { std::cout << \"Hello world!\" << std::endl; return 0; } ``` c# tab=\"C#\" using System; class Program { static void Main(string[] args) { Console.WriteLine(\"Hello world!\"); } }","title":"include "},{"location":"extensions/codehilite/#highlighting-specific-lines","text":"Specific lines can be highlighted by passing the line numbers to the hl_lines argument placed right after the language identifier. Line counts start at 1. Example: ``` python hl_lines=\"3 4\" \"\"\" Bubble sort \"\"\" def bubble_sort(items): for i in range(len(items)): for j in range(len(items) - 1 - i): if items[j] > items[j + 1]: items[j], items[j + 1] = items[j + 1], items[j] ``` Result: 1 2 3 4 5 6 \"\"\" Bubble sort \"\"\" def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ]","title":"Highlighting specific lines"},{"location":"extensions/codehilite/#supported-languages-excerpt","text":"CodeHilite uses Pygments , a generic syntax highlighter with support for over 300 languages , so the following list of examples is just an excerpt.","title":"Supported languages excerpt"},{"location":"extensions/codehilite/#bash","text":"#!/bin/bash for OPT in \" $@ \" do case \" $OPT \" in '-f' ) canonicalize = 1 ;; '-n' ) switchlf = \"-n\" ;; esac done # readlink -f function __readlink_f { target = \" $1 \" while test -n \" $target \" ; do filepath = \" $target \" cd ` dirname \" $filepath \" ` target = ` readlink \" $filepath \" ` done /bin/echo $switchlf ` pwd -P ` / ` basename \" $filepath \" ` } if [ ! \" $canonicalize \" ] ; then readlink $switchlf \" $@ \" else for file in \" $@ \" do case \" $file \" in -* ) ;; * ) __readlink_f \" $file \" ;; esac done fi exit $?","title":"Bash"},{"location":"extensions/codehilite/#c","text":"extern size_t pb_varint_scan ( const uint8_t data [], size_t left ) { assert ( data && left ); left = left > 10 ? 10 : left ; #ifdef __SSE2__ /* Mapping: remaining bytes ==> bitmask */ static const int mask_map [] = { 0x0000 , 0x0001 , 0x0003 , 0x0007 , 0x000F , 0x001F , 0x003F , 0x007F , 0x00FF , 0x01FF , 0x03FF }; /* Load buffer into 128-bit integer and create high-bit mask */ __m128i temp = _mm_loadu_si128 (( const __m128i * ) data ); __m128i high = _mm_set1_epi8 ( 0x80 ); /* Intersect and extract mask with high-bits set */ int mask = _mm_movemask_epi8 ( _mm_and_si128 ( temp , high )); mask = ( mask & mask_map [ left ]) ^ mask_map [ left ]; /* Count trailing zeroes */ return mask ? __builtin_ctz ( mask ) + 1 : 0 ; #else /* Linear scan */ size_t size = 0 ; while ( data [ size ++ ] & 0x80 ) if ( !-- left ) return 0 ; return size ; #endif /* __SSE2__ */ }","title":"C"},{"location":"extensions/codehilite/#c_1","text":"Extension :: Extension ( const Descriptor * descriptor , const Descriptor * scope ) : descriptor_ ( descriptor ), scope_ ( scope ) { /* Extract full name for signature */ variables_ [ \"signature\" ] = descriptor_ -> full_name (); /* Prepare message symbol */ variables_ [ \"message\" ] = StringReplace ( variables_ [ \"signature\" ], \".\" , \"_\" , true ); LowerString ( & ( variables_ [ \"message\" ])); /* Suffix scope to identifiers, if given */ string suffix ( \"\" ); if ( scope_ ) { suffix = scope_ -> full_name (); /* Check if the base and extension types are in the same package */ if ( ! scope_ -> file () -> package (). compare ( descriptor_ -> file () -> package ())) suffix = StripPrefixString ( suffix , scope_ -> file () -> package () + \".\" ); /* Append to signature */ variables_ [ \"signature\" ] += \".[\" + suffix + \"]\" ; suffix = \"_\" + suffix ; } /* Prepare extension symbol */ variables_ [ \"extension\" ] = StringReplace ( suffix , \".\" , \"_\" , true ); LowerString ( & ( variables_ [ \"extension\" ])); }","title":"C++"},{"location":"extensions/codehilite/#c_2","text":"public static void Send ( Socket socket , byte [] buffer , int offset , int size , int timeout ) { int startTickCount = Environment . TickCount ; int sent = 0 ; do { if ( Environment . TickCount > startTickCount + timeout ) throw new Exception ( \"Timeout.\" ); try { sent += socket . Send ( buffer , offset + sent , size - sent , SocketFlags . None ); } catch ( SocketException ex ) { if ( ex . SocketErrorCode == SocketError . WouldBlock || ex . SocketErrorCode == SocketError . IOPending || ex . SocketErrorCode == SocketError . NoBufferSpaceAvailable ) { /* Socket buffer is probably full, wait and try again */ Thread . Sleep ( 30 ); } else { throw ex ; } } } while ( sent < size ); }","title":"C&#35;"},{"location":"extensions/codehilite/#clojure","text":"( clojure-version ) ( defn partition-when [ f ] ( fn [ rf ] ( let [ a ( java.util.ArrayList. ) fval ( volatile! false )] ( fn ([] ( rf )) ([ result ] ( let [ result ( if ( .isEmpty a ) result ( let [ v ( vec ( .toArray a ))] ;; Clear first ( .clear a ) ( unreduced ( rf result v ))))] ( rf result ))) ([ result input ] ( if-not ( and ( f input ) @ fval ) ( do ( vreset! fval true ) ( .add a input ) result ) ( let [ v ( vec ( .toArray a ))] ( .clear a ) ( let [ ret ( rf result v )] ( when-not ( reduced? ret ) ( .add a input )) ret )))))))) ( into [] ( partition-when # ( .startsWith % \">>\" )) [ \"1d\" \"33\" \">> 1\" \">> 2\" \"22\" \">> 3\" ])","title":"Clojure"},{"location":"extensions/codehilite/#diff","text":"Index: grunt.js =================================================================== --- grunt.js (revision 31200) +++ grunt.js (working copy) @@ -12,6 +12,7 @@ module.exports = function (grunt) { + console.log('hello world'); // Project configuration. grunt.initConfig({ lint: { @@ -19,10 +20,6 @@ 'packages/services.web/{!(test)/**/,}*.js', 'packages/error/**/*.js' ], - scripts: [ - 'grunt.js', - 'db/**/*.js' - ], browser: [ 'packages/web/server.js', 'packages/web/server/**/*.js',","title":"Diff"},{"location":"extensions/codehilite/#docker","text":"FROM ubuntu # Install vnc, xvfb in order to create a 'fake' display and firefox RUN apt-get update && apt-get install -y x11vnc xvfb firefox RUN mkdir ~/.vnc # Setup a password RUN x11vnc -storepasswd 1234 ~/.vnc/passwd # Autostart firefox (might not be the best way, but it does the trick) RUN bash -c 'echo \"firefox\" >> /.bashrc' EXPOSE 5900 CMD [ \"x11vnc\" , \"-forever\" , \"-usepw\" , \"-create\" ]","title":"Docker"},{"location":"extensions/codehilite/#elixir","text":"require Logger def accept ( port ) do { :ok , socket } = :gen_tcp . listen ( port , [ :binary , packet : :line , active : false , reuseaddr : true ]) Logger . info \"Accepting connections on port #{ port } \" loop_acceptor ( socket ) end defp loop_acceptor ( socket ) do { :ok , client } = :gen_tcp . accept ( socket ) serve ( client ) loop_acceptor ( socket ) end defp serve ( socket ) do socket |> read_line () |> write_line ( socket ) serve ( socket ) end defp read_line ( socket ) do { :ok , data } = :gen_tcp . recv ( socket , 0 ) data end defp write_line ( line , socket ) do :gen_tcp . send ( socket , line ) end","title":"Elixir"},{"location":"extensions/codehilite/#erlang","text":"circular ( Defs ) -> [ { { Type , Base }, Fields } || { { Type , Base }, Fields } <- Defs , Type == msg , circular ( Base , Defs ) ]. circular ( Base , Defs ) -> Fields = proplists : get_value ({ msg , Base }, Defs ), circular ( Defs , Fields , [ Base ]). circular (_ Defs , [], _ Path ) -> false ; circular ( Defs , [ Field | Fields ], Path ) -> case Field #field.type of { msg , Type } -> case lists : member ( Type , Path ) of false -> Children = proplists : get_value ({ msg , Type }, Defs ), case circular ( Defs , Children , [ Type | Path ]) of false -> circular ( Defs , Fields , Path ); true -> true end ; true -> Type == lists : last ( Path ) andalso ( length ( Path ) == 1 orelse not is_tree ( Path )) end ; _ -> circular ( Defs , Fields , Path ) end .","title":"Erlang"},{"location":"extensions/codehilite/#f","text":"/// Asynchronously download retangles from the server /// and decode the JSON format to F# Rectangle record let [< Js >] getRectangles () : Async < Rectangle [] > = async { let req = XMLHttpRequest () req . Open ( \"POST\" , \"/get\" , true ) let! resp = req . AsyncSend () return JSON . parse ( resp ) } /// Repeatedly update rectangles after 0.5 sec let [< Js >] updateLoop () = async { while true do do ! Async . Sleep ( 500 ) let! rects = getRectangles () cleanRectangles () rects |> Array . iter createRectangle }","title":"F&#35;"},{"location":"extensions/codehilite/#go","text":"package main import \"fmt\" func counter ( id int , channel chan int , closer bool ) { for i := 0 ; i < 10000000 ; i ++ { fmt . Println ( \"process\" , id , \" send\" , i ) channel <- 1 } if closer { close ( channel ) } } func main () { channel := make ( chan int ) go counter ( 1 , channel , false ) go counter ( 2 , channel , true ) x := 0 // receiving data from channel for i := range channel { fmt . Println ( \"receiving\" ) x += i } fmt . Println ( x ) }","title":"Go"},{"location":"extensions/codehilite/#html","text":"<!doctype html> < html class = \"no-js\" lang = \"\" > < head > < meta charset = \"utf-8\" > < meta http-equiv = \"x-ua-compatible\" content = \"ie=edge\" > < title > HTML5 Boilerplate </ title > < meta name = \"description\" content = \"\" > < meta name = \"viewport\" content = \"width=device-width, initial-scale=1\" > < link rel = \"apple-touch-icon\" href = \"apple-touch-icon.png\" > < link rel = \"stylesheet\" href = \"css/normalize.css\" > < link rel = \"stylesheet\" href = \"css/main.css\" > < script src = \"js/vendor/modernizr-2.8.3.min.js\" ></ script > </ head > < body > < p > Hello world! This is HTML5 Boilerplate. </ p > </ body > </ html >","title":"HTML"},{"location":"extensions/codehilite/#java","text":"import java.util.LinkedList ; import java.lang.reflect.Array ; public class UnsortedHashSet < E > { private static final double LOAD_FACTOR_LIMIT = 0.7 ; private int size ; private LinkedList < E >[] con ; public UnsortedHashSet () { con = ( LinkedList < E >[] )( new LinkedList [ 10 ] ); } public boolean add ( E obj ) { int oldSize = size ; int index = Math . abs ( obj . hashCode ()) % con . length ; if ( con [ index ] == null ) con [ index ] = new LinkedList < E > (); if ( ! con [ index ] . contains ( obj )) { con [ index ] . add ( obj ); size ++ ; } if ( 1.0 * size / con . length > LOAD_FACTOR_LIMIT ) resize (); return oldSize != size ; } private void resize () { UnsortedHashSet < E > temp = new UnsortedHashSet < E > (); temp . con = ( LinkedList < E >[] )( new LinkedList [ con . length * 2 + 1 ] ); for ( int i = 0 ; i < con . length ; i ++ ) { if ( con [ i ] != null ) for ( E e : con [ i ] ) temp . add ( e ); } con = temp . con ; } public int size () { return size ; } }","title":"Java"},{"location":"extensions/codehilite/#javascript","text":"var Math = require ( 'lib/math' ); var _extends = function ( target ) { for ( var i = 1 ; i < arguments . length ; i ++ ) { var source = arguments [ i ]; for ( var key in source ) { target [ key ] = source [ key ]; } } return target ; }; var e = exports . e = 2.71828182846 ; exports [ 'default' ] = function ( x ) { return Math . exp ( x ); }; module . exports = _extends ( exports [ 'default' ], exports );","title":"JavaScript"},{"location":"extensions/codehilite/#json","text":"{ \"name\" : \"mkdocs-material\" , \"version\" : \"0.2.4\" , \"description\" : \"A Material Design theme for MkDocs\" , \"homepage\" : \"http://squidfunk.github.io/mkdocs-material/\" , \"authors\" : [ \"squidfunk <martin.donath@squidfunk.com>\" ], \"license\" : \"MIT\" , \"main\" : \"Gulpfile.js\" , \"scripts\" : { \"start\" : \"./node_modules/.bin/gulp watch --mkdocs\" , \"build\" : \"./node_modules/.bin/gulp build --production\" } ... }","title":"JSON"},{"location":"extensions/codehilite/#julia","text":"using MXNet mlp = @mx . chain mx . Variable ( : data ) => mx . FullyConnected ( name =: fc1 , num_hidden = 128 ) => mx . Activation ( name =: relu1 , act_type =: relu ) => mx . FullyConnected ( name =: fc2 , num_hidden = 64 ) => mx . Activation ( name =: relu2 , act_type =: relu ) => mx . FullyConnected ( name =: fc3 , num_hidden = 10 ) => mx . SoftmaxOutput ( name =: softmax ) # data provider batch_size = 100 include ( Pkg . dir ( \"MXNet\" , \"examples\" , \"mnist\" , \"mnist-data.jl\" )) train_provider , eval_provider = get_mnist_providers ( batch_size ) # setup model model = mx . FeedForward ( mlp , context = mx . cpu ()) # optimization algorithm optimizer = mx . SGD ( lr = 0.1 , momentum = 0.9 ) # fit parameters mx . fit ( model , optimizer , train_provider , n_epoch = 20 , eval_data = eval_provider )","title":"Julia"},{"location":"extensions/codehilite/#lua","text":"local ffi = require ( \"ffi\" ) ffi . cdef [[ void Sleep(int ms); int poll(struct pollfd *fds, unsigned long nfds, int timeout); ]] local sleep if ffi . os == \"Windows\" then function sleep ( s ) ffi . C . Sleep ( s * 1000 ) end else function sleep ( s ) ffi . C . poll ( nil , 0 , s * 1000 ) end end for i = 1 , 160 do io.write ( \".\" ); io.flush () sleep ( 0.01 ) end io.write ( \" \\n \" )","title":"Lua"},{"location":"extensions/codehilite/#mysql","text":"SELECT Employees . EmployeeID , Employees . Name , Employees . Salary , Manager . Name AS Manager FROM Employees LEFT JOIN Employees AS Manager ON Employees . ManagerID = Manager . EmployeeID WHERE Employees . EmployeeID = '087652' ;","title":"MySQL"},{"location":"extensions/codehilite/#php","text":"<?php // src/AppBundle/Controller/LuckyController.php namespace AppBundle\\Controller ; use Sensio\\Bundle\\FrameworkExtraBundle\\Configuration\\Route ; use Symfony\\Component\\HttpFoundation\\Response ; class LuckyController { /** * @Route(\"/lucky/number\") */ public function numberAction () { $number = mt_rand ( 0 , 100 ); return new Response ( '<html><body>Lucky number: ' . $number . '</body></html>' ); } }","title":"PHP"},{"location":"extensions/codehilite/#protocol-buffers","text":"syntax = \"proto2\" ; package caffe ; // Specifies the shape (dimensions) of a Blob. message BlobShape { repeated int64 dim = 1 [ packed = true ]; } message BlobProto { optional BlobShape shape = 7 ; repeated float data = 5 [ packed = true ]; repeated float diff = 6 [ packed = true ]; // 4D dimensions -- deprecated. Use \"shape\" instead. optional int32 num = 1 [ default = 0 ]; optional int32 channels = 2 [ default = 0 ]; optional int32 height = 3 [ default = 0 ]; optional int32 width = 4 [ default = 0 ]; }","title":"Protocol Buffers"},{"location":"extensions/codehilite/#python","text":"\"\"\" A very simple MNIST classifier. See extensive documentation at http://tensorflow.org/tutorials/mnist/beginners/index.md \"\"\" from __future__ import absolute_import from __future__ import division from __future__ import print_function # Import data from tensorflow.examples.tutorials.mnist import input_data import tensorflow as tf flags = tf . app . flags FLAGS = flags . FLAGS flags . DEFINE_string ( 'data_dir' , '/tmp/data/' , 'Directory for storing data' ) mnist = input_data . read_data_sets ( FLAGS . data_dir , one_hot = True ) sess = tf . InteractiveSession () # Create the model x = tf . placeholder ( tf . float32 , [ None , 784 ]) W = tf . Variable ( tf . zeros ([ 784 , 10 ])) b = tf . Variable ( tf . zeros ([ 10 ])) y = tf . nn . softmax ( tf . matmul ( x , W ) + b )","title":"Python"},{"location":"extensions/codehilite/#ruby","text":"require 'finity/event' require 'finity/machine' require 'finity/state' require 'finity/transition' require 'finity/version' module Finity class InvalidCallback < StandardError ; end class MissingCallback < StandardError ; end class InvalidState < StandardError ; end # Class methods to be injected into the including class upon inclusion. module ClassMethods # Instantiate a new state machine for the including class by accepting a # block with state and event (and subsequent transition) definitions. def finity options = {}, & block @finity ||= Machine . new self , options , & block end # Return the names of all registered states. def states @finity . states . map { | name , _ | name } end # Return the names of all registered events. def events @finity . events . map { | name , _ | name } end end # Inject methods into the including class upon inclusion. def self . included base base . extend ClassMethods end end","title":"Ruby"},{"location":"extensions/codehilite/#xml","text":"<?xml version=\"1.0\" encoding=\"UTF-8\"?> <!DOCTYPE mainTag SYSTEM \"some.dtd\" [ENTITY % entity]> <?oxygen RNGSchema=\"some.rng\" type=\"xml\"?> <xs:main-Tag xmlns:xs= \"http://www.w3.org/2001/XMLSchema\" > <!-- This is a sample comment --> <childTag attribute= \"Quoted Value\" another-attribute= 'Single quoted value' a-third-attribute= '123' > <withTextContent> Some text content </withTextContent> <withEntityContent> Some text content with &lt; entities &gt; and mentioning uint8_t and int32_t </withEntityContent> <otherTag attribute= 'Single quoted Value' /> </childTag> <![CDATA[ some CData ]]> </main-Tag>","title":"XML"},{"location":"extensions/footnotes/","text":"Footnotes \u00b6 Footnotes is another extension included in the standard Markdown library. As the name says, it adds the ability to add footnotes to your documentation. Installation \u00b6 Add the following lines to your mkdocs.yml : markdown_extensions : - footnotes Usage \u00b6 The markup for footnotes is similar to the standard Markdown markup for links. A reference is inserted in the text, which can then be defined at any point in the document. Inserting the reference \u00b6 The footnote reference is enclosed in square brackets and starts with a caret, followed by an arbitrary label which may contain numeric identifiers [1, 2, 3, ...] or names [Granovetter et al. 1998]. The rendered references are always consecutive superscripted numbers. Example: Lorem ipsum[^1] dolor sit amet, consectetur adipiscing elit.[^2] Result: Lorem ipsum 1 dolor sit amet, consectetur adipiscing elit. 2 Inserting the content \u00b6 The footnote content is also declared with a label, which must match the label used for the footnote reference. It can be inserted at an arbitrary position in the document and is always rendered at the bottom of the page. Furthermore, a backlink is automatically added to the footnote reference. on a single line \u00b6 Short statements can be written on the same line. Example: [^1]: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Result: Jump to footnote at the bottom of the page on multiple lines \u00b6 Paragraphs should be written on the next line. As with all Markdown blocks, the content must be indented by four spaces. Example: [^2]: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Jump to footnote at the bottom of the page Lorem ipsum dolor sit amet, consectetur adipiscing elit. \u21a9 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. \u21a9","title":"Footnotes"},{"location":"extensions/footnotes/#footnotes","text":"Footnotes is another extension included in the standard Markdown library. As the name says, it adds the ability to add footnotes to your documentation.","title":"Footnotes"},{"location":"extensions/footnotes/#installation","text":"Add the following lines to your mkdocs.yml : markdown_extensions : - footnotes","title":"Installation"},{"location":"extensions/footnotes/#usage","text":"The markup for footnotes is similar to the standard Markdown markup for links. A reference is inserted in the text, which can then be defined at any point in the document.","title":"Usage"},{"location":"extensions/footnotes/#inserting-the-reference","text":"The footnote reference is enclosed in square brackets and starts with a caret, followed by an arbitrary label which may contain numeric identifiers [1, 2, 3, ...] or names [Granovetter et al. 1998]. The rendered references are always consecutive superscripted numbers. Example: Lorem ipsum[^1] dolor sit amet, consectetur adipiscing elit.[^2] Result: Lorem ipsum 1 dolor sit amet, consectetur adipiscing elit. 2","title":"Inserting the reference"},{"location":"extensions/footnotes/#inserting-the-content","text":"The footnote content is also declared with a label, which must match the label used for the footnote reference. It can be inserted at an arbitrary position in the document and is always rendered at the bottom of the page. Furthermore, a backlink is automatically added to the footnote reference.","title":"Inserting the content"},{"location":"extensions/footnotes/#on-a-single-line","text":"Short statements can be written on the same line. Example: [^1]: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Result: Jump to footnote at the bottom of the page","title":"on a single line"},{"location":"extensions/footnotes/#on-multiple-lines","text":"Paragraphs should be written on the next line. As with all Markdown blocks, the content must be indented by four spaces. Example: [^2]: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Jump to footnote at the bottom of the page Lorem ipsum dolor sit amet, consectetur adipiscing elit. \u21a9 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. \u21a9","title":"on multiple lines"},{"location":"extensions/metadata/","text":"Metadata \u00b6 The Metadata extension makes it possible to add metadata to a document which gives more control over the theme in a page-specific context. Installation \u00b6 Add the following lines to your mkdocs.yml : markdown_extensions : - meta Usage \u00b6 Metadata is written as a series of key-value pairs at the beginning of the Markdown document, delimited by a blank line which ends the metadata context. Naturally, the metadata is stripped from the document before rendering the actual page content and made available to the theme. Example: title: Lorem ipsum dolor sit amet description: Nullam urna elit, malesuada eget finibus ut, ac tortor. path: path/to/file source: file.js # Headline ... See the next section which covers the metadata that is supported by Material. Setting a hero text \u00b6 Material exposes a simple text-only page-local hero via Metadata, as you can see on the current page when you scroll to the top. It's as simple as: hero: Metadata enables hero teaser texts Linking sources \u00b6 When a document is related to a specific set of source files and the repo_url is defined inside the project's mkdocs.yml , the files can be linked using the source key: source: file.js The filename is appended to the repo_url set in your mkdocs.yml , but can be prefixed with a path to ensure correct path resolving: Example: path: tree/master/docs/extensions source: metadata.md Result: See the source section for the resulting output. Redirecting to another page \u00b6 It's sometimes necessary to move documents around in the navigation tree and redirect user from the old URL to the new one. The redirect meta-tag allows to create a redirection from the current document to the address specified in the tag. For instance, if your document contains: redirect: /new/url accessing that document's URL will automatically redirect to /new/url . Overrides \u00b6 Page title \u00b6 The page title can be overridden on a per-document level: title: Lorem ipsum dolor sit amet This will set the title tag inside the document head for the current page to the provided value. It will also override the default behavior of Material for MkDocs which appends the site title using a dash as a separator to the page title. Page description \u00b6 The page description can also be overridden on a per-document level: description : Nullam urna elit, malesuada eget finibus ut, ac tortor. This will set the meta tag containing the site description inside the document head for the current page to the provided value. Disqus \u00b6 As described in the getting started guide , the Disqus comments section can be enabled on a per-document level: disqus: your-shortname Disqus can be disabled for a specific page by setting it to an empty value: disqus:","title":"Metadata"},{"location":"extensions/metadata/#metadata","text":"The Metadata extension makes it possible to add metadata to a document which gives more control over the theme in a page-specific context.","title":"Metadata"},{"location":"extensions/metadata/#installation","text":"Add the following lines to your mkdocs.yml : markdown_extensions : - meta","title":"Installation"},{"location":"extensions/metadata/#usage","text":"Metadata is written as a series of key-value pairs at the beginning of the Markdown document, delimited by a blank line which ends the metadata context. Naturally, the metadata is stripped from the document before rendering the actual page content and made available to the theme. Example: title: Lorem ipsum dolor sit amet description: Nullam urna elit, malesuada eget finibus ut, ac tortor. path: path/to/file source: file.js # Headline ... See the next section which covers the metadata that is supported by Material.","title":"Usage"},{"location":"extensions/metadata/#setting-a-hero-text","text":"Material exposes a simple text-only page-local hero via Metadata, as you can see on the current page when you scroll to the top. It's as simple as: hero: Metadata enables hero teaser texts","title":"Setting a hero text"},{"location":"extensions/metadata/#linking-sources","text":"When a document is related to a specific set of source files and the repo_url is defined inside the project's mkdocs.yml , the files can be linked using the source key: source: file.js The filename is appended to the repo_url set in your mkdocs.yml , but can be prefixed with a path to ensure correct path resolving: Example: path: tree/master/docs/extensions source: metadata.md Result: See the source section for the resulting output.","title":"Linking sources"},{"location":"extensions/metadata/#redirecting-to-another-page","text":"It's sometimes necessary to move documents around in the navigation tree and redirect user from the old URL to the new one. The redirect meta-tag allows to create a redirection from the current document to the address specified in the tag. For instance, if your document contains: redirect: /new/url accessing that document's URL will automatically redirect to /new/url .","title":"Redirecting to another page"},{"location":"extensions/metadata/#overrides","text":"","title":"Overrides"},{"location":"extensions/metadata/#page-title","text":"The page title can be overridden on a per-document level: title: Lorem ipsum dolor sit amet This will set the title tag inside the document head for the current page to the provided value. It will also override the default behavior of Material for MkDocs which appends the site title using a dash as a separator to the page title.","title":"Page title"},{"location":"extensions/metadata/#page-description","text":"The page description can also be overridden on a per-document level: description : Nullam urna elit, malesuada eget finibus ut, ac tortor. This will set the meta tag containing the site description inside the document head for the current page to the provided value.","title":"Page description"},{"location":"extensions/metadata/#disqus","text":"As described in the getting started guide , the Disqus comments section can be enabled on a per-document level: disqus: your-shortname Disqus can be disabled for a specific page by setting it to an empty value: disqus:","title":"Disqus"},{"location":"extensions/permalinks/","text":"Permalinks \u00b6 Permalinks are a feature of the Table of Contents extension, which is part of the standard Markdown library. The extension inserts an anchor at the end of each headline, which makes it possible to directly link to a subpart of the document. Installation \u00b6 To enable permalinks, add the following to your mkdocs.yml : markdown_extensions : - toc : permalink : true This will add a link containing the paragraph symbol \u00b6 at the end of each headline (exactly like on the page you're currently viewing), which the Material theme will make appear on hover. In order to change the text of the permalink, a string can be passed, e.g.: markdown_extensions: - toc: permalink: Link Usage \u00b6 When enabled, permalinks are inserted automatically.","title":"Permalinks"},{"location":"extensions/permalinks/#permalinks","text":"Permalinks are a feature of the Table of Contents extension, which is part of the standard Markdown library. The extension inserts an anchor at the end of each headline, which makes it possible to directly link to a subpart of the document.","title":"Permalinks"},{"location":"extensions/permalinks/#installation","text":"To enable permalinks, add the following to your mkdocs.yml : markdown_extensions : - toc : permalink : true This will add a link containing the paragraph symbol \u00b6 at the end of each headline (exactly like on the page you're currently viewing), which the Material theme will make appear on hover. In order to change the text of the permalink, a string can be passed, e.g.: markdown_extensions: - toc: permalink: Link","title":"Installation"},{"location":"extensions/permalinks/#usage","text":"When enabled, permalinks are inserted automatically.","title":"Usage"},{"location":"extensions/pymdown/","text":"PyMdown Extensions \u00b6 PyMdown Extensions is a collection of Markdown extensions that add some great features to the standard Markdown library. For this reason, the installation of this package is highly recommended as it's well-integrated with the Material theme. Installation \u00b6 The PyMdown Extensions package can be installed with the following command: pip install pymdown-extensions The following list of extensions that are part of the PyMdown Extensions package are recommended to be used together with the Material theme: markdown_extensions : - pymdownx.arithmatex - pymdownx.betterem : smart_enable : all - pymdownx.caret - pymdownx.critic - pymdownx.details - pymdownx.emoji : emoji_generator : !!python/name:pymdownx.emoji.to_svg - pymdownx.inlinehilite - pymdownx.magiclink - pymdownx.mark - pymdownx.smartsymbols - pymdownx.superfences - pymdownx.tasklist : custom_checkbox : true - pymdownx.tilde Usage \u00b6 Arithmatex MathJax \u00b6 Arithmatex integrates Material with MathJax which parses block-style and inline equations written in TeX markup and outputs them in mathematical notation. See this thread for a short introduction and quick reference on how to write equations in TeX syntax. Besides activating the extension in the mkdocs.yml , the MathJax JavaScript runtime needs to be included. This must be done with additional JavaScript : extra_javascript : - 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML' If you want to override the default MathJax configuration, you can do this by adding another JavaScript file before the MathJax runtime in extra_javascript which contains your MathJax configuration, e.g.: window . MathJax = { tex2jax : { inlineMath : [ [ \"\\\\(\" , \"\\\\)\" ] ], displayMath : [ [ \"\\\\[\" , \"\\\\]\" ] ] }, TeX : { TagSide : \"right\" , TagIndent : \".8em\" , MultLineWidth : \"85%\" , equationNumbers : { autoNumber : \"AMS\" , }, unicode : { fonts : \"STIXGeneral,'Arial Unicode MS'\" } }, displayAlign : \"left\" , showProcessingMessages : false , messageStyle : \"none\" }; In your mkdocs.yml , include it with: extra_javascript : - 'javascripts/extra.js' - 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML' Blocks \u00b6 Blocks are enclosed in $$ ... $$ which are placed on separate lines. Example: $$ \\frac {n ! }{k !( n - k )! } = \\binom {n}{k} $$ Result: \\frac{n!}{k!(n-k)!} = \\binom{n}{k} \\frac{n!}{k!(n-k)!} = \\binom{n}{k} Inline \u00b6 Inline equations need to be enclosed in $ ... $ : Example: Lorem ipsum dolor sit amet: $ p ( x|y ) = \\frac {p ( y|x ) p ( x ) }{p ( y ) } $ Result: Lorem ipsum dolor sit amet: p(x|y) = \\frac{p(y|x)p(x)}{p(y)} p(x|y) = \\frac{p(y|x)p(x)}{p(y)} BetterEm \u00b6 BetterEm improves the handling of emphasis markup ( bold and italic ) within Markdown by providing a more sophisticated parser for better detecting start and end tokens. Read the documentation for usage notes . Caret \u00b6 Caret makes it possible to highlight inserted text . The portion of text that should be marked as added must be enclosed in two carets ^^...^^ . Critic \u00b6 Critic implements Critic Markup , a Markdown extension that enables the tracking of changes (additions, deletions and comments) on documents. During compilation of the Markdown document, changes can be rendered (default), accepted or rejected. Text can be deleted and replacement text added . This can also be combined into one a single operation. Highlighting is also possible and comments can be added inline . Formatting can also be applied to blocks, by putting the opening and closing tags on separate lines and adding new lines between the tags and the content. Details \u00b6 Details adds collapsible Admonition-style blocks which can contain arbitrary content using the HTML5 details and summary tags. Additionally, all Admonition qualifiers can be used, e.g. note , question , warning etc.: How many Prolog programmers does it take to change a lightbulb? Yes. Emoji \u00b6 Emoji adds the ability to insert a -load of emojis that we use in our daily lives. See the EmojiOne demo for a list of all available emojis. Happy scrolling Legal disclaimer Material has no affiliation with EmojiOne which is released under CC BY 4.0 . When including EmojiOne images or CSS, please read the EmojiOne license to ensure proper usage and attribution. InlineHilite \u00b6 InlineHilite adds support for inline code highlighting. It's useful for short snippets included within body copy, e.g. var test = 0 ; and can be achieved by prefixing inline code with a shebang and language identifier, e.g. #!js . MagicLink \u00b6 MagicLink detects links in Markdown and auto-generates the necessary markup, so no special syntax is required. It auto-links http[s]:// and ftp:// links, as well as references to email addresses. Mark \u00b6 Mark adds the ability to highlight text like it was marked with a text marker . The portion of text that should be highlighted must be enclosed in two equal signs ==...== . SmartSymbols \u00b6 SmartSymbols converts markup for special characters into their corresponding symbols, e.g. arrows (\u2190, \u2192, \u2194), trademark and copyright symbols (\u00a9, \u2122, \u00ae) and fractions (\u00bd, \u00bc, ...). SuperFences \u00b6 SuperFences provides the ability to nest code blocks under blockquotes, lists and other block elements, which the Fenced Code Blocks extension from the standard Markdown library doesn't parse correctly. SuperFences does also allow grouping code blocks with tabs . Tasklist \u00b6 Tasklist adds support for styled checkbox lists. This is useful for keeping track of tasks and showing what has been done and has yet to be done. Checkbox lists are like regular lists, but prefixed with [ ] for empty or [x] for filled checkboxes. Example: * [x] Lorem ipsum dolor sit amet, consectetur adipiscing elit * [x] Nulla lobortis egestas semper * [x] Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est * [ ] Vestibulum convallis sit amet nisi a tincidunt * [x] In hac habitasse platea dictumst * [x] In scelerisque nibh non dolor mollis congue sed et metus * [x] Sed egestas felis quis elit dapibus, ac aliquet turpis mattis * [ ] Praesent sed risus massa * [ ] Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque * [ ] Nulla vel eros venenatis, imperdiet enim id, faucibus nisi Result: Lorem ipsum dolor sit amet, consectetur adipiscing elit Nulla lobortis egestas semper Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst In scelerisque nibh non dolor mollis congue sed et metus Sed egestas felis quis elit dapibus, ac aliquet turpis mattis Praesent sed risus massa Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque Nulla vel eros venenatis, imperdiet enim id, faucibus nisi Tilde \u00b6 Tilde provides an easy way to strike through cross out text. The portion of text that should be erased must be enclosed in two tildes ~~...~~ and the extension will take care of the rest.","title":"PyMdown Extensions"},{"location":"extensions/pymdown/#pymdown-extensions","text":"PyMdown Extensions is a collection of Markdown extensions that add some great features to the standard Markdown library. For this reason, the installation of this package is highly recommended as it's well-integrated with the Material theme.","title":"PyMdown Extensions"},{"location":"extensions/pymdown/#installation","text":"The PyMdown Extensions package can be installed with the following command: pip install pymdown-extensions The following list of extensions that are part of the PyMdown Extensions package are recommended to be used together with the Material theme: markdown_extensions : - pymdownx.arithmatex - pymdownx.betterem : smart_enable : all - pymdownx.caret - pymdownx.critic - pymdownx.details - pymdownx.emoji : emoji_generator : !!python/name:pymdownx.emoji.to_svg - pymdownx.inlinehilite - pymdownx.magiclink - pymdownx.mark - pymdownx.smartsymbols - pymdownx.superfences - pymdownx.tasklist : custom_checkbox : true - pymdownx.tilde","title":"Installation"},{"location":"extensions/pymdown/#usage","text":"","title":"Usage"},{"location":"extensions/pymdown/#arithmatex-mathjax","text":"Arithmatex integrates Material with MathJax which parses block-style and inline equations written in TeX markup and outputs them in mathematical notation. See this thread for a short introduction and quick reference on how to write equations in TeX syntax. Besides activating the extension in the mkdocs.yml , the MathJax JavaScript runtime needs to be included. This must be done with additional JavaScript : extra_javascript : - 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML' If you want to override the default MathJax configuration, you can do this by adding another JavaScript file before the MathJax runtime in extra_javascript which contains your MathJax configuration, e.g.: window . MathJax = { tex2jax : { inlineMath : [ [ \"\\\\(\" , \"\\\\)\" ] ], displayMath : [ [ \"\\\\[\" , \"\\\\]\" ] ] }, TeX : { TagSide : \"right\" , TagIndent : \".8em\" , MultLineWidth : \"85%\" , equationNumbers : { autoNumber : \"AMS\" , }, unicode : { fonts : \"STIXGeneral,'Arial Unicode MS'\" } }, displayAlign : \"left\" , showProcessingMessages : false , messageStyle : \"none\" }; In your mkdocs.yml , include it with: extra_javascript : - 'javascripts/extra.js' - 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML'","title":"Arithmatex MathJax"},{"location":"extensions/pymdown/#blocks","text":"Blocks are enclosed in $$ ... $$ which are placed on separate lines. Example: $$ \\frac {n ! }{k !( n - k )! } = \\binom {n}{k} $$ Result: \\frac{n!}{k!(n-k)!} = \\binom{n}{k} \\frac{n!}{k!(n-k)!} = \\binom{n}{k}","title":"Blocks"},{"location":"extensions/pymdown/#inline","text":"Inline equations need to be enclosed in $ ... $ : Example: Lorem ipsum dolor sit amet: $ p ( x|y ) = \\frac {p ( y|x ) p ( x ) }{p ( y ) } $ Result: Lorem ipsum dolor sit amet: p(x|y) = \\frac{p(y|x)p(x)}{p(y)} p(x|y) = \\frac{p(y|x)p(x)}{p(y)}","title":"Inline"},{"location":"extensions/pymdown/#betterem","text":"BetterEm improves the handling of emphasis markup ( bold and italic ) within Markdown by providing a more sophisticated parser for better detecting start and end tokens. Read the documentation for usage notes .","title":"BetterEm"},{"location":"extensions/pymdown/#caret","text":"Caret makes it possible to highlight inserted text . The portion of text that should be marked as added must be enclosed in two carets ^^...^^ .","title":"Caret"},{"location":"extensions/pymdown/#critic","text":"Critic implements Critic Markup , a Markdown extension that enables the tracking of changes (additions, deletions and comments) on documents. During compilation of the Markdown document, changes can be rendered (default), accepted or rejected. Text can be deleted and replacement text added . This can also be combined into one a single operation. Highlighting is also possible and comments can be added inline . Formatting can also be applied to blocks, by putting the opening and closing tags on separate lines and adding new lines between the tags and the content.","title":"Critic"},{"location":"extensions/pymdown/#details","text":"Details adds collapsible Admonition-style blocks which can contain arbitrary content using the HTML5 details and summary tags. Additionally, all Admonition qualifiers can be used, e.g. note , question , warning etc.: How many Prolog programmers does it take to change a lightbulb? Yes.","title":"Details"},{"location":"extensions/pymdown/#emoji","text":"Emoji adds the ability to insert a -load of emojis that we use in our daily lives. See the EmojiOne demo for a list of all available emojis. Happy scrolling Legal disclaimer Material has no affiliation with EmojiOne which is released under CC BY 4.0 . When including EmojiOne images or CSS, please read the EmojiOne license to ensure proper usage and attribution.","title":"Emoji"},{"location":"extensions/pymdown/#inlinehilite","text":"InlineHilite adds support for inline code highlighting. It's useful for short snippets included within body copy, e.g. var test = 0 ; and can be achieved by prefixing inline code with a shebang and language identifier, e.g. #!js .","title":"InlineHilite"},{"location":"extensions/pymdown/#magiclink","text":"MagicLink detects links in Markdown and auto-generates the necessary markup, so no special syntax is required. It auto-links http[s]:// and ftp:// links, as well as references to email addresses.","title":"MagicLink"},{"location":"extensions/pymdown/#mark","text":"Mark adds the ability to highlight text like it was marked with a text marker . The portion of text that should be highlighted must be enclosed in two equal signs ==...== .","title":"Mark"},{"location":"extensions/pymdown/#smartsymbols","text":"SmartSymbols converts markup for special characters into their corresponding symbols, e.g. arrows (\u2190, \u2192, \u2194), trademark and copyright symbols (\u00a9, \u2122, \u00ae) and fractions (\u00bd, \u00bc, ...).","title":"SmartSymbols"},{"location":"extensions/pymdown/#superfences","text":"SuperFences provides the ability to nest code blocks under blockquotes, lists and other block elements, which the Fenced Code Blocks extension from the standard Markdown library doesn't parse correctly. SuperFences does also allow grouping code blocks with tabs .","title":"SuperFences"},{"location":"extensions/pymdown/#tasklist","text":"Tasklist adds support for styled checkbox lists. This is useful for keeping track of tasks and showing what has been done and has yet to be done. Checkbox lists are like regular lists, but prefixed with [ ] for empty or [x] for filled checkboxes. Example: * [x] Lorem ipsum dolor sit amet, consectetur adipiscing elit * [x] Nulla lobortis egestas semper * [x] Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est * [ ] Vestibulum convallis sit amet nisi a tincidunt * [x] In hac habitasse platea dictumst * [x] In scelerisque nibh non dolor mollis congue sed et metus * [x] Sed egestas felis quis elit dapibus, ac aliquet turpis mattis * [ ] Praesent sed risus massa * [ ] Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque * [ ] Nulla vel eros venenatis, imperdiet enim id, faucibus nisi Result: Lorem ipsum dolor sit amet, consectetur adipiscing elit Nulla lobortis egestas semper Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst In scelerisque nibh non dolor mollis congue sed et metus Sed egestas felis quis elit dapibus, ac aliquet turpis mattis Praesent sed risus massa Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque Nulla vel eros venenatis, imperdiet enim id, faucibus nisi","title":"Tasklist"},{"location":"extensions/pymdown/#tilde","text":"Tilde provides an easy way to strike through cross out text. The portion of text that should be erased must be enclosed in two tildes ~~...~~ and the extension will take care of the rest.","title":"Tilde"}]}